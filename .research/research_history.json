{
  "research_topic": "Data-efficient image classification methods using novel regularization and augmentation techniques",
  "queries": [
    "data efficient classification",
    "regularization augmentation methods",
    "novel image augmentation"
  ],
  "research_study_list": [
    {
      "title": "Automated Data Augmentations for Graph Classification",
      "full_text": "Published as a conference paper at ICLR 2023 AUTOMATED DATA AUGMENTATIONS FOR GRAPH CLASSIFICATION Youzhi Luo1∗, Michael McThrow2, Wing Au2, Tao Komikado3, Kanji Uchino2, Koji Maruhashi3, Shuiwang Ji1 1Texas A&M University, TX, USA 2Fujitsu Research of America, INC., CA, USA 3Fujitsu Research, Fujitsu Limited, Kanagawa, Japan {yzluo,sji}@tamu.edu {mmcthrow,WAu,komikado.tao,kanji,maruhashi.koji}@fujitsu.com ABSTRACT Data augmentations are effective in improving the invariance of learning ma- chines. We argue that the core challenge of data augmentations lies in designing data transformations that preserve labels. This is relatively straightforward for im- ages, but much more challenging for graphs. In this work, we propose GraphAug, a novel automated data augmentation method aiming at computing label-invariant augmentations for graph classiﬁcation. Instead of using uniform transformations as in existing studies, GraphAug uses an automated augmentation model to avoid compromising critical label-related information of the graph, thereby producing label-invariant augmentations at most times. To ensure label-invariance, we de- velop a training method based on reinforcement learning to maximize an estimated label-invariance probability. Experiments show that GraphAug outperforms pre- vious graph augmentation methods on various graph classiﬁcation tasks. 1 I NTRODUCTION Many real-world objects, such as molecules and social networks, can be naturally represented as graphs. Developing effective classiﬁcation models for these graph-structured data has been highly desirable but challenging. Recently, advances in deep learning have signiﬁcantly accelerated the progress in this direction. Graph neural networks (GNNs) (Gilmer et al., 2017), a class of deep neural network models speciﬁcally designed for graphs, have been widely applied to many graph representation learning and classiﬁcation tasks, such as molecular property prediction (Wang et al., 2022b; Liu et al., 2022; Wang et al., 2022a; 2023; Yan et al., 2022). However, just like deep models on images, GNN models can easily overﬁt and fail to achieve satis- factory performance on small datasets. To address this issue, data augmentations can be used to gen- erate more data samples. An important property of desirable data augmentations is label-invariance, which requires that label-related information should not be compromised during the augmentation process. This is relatively easy and straightforward to achieve for images (Taylor & Nitschke, 2018), since commonly used image augmentations, such as ﬂipping and rotation, can preserve almost all information of original images. However, ensuring label-invariance is much harder for graphs be- cause even minor modiﬁcation of a graph may change its semantics and thus labels. Currently, most commonly used graph augmentations (You et al., 2020) are based on random modiﬁcation of nodes and edges in the graph, but they do not explicitly consider the importance of label-invariance. In this work, we propose GraphAug, a novel graph augmentation method that can produce label- invariant augmentations with an automated learning model. GraphAug uses a learnable model to automate augmentation category selection and graph transformations. It optimizes the model to maximize an estimated label-invariance probability through reinforcement learning. Experimen- tal results show that GraphAug outperforms prior graph augmentation methods on multiple graph classiﬁcation tasks. The codes of GraphAug are available in DIG (Liu et al., 2021) library. ∗Work was done while the author was at Fujitsu Research of America, INC. 1 arXiv:2202.13248v4  [cs.LG]  28 Feb 2023Published as a conference paper at ICLR 2023 2 B ACKGROUND AND RELATED WORK 2.1 G RAPH CLASSIFICATION WITH NEURAL NETWORKS In this work, we study the problem of graph classiﬁcation. Let G = (V,E,X ) be an undirected graph, where V is the set of nodes and Eis the set of edges. The node feature matrix of the graphG is X ∈R|V|×d where the i-th row of X denotes the d-dimensional feature vector for the i-th node in G. For a graph classiﬁcation task with kcategories, the objective is to learn a classiﬁcation model f : G→y∈{1,...,k }that can predict the categorical label of G. Recently, GNNs (Kipf & Welling, 2017; Veliˇckovi´c et al., 2018; Xu et al., 2019; Gilmer et al., 2017; Gao & Ji, 2019) have shown great success in various graph classiﬁcation problems. Most GNNs use the message passing mechanism to learn graph node embeddings. Formally, the message passing for any node v∈V at the ℓ-th layer of a GNN model can be described as hℓ v = UPDATE ( hℓ−1 v ,AGG ({ mℓ jv : j ∈N(v) })) , (1) where N(v) denotes the set of all nodes connected to the nodevin the graph G, hℓ v is the embedding outputted from the ℓ-th layer for v, mℓ jv is the message propagated from the node jto the node vat the ℓ-th layer and is usually a function ofhℓ−1 v and hℓ−1 j . The aggregation function AGG(·) maps the messages from all neighboring nodes to a single vector, and the function UPDATE(·) updates hℓ−1 v to hℓ v using this aggregated message vector. Assuming that the GNN model has Llayers, the graph representation hG is computed by a global pooling function READOUT over all node embeddings as hG = READOUT ({ hL v : v∈V }) . (2) Afterwards, hG is fed into a multi-layer perceptron (MLP) model to compute the probability that G belongs to each of the categories {1,...,k }. Despite the success of GNNs, a major challenge in many graph classiﬁcation problems is data scarcity. For example, GNNs have been extensively used to predict molecular properties from graph structures of molecules. However, the manual labeling of molecules usually requires expensive wet lab experiments, so the amount of labeled molecule data is usually not large enough for expressive GNNs to achieve satisfactory prediction accuracy. In this work, we address this data scarcity chal- lenge with data augmentations. We focus on designing advanced graph augmentation strategies to generate more data samples by performing transformations on data samples in the dataset. 2.2 D ATA AUGMENTATIONS Data augmentations have been demonstrated to be effective in improving the performance for image and text classiﬁcation. For images, various image transformation or distortion techniques have been proposed to generate artiﬁcial image samples, such as ﬂipping, cropping, color shifting (Krizhevsky et al., 2012), scaling, rotation, and elastic distortion (Sato et al., 2015; Simard et al., 2003). And for texts, useful augmentation techniques include synonym replacement, positional swaps (Ratner et al., 2017a), and back translation (Sennrich et al., 2016). These data augmentation techniques have been widely used to reduce overﬁtting and improve robustness in training deep neural network models. In addition to hand-crafted augmentations, automating the selection of augmentations with learnable neural network model has been a recent emerging research area. Ratner et al. (2017b) selects and composes multiple image data augmentations using an LSTM (Hochreiter & Schmidhuber, 1997) model, and proposes to make the model avoid producing out-of-distribution samples through adver- sarial training. Cubuk et al. (2019) proposes AutoAugment, which adopts reinforcement learning based method to search optimal augmentations maximizing the classiﬁcation accuracy. To speed up training and reduce computational cost, a lot of methods have been proposed to improve AutoAug- ment through either faster searching mechanism (Ho et al., 2019; Lim et al., 2019), or advanced optimization methods (Hataya et al., 2020; Li et al., 2020; Zhang et al., 2020). 2.3 D ATA AUGMENTATIONS FOR GRAPHS While image augmentations have been extensively studied, doing augmentations for graphs is much more challenging. Images are Euclidean data formed by pixel values organized in matrices. Thus, 2Published as a conference paper at ICLR 2023 many well studied matrix transformations can naturally be used to design image augmentations, such as ﬂipping, scaling, cropping or rotation. They are either strict information lossless transformation, or able to preserve signiﬁcant information at most times, so label-invariance is relatively straight- forward to be satisﬁed. Differently, graphs are non-Euclidean data formed with nodes connected by edges in an irregular manner. Even minor structural modiﬁcation of a graph can destroy important information in it. Hence, it is very hard to design generic label-invariant transformations for graphs. Currently, designing data augmentations for graph classiﬁcation (Zhao et al., 2022; Ding et al., 2022; Yu et al., 2022) is a challenging problem. Some studies (Wang et al., 2021; Han et al., 2022; Guo & Mao, 2021; Park et al., 2022) propose interpolation-based mixup methods for graph augmentations, and Kong et al. (2022) propose to augment node features through adversarial learning. Nonetheless, most commonly used graph augmentation methods (Hamilton et al., 2017; Wang et al., 2020; You et al., 2020; Zhou et al., 2020a; Rong et al., 2020; Zhu et al., 2021a) are based on the random modiﬁcation of graph structures or features, such as randomly dropping nodes, perturbing edges, or masking node features. However, such random transformations are not necessarily label-invariant, because important label-related information may be randomly compromised (see Section 3.2 for detailed analysis and discussion). Hence, in practice, these augmentations do not always improve the performance of graph classiﬁcation models. 3 T HE PROPOSED GRAPH AUG METHOD While existing graph augmentation methods do not consider the importance of label-invariance, we dive deep into this challenging problem and propose to solve it by automated data augmentations. Note that though automated data augmentations have been applied to graph contrastive learning (You et al., 2021; Yin et al., 2022; Suresh et al., 2021; Hassani & Khasahmadi, 2022; Xie et al., 2022) and node classiﬁcation (Zhao et al., 2021; Sun et al., 2021), they have not been studied in supervised graph classiﬁcation. In this work, we propose GraphAug, a novel automated data augmentation framework for graph classiﬁcation. GraphAug automates augmentation category selection and graph transformations through a learnable augmentation model. To produce label-invariant augmentations, we optimize the model to maximize an estimated label-invariance probability with reinforcement learning. To our best knowledge, GraphAug is the ﬁrst work successfully applying automated data augmentations to generate new graph data samples for supervised graph classiﬁcation. 3.1 A UGMENTATION BY SEQUENTIAL TRANSFORMATIONS Similar to the automated image augmentation method in Ratner et al. (2017b), we consider graph augmentations as a sequential transformation process. Given a graph G0 sampled from the train- ing dataset, we map it to the augmented graph GT with a sequence of transformation functions a1,a2,...,a T generated by an automated data augmentation model g. Speciﬁcally, at the t-th step (1 ≤t ≤T), let the graph obtained from the last step be Gt−1, we ﬁrst use the augmentation model to generate at based on Gt−1, and map Gt−1 to Gt with at. In summary, this sequential augmentation process can be described as at = g(Gt−1), G t = at(Gt−1), 1 ≤t≤T. (3) In our method, a1,a2,...,a T are all selected from three categories of graph transformations: • Node feature masking (MaskNF), which sets some values in node feature vectors to zero; • Node dropping (DropNode), which drops certain portion of nodes from the input graph; • Edge perturbation (PerturbEdge), which produces the new graph by removing existing edges from the input graph and adding new edges to the input graph. 3.2 L ABEL -INVARIANT AUGMENTATIONS Most automated image augmentation methods focus on automating augmentation category selection. For instance, Ratner et al. (2017b) automate image augmentations by generating a discrete sequence from an LSTM (Hochreiter & Schmidhuber, 1997) model, and each token in the sequence represents a certain category of image transformation, such as random ﬂip and rotation. Following this setting, 3Published as a conference paper at ICLR 2023 our graph augmentation model g also selects the augmentation category at each step. Speciﬁcally, g will generate a discrete token ct representing the category of augmentation transformation at, denoting whether MaskNF, DropNode, or PerturbEdge will be used at the t-th step. We have experimented to only automate augmentation category selection and use the graph trans- formations that are uniformly operated on each graph element, such as each node, edge, or node feature. For example, the uniform DropNode will randomly drop each node in the graph with the same probability. These transformations are commonly used in other studies (You et al., 2020; Zhu et al., 2021a; Rong et al., 2020), and we call them as uniform transformations. However, we ﬁnd that this automated composition of multiple uniform transformations does not improve classiﬁca- tion performance (see Section 4.3 for details). We argue that it is because uniform transformations have equal chances to randomly modify each graph element, thus may accidentally damage signif- icant label-related information and change the label of the original data sample. For instance, in a molecular graph dataset, assuming that all molecular graphs containing a cycle are labeled as toxic because the cyclic structures are exactly the cause of toxicity. If we are using DropNode transfor- mation, dropping any node belonging to the cycle will damage this cyclic structure, and map a toxic molecule to a non-toxic one. By default, data augmentations only involve modifying data samples while labels are not changed, so data augmentations that are not label-invariant may ﬁnally produce many noisy data samples and greatly harm the training of the classiﬁcation model. We use the TRIANGLES dataset (Knyazev et al., 2019) as an example to study the effect of label- invariance. The task in this dataset is classifying graphs by the number of triangles (the cycles formed by only three nodes) contained in the graph. As shown in Figure 3 of Appendix A, the uni- form DropNode transformation is not label-invariant because it produces data samples with wrong labels through dropping nodes belonging to triangles, and the classiﬁcation accuracy is low when the classiﬁcation model is trained on these data samples. However, if we intentionally avoid dropping nodes in triangles, training the classiﬁcation model with this label-invariant data augmentation im- proves the classiﬁcation accuracy. The signiﬁcant performance gap between these two augmentation strategies clearly demonstrates the importance of label-invariance for graph augmentations. Based on the above analysis and experimental results, we can conclude that uniform transforma- tions should be avoided in designing label-invariant graph augmentations. Instead, we generate transformations for each element in the graph by the augmentation modelgin our method. Next, we introduce the detailed augmentation process in Section 3.3 and the training procedure in Section 3.4. 3.3 A UGMENTATION PROCESS Our augmentation model gis composed of three parts. They are a GNN based encoder for extracting features from graphs, a GRU (Cho et al., 2014) model for generating augmentation categories, and four MLP models for computing probabilities. We use graph isomorphism network (GIN) (Xu et al., 2019) model as the encoder. At the t-th augmentation step ( 1 ≤t ≤T), let the graph obtained from the last step be Gt−1 = (Vt−1,Et−1,Xt−1), we ﬁrst add a virtual node vvirtual into Vt−1 and add edges connecting the virtual node with all the nodes in Vt−1. In other words, a new graph G′ t−1 = (V′ t−1,E′ t−1,X′ t−1) is created from Gt−1 such that V′ t−1 = Vt−1 ∪{vvirtual}, E′ t−1 = Et−1 ∪{(vvirtual,v) :v∈Vt−1}, and X′ t−1 ∈R|V′ t−1|×d is the concatenation of Xt−1 and a trainable initial feature vector for the virtual node. We use the virtual node here to extract graph-level information because it can capture long range interactions in the graph more effectively than a pooling based readout layer (Gilmer et al., 2017). The GNN encoder performs multiple message passing operations on G′ t−1 to obtain r- dimensional embeddings {ev t−1 ∈Rr : v∈Vt−1}for nodes in Vt−1 and the virtual node embedding evirtual t−1 ∈Rr. Afterwards, the probabilities of selecting each augmentation category is computed from evirtual t−1 as qt = GRU(qt−1,evirtual t−1 ), p C t = MLPC(qt),where qt is the hidden state vector of the GRU model at thet-th step, and the MLP model MLPC outputs the probability vectorpC t ∈R3 denoting the probabilities of selecting MaskNF, DropNode, or PerturbEdge as the augmentation at the t-th step. The exact augmentation categoryctfor the t-th step is then randomly sampled from the categorical distribution with the probabilities in pC t . Finally, as described below, the computation of transformation probabilities for all graph elements and the process of producing the new graph Gt from Gt−1 vary depending on ct. 4Published as a conference paper at ICLR 2023 DropNode virtual node GNN  encoder virtual node  embedding node  embeddings GRU MaskNF PerturbEdge sample node  embeddings node features virtual node feature  masked node  features 1 0 … node  embeddings sample for each  node feature sample for  each node virtual node  embedding sample for  each edge edge embeddings Extract node embeddings Select  augmentation category Perform transformation + , + , Figure 1: An illustration of the process of producing Gt from Gt−1 with the augmentation model. • If ct is MaskNF, then for any node v ∈Vt−1, the probabilities pM t,v ∈Rd of masking each node feature of v is computed by the MLP model MLP M taking the node embedding ev t−1 as input. Afterwards, a binary vector oM t,v ∈{0,1}d is randomly sampled from the Bernoulli distribution parameterized with pM t,v. If the k-th element of oM t,v is one, i.e., oM t,v[k] = 1, the k-th node feature of vis set to zero. Such MaskNF transformation is performed for every node feature in Xt−1. • If ctis DropNode, then the probabilitypD t,v of dropping any nodev∈Vt−1 from Gt−1 is computed by the MLP model MLP D taking the node embedding ev t−1 as input. Afterwards, a binary value oD t,v ∈{0,1}is sampled from the Bernoulli distribution parameterized with pD t,v and vis dropped from Vt−1 if oD t,v = 1. Such DropNode transformation is performed for every node in Vt−1. • If ct is PerturbEdge, the transformations involve dropping some existing edges from Et−1 and adding some new edges into Et−1. We consider the set Et−1 as the droppable edge set, and we create an addable edge set Et−1, by randomly sampling at most |Et−1|addable edges from the set {(u,v) : u,v ∈Vt−1,(u,v) /∈Et−1}. For any (u,v) in Et−1, we compute the probability pP t,(u,v) of dropping it by the MLP model MLP P taking [eu t−1 + ev t−1,1] as input, where [·,·] denotes the concatenation operation. For any (u,v) in Et−1, we compute the probability pP t,(u,v) of adding an edge connecting uand vby MLPP taking [eu t−1 + ev t−1,0] as input. Afterwards, for every (u,v) ∈Et−1, we randomly sample a binary value oP t,(u,v) from the Bernoulli distribution parameterized with pP t,(u,v), and drop (u,v) from Et−1 if oP t,(u,v) = 1. Similarly, we randomly sample oP t,(u,v) for every (u,v) ∈Et−1 but we will add (u,v) into Et−1 if oP t,(u,v) = 1. An illustration of the process of producing Gt from Gt−1 with our augmentation model is given in Figure 1. We also provide the detailed augmentation algorithm in Algorithm 1 of Appendix B. 3.4 L ABEL -INVARIANCE OPTIMIZATION WITH REINFORCEMENT LEARNING As our objective is generating label-invariant augmentations at most times, the ideal augmentation model g should assign low transformation probabilities to graph elements corresponding to label- related information. For instance, when DropNode is used, if the dropping of some nodes will damage important graph substructures and cause label changing, the model g should assign very low dropping probabilities to these nodes. However, we cannot directly make the model learn to produce label-invariant augmentations through supervised training because we do not have ground truth labels denoting which graph elements are important and should not be modiﬁed. To tackle this issue, we implicitly optimize the model with a reinforcement learning based training method. We formulate the sequential graph augmentations as a Markov Decision Process (MDP). This is intuitive and reasonable, because the Markov property is naturally satisﬁed, i.e., the output graph at any transformation step is only dependent on the input graph, not on previously performed transfor- mation. Speciﬁcally, at the t-th augmentation step, we deﬁne Gt−1, the graph obtained from the last step, as the current state, and the process of augmenting Gt−1 to Gt is deﬁned as state transition. The action is deﬁned as the augmentation transformation at generated from the model g, which includes the augmentation category ct and the exact transformations performed on all elements of Gt−1. The probability p(at) of taking action at for different ct is is described as below. 5Published as a conference paper at ICLR 2023 • If ct is MaskNF, then the transformation probability is the product of masking or unmasking probabilities for features of all nodes in Vt−1, so p(at) is deﬁned as p(at) =p(ct) ∗ ∏ v∈Vt−1 d∏ k=1 ( pM t,v[k] )oM t,v[k] ( 1 −pM t,v[k] )1−oM t,v[k] . (4) • If ct is DropNode, then the transformation probability is the product of dropping or non-dropping probabilities for all nodes in Vt−1, so p(at) is deﬁned as p(at) =p(ct) ∗ ∏ v∈Vt−1 ( pD t,v )oD t,v ( 1 −pD t,v )1−oD t,v . (5) • If ct is PerturbEdge, then the transformation probability is the product of perturbing or non- perturbing probabilities for all edges in Et−1 and Et−1, so p(at) is deﬁned as p(at) =p(ct) ∗ ∏ (u,v)∈Et−1∪Et−1 ( pP t,(u,v) )oP t,(u,v) ( 1 −pP t,(u,v) )1−oP t,(u,v) . (6) We use the predicted label-invariance probabilities from a reward generation modelsas the feedback reward signal in the above reinforcement learning environment. We use graph matching network (Li et al., 2019) as the backbone of the reward generation model s (see Appendix C for detailed in- troduction). When the sequential augmentation process starting from the graph G0 ends, s takes (G0,GT) as inputs and outputs s(G0,GT), which denotes the probability that the label is invariant after mapping the graph G0 to the graph GT. We use the logarithm of the predicted label-invariance probability, i.e., RT = logs(G0,GT), as the return of the sequential augmentation process. Then the augmentation model gis optimized by the REINFORCE algorithm (Sutton et al., 2000), which updates the model by the policy gradient ˆgθ computed as ˆgθ = RT∇θ ∑T t=1 log p(at), where θ denotes the trainable parameters of g. Prior to training the augmentation model g, we ﬁrst train the reward generation model on manually sampled graph pairs from the training dataset. Speciﬁcally, a graph pair (G1,G2) is ﬁrst sampled from the dataset and passed into the reward generation model to predict the probability that G1 and G2 have the same label. Afterwards, the model is optimized by minimizing the binary cross entropy loss. During the training of the augmentation model g, the reward generation model is only used to generate rewards, so its parameters are ﬁxed. See Algorithm 2 and 3 in Appendix B for the detailed training algorithm of reward generation model and augmentation model. 3.5 D ISCUSSIONS AND RELATIONS WITH PRIOR METHODS Advantages of our method. Our method explicitly estimates the transformation probability of each graph element by the automated augmentation model, thereby eliminating the negative effect of adopting a uniform transformation probability. Also, the reinforcement learning based training method can effectively help the model detect critical label-related information in the input graph, so the model can avoid damaging it and produce label-invariant augmentations with greater chances. We will show these advantages through extensive empirical studies in Section 4.1 and 4.2. Besides, the use of sequential augmentation, i.e., multiple steps of augmentation, can naturally help produce more diverse augmentations and samples, and the downstream classiﬁcation model can beneﬁt from diverse training samples. We will demonstrate it through ablation studies in Section 4.3. Relations with prior automated graph augmentations. Several automated graph augmentation methods (You et al., 2021; Yin et al., 2022; Suresh et al., 2021; Hassani & Khasahmadi, 2022) have been proposed to generate multiple graph views for contrastive learning based pre-training. However, their augmentation models are optimized by contrastive learning objectives, which are not related to graph labels. Hence, their augmentation methods may still damage label-related infor- mation, and we experimentally show that they do not perform as well as GraphAug in supervised learning scenarios in Section 4.2. Though a recent study (Trivedi et al., 2022) claims that label- invariance is also important in contrastive learning, to our best knowledge, no automated graph augmentations have been proposed to preserve label-invariance in contrastive learning. Besides, we notice that a very recent study (Yue et al., 2022) also proposes a label-invariant automated augmenta- tion method named GLA for semi-supervised graph classiﬁcation. However, GLA is fundamentally 6Published as a conference paper at ICLR 2023 Table 1: The testing accuracy on the COLORS and TRIANGLES datasets with the GIN model. We report the average accuracy and standard deviation over ten runs on ﬁxed train/validation/test splits. Dataset No augmentation Uniform MaskNF Uniform DropNode Uniform PerturbEdge Uniform Mixture GraphAug COLORS 0.578±0.012 0.507±0.014 0.547±0.012 0.618±0.014 0.560±0.016 0.633±0.009 TRIANGLES0.506±0.006 0.509±0.020 0.473±0.006 0.303±0.010 0.467±0.007 0.513±0.006 Figure 2: The changing curves of average rewards and label-invariance ratios on the validation set of the COLORS and TRIANGLES datasets as the augmentation model training proceeds. The results are averaged over ten runs, and the shadow shows the standard deviation. different from GraphAug. For a graph data sample, GLA ﬁrst obtains its graph-level representation by a GNN encoder. Then the augmentations are performed by perturbing the representation vector and label-invariant representations are selected by an auxiliary classiﬁcation model. However, our GraphAug directly augments the graph data samples, and label-invariance is ensured by our pro- posed training method based on reinforcement learning. Hence, GraphAug can generate new data samples to enrich the existing training dataset while GLA cannot achieve it. Due to the space limitation, we will discuss computational cost, augmentation step number, pre- training reward generation models, limitations, and relation with more prior methods in Appendix D. 4 E XPERIMENTS In this section, we evaluate the proposed GraphAug method on two synthetic graph datasets and seven benchmark datasets. We show that in various graph classiﬁcation tasks, GraphAug can consis- tently outperform previous graph augmentation methods. In addition, we conduct extensive ablation studies to evaluate the contributions of some components in GraphAug. 4.1 E XPERIMENTS ON SYNTHETIC GRAPH DATASETS Data. We ﬁrst show that our method can indeed produce label-invariant augmentations and outper- form uniform transformations through experiments on two synthetic graph datasets COLORS and TRIANGLES, which are synthesized by running the open sourced data synthesis code1 of Knyazev et al. (2019). The task of COLORS dataset is classifying graphs by the number of green nodes, and the color of a node is speciﬁed by its node feature. The task of TRIANGLES dataset is classifying graphs by the number of triangles (three-node cycles). We use ﬁxed train/validation/test splits for experiments on both datasets. See more information about these two datasets in Appendix E.1. Setup. We ﬁrst train the reward generation model until it converges, then train the automated aug- mentation model. To check whether our augmentation model can learn to produce label-invariant augmentations, at different training iterations, we calculate the average rewards and the label- invariance ratio achieved after augmenting graphs in the validation set. Note that since we ex- plicitly know how to obtain the labels of graphs from data generation codes, we can calculate label- invariance ratio, i.e., the ratio of augmented graphs that preserve their labels. To compare GraphAug with other augmentation methods, we train a GIN (Xu et al., 2019) based classiﬁcation model with different augmentations for ten times, and report the averaged testing classiﬁcation accuracy. We compare our GraphAug method with not using any data augmentations, and four graph augmenta- tion baseline methods. Speciﬁcally, the augmentation methods using uniform MaskNF, DropNode, and PerturbEdge transformations, and a mixture of these three uniform transformations (Uniform Mixture), i.e., randomly picking one to augment graphs at each time, are used as baselines. To en- 1https://github.com/bknyaz/graph_attention_pool 7Published as a conference paper at ICLR 2023 Table 2: The performance on seven benchmark datasets with the GIN model. We report the average ROC-AUC and standard deviation over ten runs for the ogbg-molhiv dataset, and the average ac- curacy and standard deviations over three 10-fold cross-validation runs for the other datasets. Note that for JOAOv2, AD-GCL, and AutoGCL, we evaluate the augmentation methods of them under the supervised learning setting, so the numbers here are different from those in their papers. Method PROTEINS IMDB-BINARY COLLAB MUTAG NCI109 NCI1 ogbg-molhiv No augmentation0.704±0.004 0.731±0.004 0.806±0.003 0.827±0.013 0.794±0.003 0.804±0.003 0.756±0.014 Uniform MaskNF0.702±0.008 0.720±0.006 0.815±0.002 0.788±0.012 0.777±0.006 0.794±0.002 0.741±0.010Uniform DropNode0.707±0.004 0.728±0.006 0.815±0.004 0.787±0.003 0.777±0.002 0.787±0.003 0.717±0.011Uniform PerturbEdge0.668±0.006 0.728±0.007 0.816±0.003 0.764±0.008 0.555±0.014 0.545±0.006 0.755±0.013Uniform Mixture0.707±0.004 0.730±0.009 0.815±0.003 0.779±0.014 0.776±0.006 0.783±0.003 0.746±0.010 DropEdge 0.707±0.002 0.733±0.012 0.812±0.003 0.779±0.005 0.762±0.007 0.780±0.002 0.762±0.010M-Mixup 0.706±0.003 0.736±0.004 0.811±0.005 0.798±0.015 0.788±0.005 0.803±0.003 0.753±0.013G-Mixup 0.715±0.006 0.748±0.004 0.811±0.009 0.805±0.020 0.654±0.043 0.686±0.037 0.771±0.005FLAG 0.709±0.007 0.747±0.008 0.803±0.006 0.835±0.015 0.804±0.002 0.804±0.002 0.765±0.011 JOAOv2 0.700±0.003 0.707±0.008 0.688±0.003 0.775±0.016 0.675±0.003 0.670±0.006 0.744±0.014AD-GCL 0.699±0.008 0.712±0.008 0.670±0.008 0.837±0.010 0.634±0.003 0.641±0.004 0.762±0.013AutoGCL 0.684±0.008 0.707±0.007 0.745±0.002 0.783±0.022 0.705±0.003 0.737±0.002 0.704±0.016 GraphAug 0.722±0.004 0.762±0.004 0.829±0.002 0.853±0.008 0.811±0.002 0.816±0.001 0.774±0.010 sure fair comparison, we use the same hyper-parameter setting in training classiﬁcation models for all methods. See hyper-parameters and more experimental details in Appendix E.1. Results. The changing curves of average rewards and label-invariance ratios are visualized in Fig- ure 2. These curves show that as the training proceeds, our model can gradually learn to obtain higher rewards and produce augmentations leading to higher label-invariance ratio. In other words, they demonstrate that our augmentation model can indeed learn to produce label-invariant augmentations after training. The testing accuracy of all methods on two datasets are presented in Table 1. From the results, we can clearly ﬁnd using some uniform transformations that do not satisfy label-invariance, such as uniform MaskNF on the COLORS dataset, achieve much worse performance than not us- ing augmentations. However, using augmentations produced by the trained GraphAug models can consistently achieve the best performance, which demonstrates the signiﬁcance of label-invariant augmentations to improving the performance of graph classiﬁcation models. We further study the training stability and generalization ability of GraphAug models, conduct an exploration experiment about training GraphAug models with adversarial learning, compare with some manually designed label-invariant augmentations, and compare label-invariance ratios with baseline methods on the COLORS and TRIANGLES datasets. See Appendix F.1, F.2, F.3, and F.6 for details. 4.2 E XPERIMENTS ON GRAPH BENCHMARK DATASETS Data. We further demonstrate the advantages of our GraphAug method over previous graph aug- mentation methods on six widely used datasets from the TUDatasets benchmark (Morris et al., 2020), including MUTAG, NCI109, NCI1, PROTEINS, IMDB-BINARY , and COLLAB. We also conduct experiments on the ogbg-molhiv dataset, which is a large molecular graph dataset from the OGB benchmark (Hu et al., 2020). See more information about datasets in Appendix E.2. Setup. We evaluate the performance by testing accuracy for the six datasets of the TUDatasets benchmark, and use testing ROC-AUC for the ogbg-molhiv dataset. We use two classiﬁcation mod- els, including GIN (Xu et al., 2019) and GCN (Kipf & Welling, 2017). We use the 10-fold cross- validation scheme with train/validation/test splitting ratios of 80%/10%/10% on the datasets from the TU-Datasets benchmark, and report the averaged testing accuracy over three different runs. For the ogbg-molhiv dataset, we use the ofﬁcial train/validation/test splits and report the averaged testing ROC-AUC over ten runs. In addition to the baselines in Section 4.1, we also compare with previ- ous graph augmentation methods, including DropEdge (Rong et al., 2020), M-Mixup (Wang et al., 2021), G-Mixup (Han et al., 2022), and FLAG (Kong et al., 2022). Besides, we compare with three automated augmentations proposed for graph self-supervised learning, including JOAOv2 (You et al., 2021), AD-GCL (Suresh et al., 2021), and AutoGCL (Yin et al., 2022). Note that we take their trained augmentation modules as the data augmenter, and evaluate the performance of supervised classiﬁcation models trained on the samples produced by these data augmenters. For fair compar- ison, we use the same hyper-parameter setting in training classiﬁcation models for GraphAug and baseline methods. See hyper-parameters and more experimental details in Appendix E.2. 8Published as a conference paper at ICLR 2023 Table 3: Results of ablation studies about learnable and sequential augmentation. We report the av- erage accuracy and standard deviation over three 10-fold cross-validation runs with the GIN model. Method PROTEINS IMDB-BINARY NCI1 GraphAug w/o learnable graph transformation 0.696±0.006 0.724 ±0.003 0.760 ±0.003 GraphAug w/o learnable category selection 0.702±0.004 0.746 ±0.009 0.796 ±0.006 GraphAug w/o sequential augmentation 0.712±0.002 0.753 ±0.003 0.809 ±0.004 GraphAug 0.722±0.004 0.762 ±0.004 0.816 ±0.001 Results. The performance of different methods on all seven datasets with the GIN model is sum- marized in Table 2, and see Table 9 in Appendix F.4 for the results of the GCN model. According to the results, our GraphAug method can achieve the best performance among all graph augmenta- tion methods over seven datasets. Similar to the results in Table 1, for molecule datasets including MUTAG, NCI109, NCI1, and ogbg-molhiv, using some uniform transformations based augmenta- tion methods dramatically degrades the classiﬁcation accuracy. On the other hand, our GraphAug method consistently outperforms baseline methods, such as mixup methods and existing automated data augmentations in graph self-supervised learning. The success on graph benchmark datasets once again validates the effectiveness of our proposed GraphAug method. 4.3 A BLATION STUDIES In addition to demonstrating the effectiveness of GraphAug, we conduct a series of ablation experi- ments and use empirical results to answer (1) why we make augmentation automated and learnable, (2) why we use sequential, multi-step augmentation, (3) why we adopt a combination of three differ- ent transformations (MaskNF, DropNode, PerturbEdge) instead of using only one, and (4) why we use virtual nodes. We present the ablation studies (1) and (2) in this section and leave (3) and (4) in Appendix F.5. For all ablation studies, we train GIN based classiﬁcation models on the PROTEINS, IMDB-BINARY , and NCI1 datasets, and use the same evaluation pipeline as Section 4.2. Ablation on learnable graph transformation and category selection. We ﬁrst show that making the model learn to generate graph transformations for each graph element and select augmenta- tion category are both important. We compare with a variant of GraphAug that does not learn graph transformations but simply adopts uniform transformations, and another variant that randomly select the category of graph transformation, instead of explicitly predicting it. The classiﬁcation accuracy on three datasets of these two variants are presented in the ﬁrst two rows of Table 3. Results show that the performance of two variants is worse, and particularly removing learnable graph transfor- mation will signiﬁcantly degrade the performance. It is demonstrated that learning to generate graph transformations and select augmentation category are both key success factors of GraphAug. Ablation on sequential augmentation. We next show the advantage of sequential augmentation over one-step augmentation. We compare with the variant of GraphAug that performs only one step of augmentation, i.e., with the augmentation step number T=1, and present its performance in the third row of Table 3. It is clear that using one step of augmentation will result in worse performance over all datasets. We think this demonstrates that the downstream classiﬁcation model will beneﬁt from the diverse training samples generated from sequential and multi-step augmentation. 5 C ONCLUSIONS AND FUTURE WORK We propose GraphAug, the ﬁrst automated data augmentation framework for graph classiﬁcation. GraphAug considers graph augmentations as a sequential transformation process. To eliminate the negative effect of uniform transformations, GraphAug uses an automated augmentation model to generate transformations for each element in the graph. In addition, GraphAug adopts a reinforce- ment learning based training procedure, which helps the augmentation model learn to avoid damag- ing label-related information and produce label-invariant augmentations. Through extensive empiric studies, we demonstrate that GraphAug can achieve better performance than many existing graph augmentation methods on various graph classiﬁcation tasks. In the future, we would like to explore simplifying the current training procedure of GraphAug and applying GraphAug to other graph representation learning problems, such as the node classiﬁcation problem. 9Published as a conference paper at ICLR 2023 REPRODUCIBILITY STATEMENT We have provided the detailed algorithm pseudocodes in Appendix B and experimental setting de- tails in Appendix E for reproducing the results. The source codes of our method are included in DIG (Liu et al., 2021) library. REFERENCES Alessandro Bicciato and Andrea Torsello. GAMS: Graph augmentation with module swapping. In Proceedings of the 11th International Conference on Pattern Recognition Applications and Methods (ICPRAM 2022), pp. 249–255, 2022. Kyunghyun Cho, Bart van Merri ¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder– decoder for statistical machine translation. In Proceedings of the 2014 conference on em- pirical methods in natural language processing (EMNLP) , pp. 1724–1734, Doha, Qatar, Oc- tober 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1179. URL https://www.aclweb.org/anthology/D14-1179. Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. AutoAugment: Learning augmentation strategies from data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 113–123, 2019. Kaize Ding, Zhe Xu, Hanghang Tong, and Huan Liu. Data augmentation for deep graph learning: A survey. SIGKDD Explor. Newsl., 24(2):61–77, dec 2022. ISSN 1931-0145. doi: 10.1145/ 3575637.3575646. URL https://doi.org/10.1145/3575637.3575646. Hongyang Gao and Shuiwang Ji. Graph U-Nets. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Research, pp. 2083–2092. PMLR, 09–15 Jun 2019. Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In Doina Precup and Yee Whye Teh (eds.),Proceedings of the 34th International Conference on Machine Learning, volume 70 ofProceedings of Machine Learning Research, pp. 1263–1272, International Convention Centre, Sydney, Australia, 2017. Hongyu Guo and Yongyi Mao. Intrusion-free graph mixup.arXiv preprint arXiv:2110.09344, 2021. William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 1025–1035, 2017. Xiaotian Han, Zhimeng Jiang, Ninghao Liu, and Xia Hu. G-Mixup: Graph data augmentation for graph classiﬁcation. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.),Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research , pp. 8230–8248. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/han22c.html. Kaveh Hassani and Amir Hosein Khasahmadi. Learning graph augmentations to learn graph repre- sentations. arXiv preprint arXiv:2201.09830, 2022. Ryuichiro Hataya, Jan Zdenek, Kazuki Yoshizoe, and Hideki Nakayama. Faster AutoAugment: Learning augmentation strategies using backpropagation. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision – ECCV 2020, pp. 1–16, Cham, 2020. Springer International Publishing. ISBN 978-3-030-58595-2. Daniel Ho, Eric Liang, Xi Chen, Ion Stoica, and Pieter Abbeel. Population based augmentation: Efﬁcient learning of augmentation policy schedules. In International Conference on Machine Learning, pp. 2731–2741. PMLR, 2019. Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735–1780, 1997. 10Published as a conference paper at ICLR 2023 Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Ad- vances in Neural Information Processing Systems , volume 33, pp. 22118–22133. Curran As- sociates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ fb60d411a5c5b72b2e7d3527cfc84fd0-Paper.pdf. Diederick P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceddings of the 3rd international conference on learning representations, 2015. Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional net- works. In 5th International Conference on Learning Representations, 2017. Boris Knyazev, Graham W Taylor, and Mohamed Amer. Understanding attention and generalization in graph neural networks. Advances in Neural Information Processing Systems , 32:4202–4212, 2019. Kezhi Kong, Guohao Li, Mucong Ding, Zuxuan Wu, Chen Zhu, Bernard Ghanem, Gavin Taylor, and Tom Goldstein. FLAG: Adversarial data augmentation for graph neural networks. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classiﬁcation with deep con- volutional neural networks. Advances in neural information processing systems, 25:1097–1105, 2012. Yonggang Li, Guosheng Hu, Yongtao Wang, Timothy Hospedales, Neil M. Robertson, and Yongxin Yang. Differentiable automatic data augmentation. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision – ECCV 2020, pp. 580–595, Cham, 2020. Springer International Publishing. ISBN 978-3-030-58542-6. Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli. Graph matching net- works for learning the similarity of graph structured objects. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Research , pp. 3835–3845. PMLR, 09–15 Jun 2019. Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast AutoAug- ment. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch ´e-Buc, E. Fox, and R. Gar- nett (eds.), Advances in Neural Information Processing Systems , volume 32. Curran Asso- ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/ 6add07cf50424b14fdf649da87843d01-Paper.pdf. Hongyi Ling, Zhimeng Jiang, Youzhi Luo, Shuiwang Ji, and Na Zou. Learning fair graph represen- tations via automated data augmentations. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=1_OGWcP1s9w. Meng Liu, Youzhi Luo, Limei Wang, Yaochen Xie, Hao Yuan, Shurui Gui, Haiyang Yu, Zhao Xu, Jingtun Zhang, Yi Liu, Keqiang Yan, Haoran Liu, Cong Fu, Bora M Oztekin, Xuan Zhang, and Shuiwang Ji. DIG: A turnkey library for diving into graph deep learning research. Journal of Machine Learning Research, 22(240):1–9, 2021. URL http://jmlr.org/papers/v22/ 21-0343.html. Yi Liu, Limei Wang, Meng Liu, Yuchao Lin, Xuan Zhang, Bora Oztekin, and Shuiwang Ji. Spherical message passing for 3D molecular graphs. In International Conference on Learning Representa- tions, 2022. URL https://openreview.net/forum?id=givsRXsOt9r. Koji Maruhashi, Masaru Todoriki, Takuya Ohwa, Keisuke Goto, Yu Hasegawa, Hiroya Inakoshi, and Hirokazu Anai. Learning multi-way relations via tensor decomposition with neural networks. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 32, 2018. Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. TUDataset: A collection of benchmark datasets for learning with graphs. In ICML 2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020), 2020. URL www. graphlearning.io. 11Published as a conference paper at ICLR 2023 Joonhyung Park, Hajin Shim, and Eunho Yang. Graph Transplant: Node saliency-guided graph mixup with local structure preservation. Proceedings of the AAAI Conference on Artiﬁcial Intel- ligence, 2022. Alex Ratner, Henry Ehrenberg, Zeshan Hussain, Jared Dunnmon, and Chris R´e. Data augmentation with Snorkel. https://www.snorkel.org/blog/tanda, 2017a. Accessed:2022-01-24. Alexander J. Ratner, Henry R. Ehrenberg, Zeshan Hussain, Jared Dunnmon, and Christopher R ´e. Learning to compose domain-speciﬁc transformations for data augmentation. In Proceedings of the 31st International Conference on Neural Information Processing Systems , NIPS’17, pp. 3239–3249, Red Hook, NY , USA, 2017b. Curran Associates Inc. ISBN 9781510860964. Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. DropEdge: Towards deep graph convolutional networks on node classiﬁcation. In International Conference on Learning Repre- sentations, 2020. URL https://openreview.net/forum?id=Hkx1qkrKPr. Ikuro Sato, Hiroki Nishimura, and Kensuke Yokoi. APAC: Augmented pattern classiﬁcation with neural networks. arXiv preprint arXiv:1505.03229, 2015. Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation mod- els with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 86–96, Berlin, Germany, Au- gust 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1009. URL https://aclanthology.org/P16-1009. P.Y . Simard, D. Steinkraus, and J.C. Platt. Best practices for convolutional neural networks applied to visual document analysis. In Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings., pp. 958–963, 2003. doi: 10.1109/ICDAR.2003.1227801. Junwei Sun, Bai Wang, and Bin Wu. Automated graph representation learning for node classiﬁca- tion. In 2021 International Joint Conference on Neural Networks (IJCNN) , pp. 1–7, 2021. doi: 10.1109/IJCNN52387.2021.9533811. Susheel Suresh, Pan Li, Cong Hao, and Jennifer Neville. Adversarial graph augmentation to im- prove graph contrastive learning. In A. Beygelzimer, Y . Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems , 2021. URL https: //openreview.net/forum?id=ioyq7NsR1KJ. Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in neural informa- tion processing systems, pp. 1057–1063, 2000. Luke Taylor and Geoff Nitschke. Improving deep learning with generic data augmentation. In 2018 IEEE Symposium Series on Computational Intelligence (SSCI), pp. 1542–1547. IEEE, 2018. Puja Trivedi, Ekdeep Singh Lubana, Yujun Yan, Yaoqing Yang, and Danai Koutra. Augmentations in graph contrastive learning: Current methodological ﬂaws & towards better practices. In Proceed- ings of the ACM Web Conference 2022, WWW ’22, pp. 1538–1549, New York, NY , USA, 2022. Association for Computing Machinery. ISBN 9781450390965. doi: 10.1145/3485447.3512200. URL https://doi.org/10.1145/3485447.3512200. Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li `o, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations , 2018. URL https://openreview.net/forum?id=rJXMpikCZ. Limei Wang, Yi Liu, Yuchao Lin, Haoran Liu, and Shuiwang Ji. ComENet: Towards complete and efﬁcient message passing for 3d molecular graphs. In Alice H. Oh, Alekh Agarwal, Danielle Bel- grave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022a. URL https://openreview.net/forum?id=mCzMqeWSFJ. Limei Wang, Haoran Liu, Yi Liu, Jerry Kurtin, and Shuiwang Ji. Hierarchical protein representations via complete 3d graph networks. InInternational Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=9X-hgLDLYkQ. 12Published as a conference paper at ICLR 2023 Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. GraphCrop: Subgraph cropping for graph classiﬁcation. arXiv preprint arXiv:2009.10564, 2020. Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. Mixup for node and graph classiﬁcation. In Proceedings of the Web Conference 2021 , WWW ’21, pp. 3663–3674, New York, NY , USA, 2021. Association for Computing Machinery. ISBN 9781450383127. doi: 10. 1145/3442381.3449796. URL https://doi.org/10.1145/3442381.3449796. Zhengyang Wang, Meng Liu, Youzhi Luo, Zhao Xu, Yaochen Xie, Limei Wang, Lei Cai, Qi Qi, Zhuoning Yuan, Tianbao Yang, and Shuiwang Ji. Advanced graph and sequence neural net- works for molecular property prediction and drug discovery. Bioinformatics, 38(9):2579–2586, 02 2022b. ISSN 1367-4803. doi: 10.1093/bioinformatics/btac112. URL https://doi.org/ 10.1093/bioinformatics/btac112. Yaochen Xie, Zhao Xu, Jingtun Zhang, Zhengyang Wang, and Shuiwang Ji. Self-supervised learning of graph neural networks: A uniﬁed review. IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1–1, 2022. doi: 10.1109/TPAMI.2022.3170559. Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In International Conference on Learning Representations , 2019. URL https: //openreview.net/forum?id=ryGs6iA5Km. Keqiang Yan, Yi Liu, Yuchao Lin, and Shuiwang Ji. Periodic graph transformers for crys- tal material property prediction. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022. URL https://openreview.net/forum?id=pqCT3L-BU9T. Yihang Yin, Qingzhong Wang, Siyu Huang, Haoyi Xiong, and Xiang Zhang. AutoGCL: Automated graph contrastive learning via learnable view generators. Proceedings of the AAAI Conference on Artiﬁcial Intelligence , 36(8):8892–8900, Jun. 2022. doi: 10.1609/aaai.v36i8.20871. URL https://ojs.aaai.org/index.php/AAAI/article/view/20871. Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. Advances in Neural Information Processing Systems , 33:5812–5823, 2020. Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning au- tomated. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Con- ference on Machine Learning , volume 139 of Proceedings of Machine Learning Research , pp. 12121–12132. PMLR, 18–24 Jul 2021. Shuo Yu, Huafei Huang, Minh N. Dao, and Feng Xia. Graph augmentation learning. In Companion Proceedings of the Web Conference 2022, WWW ’22, pp. 1063–1072, New York, NY , USA, 2022. Association for Computing Machinery. ISBN 9781450391306. doi: 10.1145/3487553.3524718. URL https://doi.org/10.1145/3487553.3524718. Hao Yuan, Jiliang Tang, Xia Hu, and Shuiwang Ji. XGNN: Towards Model-Level Explanations of Graph Neural Networks , pp. 430–438. Association for Computing Machinery, New York, NY , USA, 2020. ISBN 9781450379984. URL https://doi.org/10.1145/3394486. 3403085. Hao Yuan, Haiyang Yu, Jie Wang, Kang Li, and Shuiwang Ji. On explainability of graph neural networks via subgraph explorations. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 12241–12252. PMLR, 18–24 Jul 2021. Han Yue, Chunhui Zhang, Chuxu Zhang, and Hongfu Liu. Label-invariant augmentation for semi-supervised graph classiﬁcation. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems , 2022. URL https://openreview.net/forum?id=rg_yN3HpCp. Xinyu Zhang, Qiang Wang, Jian Zhang, and Zhao Zhong. Adversarial AutoAugment. In Interna- tional Conference on Learning Representations , 2020. URL https://openreview.net/ forum?id=ByxdUySKvS. 13Published as a conference paper at ICLR 2023 Tong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, and Neil Shah. Data augmentation for graph neural networks. Proceedings of the AAAI Conference on Artiﬁcial In- telligence, 35(12):11015–11023, May 2021. doi: 10.1609/aaai.v35i12.17315. URL https: //ojs.aaai.org/index.php/AAAI/article/view/17315. Tong Zhao, Gang Liu, Stephan G ¨unneman, and Meng Jiang. Graph data augmentation for graph machine learning: A survey. arXiv preprint arXiv:2202.08871, 2022. Jiajun Zhou, Jie Shen, and Qi Xuan. Data Augmentation for Graph Classiﬁcation, pp. 2341–2344. Association for Computing Machinery, New York, NY , USA, 2020a. ISBN 9781450368599. URL https://doi.org/10.1145/3340531.3412086. Jiajun Zhou, Jie Shen, Shanqing Yu, Guanrong Chen, and Qi Xuan. M-Evolve: Structural-mapping- based data augmentation for graph classiﬁcation. IEEE Transactions on Network Science and Engineering, 8(1):190–200, 2020b. Yanqiao Zhu, Yichen Xu, Qiang Liu, and Shu Wu. An empirical study of graph contrastive learning. In Thirty-ﬁfth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021a. URL https://openreview.net/forum?id=UuUbIYnHKO. Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning with adaptive augmentation. In Proceedings of the Web Conference 2021, WWW ’21, pp. 2069–2080, New York, NY , USA, 2021b. Association for Computing Machinery. ISBN 9781450383127. doi: 10.1145/3442381.3449802. URL https://doi.org/10.1145/ 3442381.3449802. 14Published as a conference paper at ICLR 2023 A V ISUALIZATION OF DIFFERENT AUGMENTATION METHODS (a) An illustration of a data sam- ple from the TRIANGLES dataset. Red nodes represent the nodes be- longing to triangles. The label of this data sample is 4 since there are four triangles. Training without any augmentations on the TRIAN- GLES dataset achieves the average testing accuracy of 0.506 ±0.006. (b) The data sample generated by augmenting the data sample in (a) with the uniform DropNode transformation. Note that two nodes originally belonging to tri- angles are removed, and the label is changed to 1. Training with the uniform DropNode transformation achieves the average testing accu- racy of 0.473 ± 0.006. (c) The data sample generated by augmenting the data sample in (a) with the label-invariant DropN- ode transformation (the DropNode with GT method in Appendix F.3), which intentionally avoids drop- ping nodes in triangles. Training with this label-invariant augmen- tation achieves the average testing accuracy of 0.522 ± 0.007. Figure 3: Comparison of different augmentation methods on the TRIANGLES dataset. We use a GIN (Xu et al., 2019) based classiﬁcation model to evaluate different augmentation methods, and report the average accuracy and standard deviation over ten runs on a ﬁxed train/validation/test split. In (a), we show a graph data sample with 4 triangles. In (b) and (c), we the data samples generated by augmenting the data sample in (a) with two different augmentation methods. We can clearly ﬁnd that using the uniform DropNode transformation degrades the classiﬁcation performance but using the label-invariant augmentation improves the performance. 15Published as a conference paper at ICLR 2023 B A UGMENTATION AND TRAINING ALGORITHMS Algorithm 1: Augmentation Algorithm of GraphAug 1: Input: Graph G0 = (V0,E0,X0); total number of augmentation steps T; the augmentation model gcomposed of GNN-encoder, GRU, and four MLP models MLPC, MLPM, MLPD, MLPP 2: Initialize the hidden state q0 of the GRU model to zero vector 3: for t= 1to T do 4: Obtain G′ t−1 by adding a virtual node to Gt−1 5: evirtual t−1 ,{ev t−1 : v∈Vt−1}= GNN-encoder(G′ t−1) 6: qt = GRU(qt−1,evirtual t−1 ) 7: pC t = MLPC(qt) 8: Sample ct from the categorical distribution of pC t 9: if ct is MaskNF then 10: for v∈Vt−1 do 11: pM t,v = MLPM(ev t−1) 12: Sample oM t,v from the Bernoulli distribution parameterized with pM t,v 13: for k= 1to ddo 14: Set the k-th node feature of vto zero if oM t,v[k] = 1 15: else if ct is DropNode then 16: for v∈Vt−1 do 17: pD t,v = MLPD(ev t−1) 18: Sample oD t,v from the Bernoulli distribution parameterized with pD t,v 19: Drop the node vfrom Vt−1 if oD t,v = 1 20: else if ct is PerturbEdge then 21: Obtain the addable edge set Et−1 by randomly sampling at most |Et−1|addable edges from {(u,v) :u,v ∈Vt−1,(u,v) /∈Et−1} 22: for (u,v) ∈Et−1 do 23: pP t,(u,v) = MLPP ( [eu t−1 + ev t−1,1] ) 24: Sample oP t,(u,v) from the Bernoulli distribution parameterized with pP t,(u,v) 25: Drop (u,v) from Et−1 if oP t,(u,v) = 1 26: for (u,v) ∈Et−1 do 27: pP t,(u,v) = MLPP ( [eu t−1 + ev t−1,0] ) 28: Sample oP t,(u,v) from the Bernoulli distribution parameterized with pP t,(u,v) 29: Add (u,v) into Et−1 if oP t,(u,v) = 1 30: Set Gt as the outputted graph from the t-th augmentation step 31: Output GT 16Published as a conference paper at ICLR 2023 Algorithm 2: Training Algorithm of the reward generation model of GraphAug 1: Input: Graph dataset D; batch size B; learning rate α; the reward generation model swith the parameter ϕ 2: repeat 3: Sample a batch Gof Bdata samples from D 4: L= 0 5: for G∈G do 6: Randomly sample a graph G+ with the same label as Gfrom Gand a graph G−with different label as G 7: L= L−log s(G,G+) −log(1 −s(G,G−)) 8: Update the parameter ϕof sas ϕ= ϕ−α∇ϕL/B 9: until the training converges 10: Output the trained reward generation model s 17Published as a conference paper at ICLR 2023 Algorithm 3: Training Algorithm of the augmentation model of GraphAug 1: Input: Graph dataset D; batch size B; learning rate α; total number of augmentation steps T; the augmentation model gwith the parameter θcomposed of GNN-encoder, GRU, and four MLP models MLPC, MLPM, MLPD, MLPP; the trained reward generation model s 2: repeat 3: Sample a batch Gof Bdata samples from D 4: ˆgθ = 0 5: for G∈G do 6: Set G0 = (V0,E0,X0) as G 7: Initialize the hidden state q0 of the GRU model to zero vector 8: for t= 1to T do 9: Obtain G′ t−1 by adding a virtual node to Gt−1 10: evirtual t−1 ,{ev t−1 : v∈Vt−1}= GNN-encoder(G′ t−1) 11: qt = GRU(qt−1,evirtual t−1 ) 12: pC t = MLPC(qt) 13: Sample ct from the categorical distribution of pC t , set log p(at) = logpC t (ct) 14: if ct is MaskNF then 15: for v∈Vt−1 do 16: pM t,v = MLPM(ev t−1) 17: Sample oM t,v from the Bernoulli distribution parameterized with pM t,v 18: for k= 1to ddo 19: log p(at) = logp(at) +oM t,v[k] logpM t,v[k] + (1−oM t,v[k]) log(1−pM t,v[k]) 20: Set the k-th node feature of vto zero if oM t,v[k] = 1 21: else if ct is DropNode then 22: for v∈Vt−1 do 23: pD t,v = MLPD(ev t−1) 24: Sample oD t,v from the Bernoulli distribution parameterized with pD t,v 25: log p(at) = logp(at) +oD t,vlog pD t,v + (1−oD t,v) log(1−pD t,v) 26: Drop the node vfrom Vt−1 if oD t,v = 1 27: else if ct is PerturbEdge then 28: Obtain the addable edge set Et−1 by randomly sampling at most |Et−1|addable edges from {(u,v) :u,v ∈Vt−1,(u,v) /∈Et−1} 29: for (u,v) ∈Et−1 do 30: pP t,(u,v) = MLPP ( [eu t−1 + ev t−1,1] ) 31: Sample oP t,(u,v) from the Bernoulli distribution parameterized with pP t,(u,v) 32: log p(at) = logp(at) +oP t,(u,v) log pP t,(u,v) + (1−oP t,(u,v)) log(1−pP t,(u,v)) 33: Drop (u,v) from Et−1 if oP t,(u,v) = 1 34: for (u,v) ∈Et−1 do 35: pP t,(u,v) = MLPP ( [eu t−1 + ev t−1,0] ) 36: Sample oP t,(u,v) from the Bernoulli distribution parameterized with pP t,(u,v) 37: log p(at) = logp(at) +oP t,(u,v) log pP t,(u,v) + (1−oP t,(u,v)) log(1−pP t,(u,v)) 38: Add (u,v) into Et−1 if oP t,(u,v) = 1 39: Set Gt as the outputted graph from the t-th augmentation step 40: ˆgθ = ˆgθ + logs(G0,GT)∇θ ∑T t=1 log p(at) 41: Update the parameter θof gas θ= θ+ αˆgθ/B. 42: until the training converges 43: Output the trained augmentation model g 18Published as a conference paper at ICLR 2023 C D ETAILS OF REWARD GENERATION MODEL We use the graph matching network (Li et al., 2019) as the reward generation modelsto predict the probability s(G0,GT) that G0 and GT have the same label (here G0 is a graph sampled from the dataset, i.e., the starting graph of the sequential augmentation process, andGT is the graph produced from T steps of augmentation by the augmentation model). The graph matching network takes both G0 = (V0,E0,X0) and GT = (VT,ET,XT) as input, performs multiple message operations on them with a shared GNN model separately. The computational process of the message passing for any node vin G0 at the ℓ-th layer of the model is hℓ v = UPDATE ( hℓ−1 v ,AGG ({ mℓ jv : j ∈N(v) }) ,µGT v ) , (7) which is the same as the message passing of vanilla GNNs in Equation (1) other than involving propagating the messageµGT v from the graph GT to the node vin G0. The message µGT v is extracted by an attention based module as wiv = exp ( sim ( hℓ−1 v ,hℓ−1 i )) ∑ u∈VT exp ( sim ( hℓ−1v ,hℓ−1u )),µGT v = ∑ i∈VT wiv(hℓ−1 v −hℓ−1 i ),v ∈V0, (8) where sim(·,·) computes the similarity between two vectors by dot-product. The message passing for any node inGT is similarly computed as in Equation (7), and this also involves propagating mes- sage from G0 to nodes in GT with the attention module in Equation (8). Afterwards, the graph-level representations hG0 and hGT of G0 and GT are separately obtained from their node embeddings as in Equation (2). We pass |hG0 −hGT |, the element-wise absolute deviation of hG0 and hGT , to an MLP model to compute s(G0,GT). 19Published as a conference paper at ICLR 2023 D M ORE DISCUSSIONS Discussions about computational cost. Considering a graph with|V|nodes and |E|edges, the time complexity of performing our deﬁned DropNode or MaskNF transformation on it isO(|V|), and the time complexity is O(|E|) for the PerturbEdge transformation since both edge dropping or addition is operated on at most |E|edges. Hence, the time complexity of augmenting the graph forT steps is O(T|V|+ T|E|). This time cost is affordable for most real-world applications. We test the average time used to augment a graph on each benchmark dataset used in our experiments with our trained augmentation model, see Table 10 for time results. We can ﬁnd that for most dataset, our method only takes a very small amount of time ( < 0.05 s) to augment a graph in average. Besides, during the training of the augmentation model, the computation of rewards by the reward generation model involves attention module (see Equation (9)), which causes an extra computational cost ofO(|V|2). In practice, this does not have much effect on small graphs, but may lead to large computation and memory cost on large graphs. Discussions about augmentation step number. For the number of augmentation steps T, we do not let the model to learn or decide T itself but make T a ﬁxed hyper-parameter to avoid the model being stucked in the naive solution of not doing augmentation at all (i.e., learnT = 0). This strategy is also adopted by previous image augmentation method (e.g. AutoAugment (Cubuk et al., 2019)). A larger T encourages the model to produce more diverse augmentations but makes it harder to keep label-invariance. We experimentally ﬁnd that if T ≥8, it is hard to obtain sufﬁciently high reward for the model. Hence, we tune T in [1,8] for each dataset to achieve the best trade-off between producing diverse augmentations and keeping label-invariance. Discussions about pre-training reward generation model. In our method, before training the augmentation model, we ﬁrst pre-train the reward generation model and make it ﬁxed while training the augmentation model. Such a training pipeline has both advantages and disadvantages. The ad- vantages of using the ﬁxed/pre-trained reward generation model are two-fold. (1) First, pre-training the reward generation model enables it to accurately predict whether two input graphs have the same labels or not, so that the generated reward signals can provide accurate feedback for the augmen- tation model. (2) Second, using the ﬁxed reward generation model can stabilize the training of the augmentation model in practice. As we shown in Appendix F.2, if the reward generation model is not ﬁxed and jointly trained with the augmentation model, the training becomes unstable and mod- els consistently diverge. The disadvantage of pre-training the reward generation model is that this training pipeline is time-consuming, because we have to train two models every time to obtain the ﬁnally usable graph augmenter. Limitations of our method. There are some limitations in our method. (1) First, our method adopts a complicated two-step training pipeline which ﬁrst trains the reward generation model and then trains the augmentation model. We have tried simplifying it to one-step training through adversarial training as in Ratner et al. (2017b). However, we found it to be very unstable and the augmenta- tion model consistently diverges (see Appendix F.2 for an exploration experiment about adversarial training on the COLORS and TRIANGLES dataset). We leave the problem of simplifying the train- ing to the future. (2) Second, our augmentation method will take extra computational cost in both training the augmentation model and providing augmented samples for the downstream graph clas- siﬁcation training. The time and resource cost of training models can be large on the large datasets. For instance, on the ogbg-molhiv dataset, we ﬁnd it takes the total time of around 10 hours to train the reward generation model and augmentation model before we obtain a ﬁnally usable graph aug- menter. Given that the performance improvement is not signiﬁcant on the ogbg-molhiv dataset, such a large time cost is not a worthwhile investment. Our GraphAug mainly targets on improving the graph classiﬁcation performance by generating more training data samples for the tasks with small datasets, particularly for those that need huge cost to manually collect and label new data samples. But for the classiﬁcation task with sufﬁcient training data, the beneﬁts of using GraphAug are limited and not worth the large time and resource cost to train GraphAug models. Relations with automated image augmentations. GraphAug are somehow similar to some auto- mated image augmentations (Cubuk et al., 2019; Zhang et al., 2020) in that they both use sequential augmentation and reinforcement learning based training. However, they are actually fundamen- tally different. Label-invariance is not a problem in automated image augmentations because the used image transformations ensure label-invariance. On the other hand, as discussed in Section 3.2, 20Published as a conference paper at ICLR 2023 it is non-trivial to make graph transformations ensure label-invariance. In GraphAug, the learn- able graph transformation model and the reinforcement learning based training are used to produce label-invariant augmentations, which are actually the main contribution of GraphAug. Another fundamental difference between GraphAug and automated image augmentations lies in the reward design. Many automated image augmentation methods, such as AutoAugment (Cubuk et al., 2019), train a child network model on the training data and use the achieved classiﬁcation accuracy on the validation data as the reward. Instead, our GraphAug uses the label-invariance probability predicted by the reward generation model as the reward signal to train the augmentation model. We argue that such reward design has several advantages over using the classiﬁcation accuracy as the reward. (1) First, maximizing the label-invariance probability can directly encourage the augmentation model to produce label-invariant augmentations. However, the classiﬁcation accuracy is not directly related to label-invariance, so using it as the reward feedback does not necessarily make the augmentation model learn to ensure label-invariance. (2) Second, predicting the label-invariance probability only needs one simple model inference process that is computationally cheap, while obtaining the clas- siﬁcation accuracy is computationally expensive because it needs to train a model from scratch. (3) Most importantly, our reward generation scheme facilitates the learning of the augmentation model by providing the reward feedback for every individual graph . Even in the same dataset, the label-related structures or patterns in different graphs may vary a lot, hence, good augmentation strategies for different graphs can be different. However, the classiﬁcation accuracy evaluates the classiﬁcation performance when using the produced graph augmentations to train models on the overall dataset, which does not provide any feedback about whether the produced augmentation on every individual graph sample is good or not. Differently, the label-invariance probability is com- puted for every individual graph sample, thereby enabling the model to capture good augmentation strategies for every individual graph. Considering these advantages, we do not use the classiﬁcation accuracy but take the label-invariance probability predicted by the reward generation model as the reward. Overall, GraphAug cannot be considered as a simple extension of automated image augmentations to graphs. Relation with prior graph augmentation methods. In addition to GLA (Yue et al., 2022), we also notice Graphair (Ling et al., 2023), another recently proposed automated graph augmentation method. However, Graphair aims to produce fairness-aware graphs for fair graph representation learning, while our method is proposed for graph classiﬁcation. Additionally, graph mixup methods (Wang et al., 2021; Han et al., 2022; Guo & Mao, 2021; Park et al., 2022) synthesize a new graph or graph representation from two input graphs. Because the new data sample is assigned with the combination of labels of two input graphs, mixup operations are supposed to detect and mix the label-related information of two graphs (Guo & Mao, 2021). However, our method is simpler and more intuitive because it only needs to detect and preserve the label-related information of one in- put graph. In addition, another method FLAG (Kong et al., 2022) can only augment node features, while our method can produce augmentations in node features, nodes and edges. Besides, similar to the motivation of our GraphAug, some other studies have also found that preserving important structures or node features is signiﬁcant in designing effective graph augmentations. A pioneering method in this direction is GCA (Zhu et al., 2021b), which proposes to identify important edges and node features in the graph by node centralities. GCA augments the graph by random edge dropping and node feature masking, but assigns lower perturbation probabilities to the identiﬁed important edges and node features. Also, other studies (Wang et al., 2020; Bicciato & Torsello, 2022; Zhou et al., 2020b) assume that some motif or subgraph structures in the graph is signiﬁcant, and propose to augment graphs by manually designed transformations to avoid removing them. Overall, these augmentations are based on some rules or assumptions about how to preserve important structures of the input graph. Differently, our GraphAug method does not aim to deﬁne a ﬁxed graph aug- mentation strategy for every graph. Instead, it seeks to make the augmentation model ﬁnd good augmentation strategies automatically with reinforcement learning based training. Relations with graph explainability. Our method is related to graph explainability in that the predicted transformation probabilities from our augmentation model g is similar to explainability scores of some graph explainability methods (Maruhashi et al., 2018; Yuan et al., 2020; 2021). Hence, we hope that our augmentation method can bring inspiration to researchers in the graph explainability area. 21Published as a conference paper at ICLR 2023 Table 4: Statistics of graph benchmark datasets. Datasets # graphs Average # nodes Average # edges # classes PROTEINS 1113 39.06 72.82 2 IMDB-BINARY 1000 19.77 96.53 2 COLLAB 5000 74.49 2457.78 3 MUTAG 188 17.93 19.79 2 NCI109 4127 29.68 32.13 2 NCI1 4110 29.87 32.30 2 ogbg-molhiv 41,127 25.5 27.5 2 Table 5: Some hyper-parameters for the reward generation model and its training. Datasets # layers batch size # training epochs PROTEINS 6 32 420 IMDB-BINARY 6 32 320 COLLAB 5 8 120 MUTAG 5 32 230 NCI109 5 32 200 NCI1 5 32 200 ogbg-molhiv 5 32 200 E M ORE DETAILS ABOUT EXPERIMENTAL SETTING E.1 E XPERIMENTS ON SYNTHETIC GRAPH DATASETS Data information. We synthesize the COLORS and TRIANGLES dataset by running the open sourced data synthesis code of Knyazev et al. (2019). For the COLORS dataset, we synthesize 8000 graphs for training, 1000 graphs for validation, and 1000 graphs for testing. For the TRIANGLES dataset, we synthesize 30000 graphs for training, 5000 graphs for validation, and 5000 graphs for testing. The labels of all data samples in both datasets belong to {1,..., 10}. Details of the model and training. The Adam optimizer (Kingma & Ba, 2015) is used for the training of all models. For both datasets, we use a reward generation model with 5 layers and the hidden size of 256, and the graph level embedding is obtained by sum pooling. It is trained for 1 epoch on the COLORS dataset and 200 epochs on the TRIANGLES dataset. The batch size is 32 and the learning rate is 0.0001. For the augmentation model, we use a GIN model with 3 layers and the hidden size of 64 for GNN encoder, an MLP model with 2 layers, the hidden size of 64, and ReLU as the non-linear activation function for MLP C, and an MLP model with 2 layers, the hidden size of 128, and ReLU as the non-linear activation function for MLPM, MLPD, and MLPP. The augmentation model is trained for 5 epochs with the batch size of 32 and the learning rate of 0.0001 on both datasets. To stabilize the training of the augmentation model, we manually control the augmentation model to only modify 5% of graph elements at each augmentation step during the training. On the COLORS dataset, we use a classiﬁcation model where the number of layers is 3, the hidden size is 128, and the readout layer is max pooling. On the TRIANGLES dataset, we use a classiﬁcation model where the number of layers is 3, the hidden size is 64, and the readout layer is sum pooling. On both datasets, we set the training batch size as 32 and the learning rate as 0.001 when training classiﬁcation models, and all classiﬁcation models are trained for 100 epochs. E.2 E XPERIMENTS ON GRAPH BENCHMARK DATASETS Data information. We use six datasets from the TUDatasets benchmark (Morris et al., 2020), in- cluding three molecule datasets MUTAG, NCI109, NCI1, one bioinformatics dataset PROTEINS, and two social network datasets IMDB-BINARY and COLLAB. We also use the ogbg-molhiv 22Published as a conference paper at ICLR 2023 Table 6: Some hyper-parameters for the augmentation model and its training. Datasets # augmentation steps T batch size # training epochs PROTEINS 2 32 30 IMDB-BINARY 8 32 30 COLLAB 8 32 10 MUTAG 4 16 200 NCI109 2 32 20 NCI1 2 32 20 ogbg-molhiv 2 128 10 Table 7: Some hyper-parameters for the classiﬁcation model and its training. Datasets # layers hidden size batch size PROTEINS 3 128 32 IMDB-BINARY 4 128 32 COLLAB 4 64 32 MUTAG 4 128 16 NCI109 4 128 32 NCI1 3 128 32 ogbg-molhiv 5 300 32 dataset from the OGB benchmark (Hu et al., 2020). See Table 4 for the detailed statistics of all benchmark datasets used in our experiments. Details of model and training. The Adam optimizer (Kingma & Ba, 2015) is used for training of all models. For all six datasets, we set the hidden size as 256 and the readout layer as sum pooling for the reward generation model, and the reward generation model is trained using 0.0001 as the learning rate. See other hyper-parameters about the reward generation model and its training in Table 5. The hyper-parameters of the augmentation model is the same as those in experiments of synthetic graph datasets and the learning rate is 0.0001 during its training, but we tune the batch size, the training epochs and the number of augmentation steps T on each dataset. See Table 6 for the optimal values of them on each dataset. The strategy of modifying only 5% of graph elements is also used during the training of augmentation models. Besides, for classiﬁcation models, we set the readout layer as mean pooling, and tune the number of layers, the hidden size, and the training batch size. See Table 7 for these hyper-parameters. All classiﬁcation models are trained for 100 epochs with the learning rate of 0.001. 23Published as a conference paper at ICLR 2023 Figure 4: The changing curves of training and validation loss on the COLORS and TRIANGLES datasets when training the reward generation model of GraphAug with Algorithm 2. The results are averaged over ten runs, and the shadow shows the standard deviation. Figure 5: The changing curves of training and validation rewards on the COLORS and TRIANGLES datasets when training the augmentation model of GraphAug with Algorithm 3. The results are averaged over ten runs, and the shadow shows the standard deviation. Figure 6: The changing curves of training rewards of augmentation model and training loss of reward generation model when training two models together with adversarial learning on the COLORS dataset. The results are averaged over ten runs, and the shadow shows the standard deviation. F M ORE EXPERIMENTAL RESULTS F.1 S TUDY OF TRAINING STABILITY AND GENERALIZATION Taking the COLORS and TRIANGLES datasets as examples, we show the learning curves of reward generation models and augmentation models in Figure 4 and Figure 5, respectively. The learning curves on the training set show that the training is generally very stable for both reward generation models and augmentation models since no sharp oscillation happens. Comparing the learning curves on the training and validation set, we can ﬁnd that on the COLORS dataset, the curves converge to around the same loss and rewards on the training and validation set when the training converges. Hence, reward generation model and the augmentation model both have very good generalization abilities. Differently, on more complicated TRIANGLES dataset, slight overﬁtting exists for both models but the overall generalization ability is still acceptable. Actually, to eliminatee the negative effect of overﬁtting, we always take the reward generation model with lowest validation loss and the augmentation model with highest validation reward in our experiments. In a word, our studies about training stability and generalization show that both the reward generation model and augmentation model can be trained stably and have acceptable generalization ability. 24Published as a conference paper at ICLR 2023 Table 8: The testing accuracy on the COLORS and TRIANGLES datasets with the GIN model. We report the average accuracy and standard deviation over ten runs on ﬁxed train/validation/test splits. Dataset No augmentation MaskNF with GT DropNode with GT PerturbEdge with GT GraphAug COLORS 0.578±0.012 0.627±0.013 0.627 ±0.017 n/a 0.633±0.009 TRIANGLES 0.506±0.006 n/a 0.522 ±0.007 0.524 ±0.006 0.513±0.006 F.2 A DVERSARIAL TRAINING EXPERIMENT One possible one-stage training alternative of GraphAug is the adversarial training strategy in Ratner et al. (2017b). Speciﬁcally, the augmentation model is trained jointly with the reward generation model. We construct the positive graph pair (G,G+) sampled from the dataset in which G and G+ have the same label, and use the augmentation model to augment the graph Gto G−and form the negative graph pair (G,G−). The reward generation model is then trained to minimize the loss function L= −log s(G,G+) −log (1−s(G,G−)), but the augmentation model is trained to maximize the reward log s(G,G−) received from the reward generation model. In this adversarial training method, the reward generation model can actually be considered as the discriminator model. We conduct an exploration experiment of it on the COLORS and TRIANGLES datasets, but we ﬁnd that this strategy cannot work well on both datasets. On the TRIANGLES dataset, gradient explosion consistently happens during the training but we have not yet ﬁgure out how to ﬁx it. On the COLORS dataset, we show the learning curves of two models in Figure 6. Note that different from the learning curves of GraphAug in Figure 2, the augmentation model diverges and fails to learn to obtain more rewards as the training proceeds. In other words, the augmentation model struggles to learn to generate new graphs that can deceive the reward generation model. Given these existing problems in adversarial learning, we adopts the two-stage training pipeline in GraphAug and leaves the problem of simplifying the training to the future. F.3 C OMPARISON WITH MANUALLY DESIGNED LABEL -INVARIANT AUGMENTATIONS An interesting question is how does our GraphAug compare with the manually designed label- invariant augmentation methods (assuming we can design them from some domain knowledge)? We try answering this question by empirical studies on COLORS and TRIANGLES datasets. Since we explicitly know how the labels of graphs are obtained from their data generation codes, we can design some label-invariant augmentation strategies. We compare GraphAug with three designed label-invariant augmentation methods, which are based on MaskNF, DropNode, and PerturbEdge transformations intentionally avoiding damaging label-related information. Speciﬁcally, for the COLORS dataset, we compare with MaskNF that uniformly masks the node features other than the color feature, and DropNode that uniformly drops the nodes other than green nodes. In other words, they are exactly using the ground truth labels indicating which graph elements are label-related in- formation, so we call them as MaskNF with GT and DropNode with GT. Note that no PerturbEdge with GT is deﬁned on the COLORS dataset because the modiﬁcation of edges naturally ensures label-invariance. Similarly, for the TRIANGLES dataset, we compare with DropNode with GT and PerturbEdge with GT which intentionally avoid damaging any nodes or edges in triangles. The performance of no augmentation baseline, three manually designed augmentation methods, and our GraphAug method is summarized in Table 8. It is not surprising that all augmentation meth- ods can outperform no augmentation baseline since they all can produce label-invariant training samples. Interestingly, GraphAug is a competitive method compared with these manually designed label-invariant methods. GraphAug outperforms manually designed augmentations on the COLORS dataset but fails to do it on the TRIANGLES dataset. We ﬁnd that is because GraphAug model se- lects MaskNF with higher chances than DropNode and PerturbEdge, but graph classiﬁcation models beneﬁts more from diverse topology structures produced by DropNode and PerturbEdge transforma- tions. Note that although our GraphAug may not show signiﬁcant advantages over manually designed label-invariant augmentations on these two synthetic datasets, in most scenarios, de- signing such label-invariant augmentations is impossible because we do not know which graph 25Published as a conference paper at ICLR 2023 Table 9: The performance on seven benchmark datasets with the GCN model. We report the aver- age ROC-AUC and standard deviation over ten runs for the ogbg-molhiv dataset, and the average accuracy and standard deviations over three 10-fold cross-validation runs for the other datasets. Method PROTEINS IMDB-BINARY COLLAB MUTAG NCI109 NCI1 ogbg-molhiv No augmentation0.711±0.003 0.734±0.010 0.797±0.002 0.803±0.016 0.742±0.004 0.731±0.002 0.761±0.010 Uniform MaskNF0.716±0.001 0.723±0.006 0.802±0.002 0.765±0.017 0.734±0.005 0.729±0.004 0.745±0.011 Uniform DropNode0.714±0.005 0.733±0.001 0.798±0.002 0.759±0.007 0.727±0.003 0.722±0.003 0.723±0.012 Uniform PerturbEdge0.694±0.003 0.732±0.010 0.795±0.003 0.744±0.004 0.634±0.006 0.638±0.011 0.746±0.013 Uniform Mixture0.714±0.003 0.734±0.009 0.797±0.004 0.754±0.015 0.731±0.002 0.722±0.002 0.743±0.011 DropEdge 0.710±0.006 0.735±0.013 0.797±0.004 0.762±0.003 0.724±0.004 0.723±0.003 0.757±0.012 M-Mixup 0.714±0.004 0.728±0.007 0.794±0.003 0.783±0.007 0.739±0.005 0.741±0.002 0.753±0.014 G-Mixup 0.724±0.006 0.749±0.010 0.800±0.027 0.799±0.004 0.509±0.005 0.506±0.005 0.763±0.008 FLAG 0.723±0.003 0.743±0.008 0.797±0.002 0.819±0.004 0.746±0.003 0.734±0.004 0.768±0.010 JOAOv2 0.722±0.003 0.687±0.010 0.681±0.004 0.736±0.007 0.691±0.007 0.672±0.004 0.722±0.009 AD-GCL 0.691±0.011 0.697±0.011 0.612±0.004 0.665±0.001 0.634±0.003 0.641±0.004 0.752±0.013 AutoGCL 0.668±0.008 0.719±0.002 0.745±0.002 0.769±0.022 0.707±0.002 0.714±0.005 0.701±0.014 GraphAug 0.736±0.007 0.764±0.008 0.808±0.001 0.832±0.005 0.760±0.003 0.748±0.002 0.774±0.010 Table 10: Average augmentation time per graph with the trained augmentation model. Method PROTEINS IMDB-BINARY COLLAB MUTAG NCI109 NCI1 ogbg-molhiv JOAOv2 0.0323s 0.0854s 0.2846s 0.0397s 0.0208s 0.0223s 0.0299s AD-GCL 0.0127s 0.0418s 0.1478s 0.0169s 0.0092s 0.0083s 0.0115s AutoGCL 0.0218s 0.0643s 0.2398s 0.0256s 0.0162s 0.0168s 0.0221s GraphAug 0.0073s 0.0339s 0.1097s 0.0136s 0.0075s 0.0078s 0.0106s elements are label-related. However, our GraphAug can still work in these scenarios because it can automatically learn to produce label-invariant augmentations. F.4 M ORE EXPERIMENTAL RESULTS ON GRAPH BENCHMARK DATASETS The performance of different augmentation methods on all seven datasets with the GCN model is presented in Table 9. Besides, to quantify and compare the computational cost of our method and some automated graph augmentation baseline methods on each dataset, we test the average time they use to augment each graph and summarize the average augmentation time results in Table 10. For most dataset, our method only takes a very small amount of time (<0.05s) to augment a graph in average, which is an acceptable time cost for most real-world applications. In addition, from Table 10, we can clearly ﬁnd that among all automated graph augmentation methods, our GraphAug takes the least average runtime to augment graphs. For the other baseline methods in Table 2, because they do not need the computation with neural networks in augmentations, their runtime is unsurprisingly lower ( < 0.001s per graph). However, the classiﬁcation performance of them is consistently worse than our GraphAug. Overall, our GraphAug achieves the best classiﬁcation performance, and its time cost is the lowest among all automated graph augmentations. F.5 M ORE ABLATION STUDIES Ablation on combining three different transformations. In our method, we use a combination of three different graph transformations, including MaskNF, DropNode, and PerturbEdge. Our GraphAug model are designed to automatically select one of them at each augmentation step. Here we explore how the performance will change if only one category of graph transformation is used. Speciﬁcally, we compare with three variants of GraphAug that only uses learnable MaskNF, DropN- ode, and PerturbEdge, whose performance are listed in the ﬁrst three rows of Table 11. We can ﬁnd that sometimes using a certain category of learnable augmentation gives very good results, e.g., learnable DropNode on the NCI1 dataset. However, not all categories can achieve it, and actually the optimal category varies among datasets because graph structure distributions or modalities are very 26Published as a conference paper at ICLR 2023 Table 11: Results of ablation studies about combining three different transformations. We report the average accuracy and standard deviation over three 10-fold cross-validation runs with the GIN model. Method PROTEINS IMDB-BINARY NCI1 GraphAug with only learnable MaskNF transformation0.712±0.001 0.751 ±0.002 0.809 ±0.002 GraphAug with only learnable DropNode transformation0.716±0.003 0.752 ±0.005 0.814 ±0.002 GraphAug with only learnable PerturbEdge transformation0.702±0.009 0.754 ±0.005 0.780 ±0.001 GraphAug 0.722±0.004 0.762 ±0.004 0.816 ±0.001 Table 12: Results of ablation studies about using virtual nodes. We report the average accuracy and standard deviation over three 10-fold cross-validation runs with the GIN model. Method PROTEINS IMDB-BINARY NCI1 GraphAug with sum pooling 0.711±0.005 0.750 ±0.008 0.788 ±0.004 GraphAug with mean pooling 0.711±0.004 0.752 ±0.004 0.801 ±0.005 GraphAug with max pooling 0.713±0.002 0.737 ±0.005 0.795 ±0.005 GraphAug with virtual nodes 0.722±0.004 0.762 ±0.004 0.816 ±0.001 Table 13: The label-invariance ratios on the test sets of COLORS and TRIANGLES datasets. Dataset Uniform MaskNF Uniform DropNode Uniform PerturbEdge Uniform Mixture GraphAug COLORS 0.3547 0.3560 1.0000 0.5645 0.9994 TRIANGLES 1.0000 0.6674 0.1957 0.6181 1.0000 different in different datasets. Nonetheless, GraphAug can consistently achieve good performance without manually searching the optimal category on different datasets. Hence, combining different transformations makes it easier for the GraphAug model to adapt to different graph datasets than using only one category of transformation. Ablation on using virtual nodes. In our method, virtual nodes are used to capture graph-level representation and predict augmentation categories due to two advantages. (1) First, in the message passing process of GNNs, virtual nodes can help propagate messages among far-away nodes in the graph. (2) Second, virtual nodes can learn to more effectively capture graph-level representations through aggregating more information from important nodes or structures in the graph (similar to the attention mechanism). In fact, many prior studies (Gilmer et al., 2017; Hu et al., 2020) have demonstrated the advantages of using virtual nodes in graph representation learning. To justify the advantages of using virtual nodes in GraphAug, we compare the performance of taking different ways to predict the augmentation category in an ablation experiment. Speciﬁcally, we evaluate the performance of GraphAug model variants in which virtual nodes are not used, but the augmentation category is predicted from the graph-level representations obtained by sum pooling, mean pooling, or max pooling. The results of them are summarized in the ﬁrst three rows of Table 12. From the results, we can ﬁnd that using virtual nodes achieves the best performance, hence it is the best option. F.6 E VALUATION OF LABEL -INVARIANCE PROPERTY We evaluate the label-invariance ratios of our GraphAug method and the baseline methods used in Table 1 on the test sets of two synthetic datasets. The results are summarized in Table 13. Since the label is deﬁned as the number of nodes with green colors (indicated by node features) in the COLORS dataset, Uniform DropNode and Uniform PerturbEdge will destroy label-related informa- tion and achieve a very low label-invariance ratio. Similarly, the label is deﬁned as the number of 3-cycles in the TRIANGLES dataset, Uniform DropNode and Uniform PerturbEdge also achieve a 27Published as a conference paper at ICLR 2023 very low label-invariance ratio. However, our GraphAug can achieve a very high label-invariance ratio of close to 1.0 on both datasets. Besides, combining with the classiﬁcation performance in Table 1, we can ﬁnd that only the augmentations with high label-invariance ratios can outperform no augmentation baseline. This phenomenon demonstrates that label-invariance is signiﬁcant to achieve effective graph augmentations. 28",
      "references": [
        "GAMS: Graph augmentation with module swapping",
        "Learning phrase representations using RNN encoder–decoder for statistical machine translation",
        "AutoAugment: Learning augmentation strategies from data",
        "Data augmentation for deep graph learning: A survey",
        "Graph U-Nets",
        "Neural message passing for quantum chemistry",
        "Intrusion-free graph mixup",
        "Inductive representation learning on large graphs",
        "G-Mixup: Graph data augmentation for graph classiﬁcation",
        "Learning graph augmentations to learn graph representations",
        "Faster AutoAugment: Learning augmentation strategies using backpropagation",
        "Population based augmentation: Efﬁcient learning of augmentation policy schedules",
        "Long short-term memory",
        "Open graph benchmark: Datasets for machine learning on graphs",
        "Adam: A method for stochastic optimization",
        "Semi-supervised classiﬁcation with graph convolutional net- works",
        "Understanding attention and generalization in graph neural networks",
        "FLAG: Adversarial data augmentation for graph neural networks",
        "ImageNet classiﬁcation with deep con- volutional neural networks",
        "Differentiable automatic data augmentation",
        "Graph matching net- works for learning the similarity of graph structured objects",
        "Fast AutoAugment",
        "Learning fair graph represen- tations via automated data augmentations",
        "DIG: A turnkey library for diving into graph deep learning research",
        "Spherical message passing for 3D molecular graphs",
        "Learning multi-way relations via tensor decomposition with neural networks",
        "TUDataset: A collection of benchmark datasets for learning with graphs",
        "Graph Transplant: Node saliency-guided graph mixup with local structure preservation",
        "Data augmentation with Snorkel",
        "Learning to compose domain-speciﬁc transformations for data augmentation",
        "Improving deep learning with generic data augmentation",
        "Augmentations in graph contrastive learning: Current methodological ﬂaws & towards better practices",
        "Graph attention networks",
        "ComENet: Towards complete and efﬁcient message passing for 3d molecular graphs",
        "Hierarchical protein representations via complete 3d graph networks",
        "GraphCrop: Subgraph cropping for graph classiﬁcation",
        "Mixup for node and graph classiﬁcation",
        "Advanced graph and sequence neural net- works for molecular property prediction and drug discovery",
        "Self-supervised learning of graph neural networks: A uniﬁed review",
        "How powerful are graph neural networks?",
        "Periodic graph transformers for crys- tal material property prediction",
        "AutoGCL: Automated graph contrastive learning via learnable view generators",
        "Graph contrastive learning with augmentations",
        "Graph contrastive learning au- tomated",
        "Graph augmentation learning",
        "XGNN: Towards Model-Level Explanations of Graph Neural Networks",
        "On explainability of graph neural networks via subgraph explorations",
        "Label-invariant augmentation for semi-supervised graph classiﬁcation",
        "Adversarial AutoAugment",
        "Data augmentation for graph neural networks",
        "Graph data augmentation for graph machine learning: A survey",
        "Data Augmentation for Graph Classiﬁcation",
        "M-Evolve: Structural-mapping- based data augmentation for graph classiﬁcation",
        "An empirical study of graph contrastive learning",
        "Graph contrastive learning with adaptive augmentation"
      ],
      "meta_data": {
        "arxiv_id": "2202.13248v4",
        "authors": [
          "Youzhi Luo",
          "Michael McThrow",
          "Wing Yee Au",
          "Tao Komikado",
          "Kanji Uchino",
          "Koji Maruhashi",
          "Shuiwang Ji"
        ],
        "published_date": "2022-02-26T23:00:34Z",
        "github_url": "https://github.com/bknyaz/graph_attention_pool"
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces GraphAug, the first automated data augmentation framework tailored for graph classification. The method focuses on generating label‐invariant graph augmentations by automating augmentation category selection and designing non-uniform transformations that preserve critical label-related information in graphs.",
        "methodology": "GraphAug treats graph augmentation as a sequential transformation process, where a learnable model consisting of a GNN encoder, a GRU for predicting augmentation categories, and MLPs for computing transformation probabilities is used. It applies three types of transformations (node feature masking, node dropping, and edge perturbation) on graph elements. The augmentation model is trained using reinforcement learning by maximizing an estimated label-invariance probability provided by a graph matching network based reward generation model.",
        "experimental_setup": "Experiments were conducted on synthetic datasets (COLORS and TRIANGLES) where label invariance is critical, as well as on real-world benchmark datasets from TU-Datasets (PROTEINS, IMDB-BINARY, COLLAB, MUTAG, NCI109, NCI1) and ogbg-molhiv. The setup involves comparing GraphAug with uniform transformation baselines, state-of-the-art graph mixup methods, and automated augmentation techniques designed for self-supervised learning. Evaluation metrics included classification accuracy, ROC-AUC, and label-invariance ratios. Ablation studies were also performed to analyze the importance of sequential augmentation, learnable transformation, and use of virtual nodes.",
        "limitations": "The method uses a complex two-stage training pipeline where the reward generation model is pre-trained and then fixed, which adds time overhead. Although effective for scenarios with limited data, the computational cost may be prohibitive for larger datasets. Additionally, the current reinforcement learning scheme could become unstable if integrated with adversarial training, and the significance of improvements may vary depending on the dataset size and graph structure.",
        "future_research_directions": "Future work includes simplifying the training procedure (e.g., exploring stable one-step or adversarial learning alternatives), extending the approach to other graph-related problems such as node classification, and improving scalability and efficiency for larger graphs and more computationally intensive tasks.",
        "experimental_code": "# File: chebygin.py\nimport numpy as np\nimport torch\nimport torch.sparse\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\nfrom attention_pooling import *\nfrom utils import *\n\nclass ChebyGINLayer(nn.Module):\n    \"\"\"\n    General Graph Neural Network layer that depending on arguments can be:\n    1. Graph Convolution Layer (T. Kipf and M. Welling, ICLR 2017)\n    2. Chebyshev Graph Convolution Layer (M. Defferrard et al., NeurIPS 2017)\n    3. GIN Layer (K. Xu et al., ICLR 2019)\n    4. ChebyGIN Layer (B. Knyazev et al., ICLR 2019 Workshop on Representation Learning on Graphs and Manifolds)\n    The first three types (1-3) of layers are particular cases of the fourth (4) case.\n    \"\"\"\n\n    def __init__(self, in_features, out_features, K, n_hidden=0, aggregation='mean', activation=nn.ReLU(True), n_relations=1):\n        super(ChebyGINLayer, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.n_relations = n_relations\n        assert K > 0, 'order is assumed to be > 0'\n        self.K = K\n        assert n_hidden >= 0, ('invalid n_hidden value', n_hidden)\n        self.n_hidden = n_hidden\n        assert aggregation in ['mean', 'sum'], ('invalid aggregation', aggregation)\n        self.aggregation = aggregation\n        self.activation = activation\n        n_in = self.in_features * self.K * n_relations\n        if self.n_hidden == 0:\n            fc = [nn.Linear(n_in, self.out_features)]\n        else:\n            fc = [nn.Linear(n_in, n_hidden), nn.ReLU(True), nn.Linear(n_hidden, self.out_features)]\n        if activation is not None:\n            fc.append(activation)\n        self.fc = nn.Sequential(*fc)\n        print('ChebyGINLayer', list(self.fc.children())[0].weight.shape, torch.norm(list(self.fc.children())[0].weight, dim=1)[:10])\n\n    def __repr__(self):\n        return 'ChebyGINLayer(in_features={}, out_features={}, K={}, n_hidden={}, aggregation={})\\nfc={}'.format(self.in_features, self.out_features, self.K, self.n_hidden, self.aggregation, str(self.fc))\n\n    def chebyshev_basis(self, L, X, K):\n        \"\"\"\n        Return T_k X where T_k are the Chebyshev polynomials of order up to K.\n        :param L: graph Laplacian, batch (B), nodes (N), nodes (N)\n        :param X: input of size batch (B), nodes (N), features (F)\n        :param K: Chebyshev polynomial order, i.e. filter size (number of hopes)\n        :return: Tensor of size (B,N,K,F) as a result of multiplying T_k(L) by X for each order\n        \"\"\"\n        if K > 1:\n            Xt = [X]\n            Xt.append(torch.bmm(L, X))\n            for k in range(2, K):\n                Xt.append(2 * torch.bmm(L, Xt[k - 1]) - Xt[k - 2])\n            Xt = torch.stack(Xt, 2)\n            return Xt\n        else:\n            assert K == 1, K\n            return torch.bmm(L, X).unsqueeze(2)\n\n    def laplacian_batch(self, A, add_identity=False):\n        \"\"\"\n        Computes normalized Laplacian transformed so that its eigenvalues are in range [-1, 1].\n        Note that sum of all eigenvalues = trace(L) = 0.\n        :param A: Tensor of size (B,N,N) containing batch (B) of adjacency matrices of shape N,N\n        :return: Normalized Laplacian of size (B,N,N)\n        \"\"\"\n        B, N = A.shape[:2]\n        if add_identity:\n            A = A + torch.eye(N, device=A.get_device() if A.is_cuda else 'cpu').unsqueeze(0)\n        D = torch.sum(A, 1)\n        D_hat = (D + 1e-05) ** (-0.5)\n        L = D_hat.view(B, N, 1) * A * D_hat.view(B, 1, N)\n        if not add_identity:\n            L = -L\n        return (D, L)\n\n    def forward(self, data):\n        x, A, mask = data[:3]\n        B, N, F = x.shape\n        assert N == A.shape[1] == A.shape[2], ('invalid shape', N, x.shape, A.shape)\n        if len(A.shape) == 3:\n            A = A.unsqueeze(3)\n        y_out = []\n        for rel in range(A.shape[3]):\n            D, L = self.laplacian_batch(A[:, :, :, rel], add_identity=self.K == 1)\n            y = self.chebyshev_basis(L, x, self.K)\n            if self.aggregation == 'sum':\n                if self.K == 1:\n                    y = y * D.view(B, N, 1, 1)\n                else:\n                    D_GIN = torch.ones(B, N, self.K, device=x.get_device() if x.is_cuda else 'cpu')\n                    D_GIN[:, :, 1:] = D.view(B, N, 1).expand(-1, -1, self.K - 1)\n                    y = y * D_GIN.view(B, N, self.K, 1)\n            y_out.append(y)\n        y = torch.cat(y_out, dim=2)\n        y = self.fc(y.view(B, N, -1))\n        if len(mask.shape) == 2:\n            mask = mask.unsqueeze(2)\n        y = y * mask.float()\n        output = [y, A, mask]\n        output.extend(data[3:] + [x])\n        return output\n\nclass GraphReadout(nn.Module):\n    \"\"\"\n    Global pooling layer applied after the last graph layer.\n    \"\"\"\n\n    def __init__(self, pool_type):\n        super(GraphReadout, self).__init__()\n        self.pool_type = pool_type\n        dim = 1\n        if pool_type == 'max':\n            self.readout_layer = lambda x, mask: torch.max(x, dim=dim)[0]\n        elif pool_type in ['avg', 'mean']:\n            self.readout_layer = lambda x, mask: torch.sum(x, dim=dim) / torch.sum(mask, dim=dim).float()\n        elif pool_type in ['sum']:\n            self.readout_layer = lambda x, mask: torch.sum(x, dim=dim)\n        else:\n            raise NotImplementedError(pool_type)\n\n    def __repr__(self):\n        return 'GraphReadout({})'.format(self.pool_type)\n\n    def forward(self, data):\n        x, A, mask = data[:3]\n        B, N = x.shape[:2]\n        x = self.readout_layer(x, mask.view(B, N, 1))\n        output = [x]\n        output.extend(data[1:])\n        return output\n\nclass ChebyGIN(nn.Module):\n    \"\"\"\n    Graph Neural Network class.\n    \"\"\"\n\n    def __init__(self, in_features, out_features, filters, K=1, n_hidden=0, aggregation='mean', dropout=0, readout='max', pool=None, pool_arch='fc_prev'.split('_'), large_graph=False, kl_weight=None, graph_layer_fn=None, init='normal', scale=None, debug=False):\n        super(ChebyGIN, self).__init__()\n        self.out_features = out_features\n        assert len(filters) > 0, 'filters must be an iterable object with at least one element'\n        assert K > 0, 'filter scale must be a positive integer'\n        self.pool = pool\n        self.pool_arch = pool_arch\n        self.debug = debug\n        n_prev = None\n        attn_gnn = None\n        if graph_layer_fn is None:\n            graph_layer_fn = lambda n_in, n_out, K_, n_hidden_, activation: ChebyGINLayer(in_features=n_in, out_features=n_out, K=K_, n_hidden=n_hidden_, aggregation=aggregation, activation=activation)\n            if self.pool_arch is not None and self.pool_arch[0] == 'gnn':\n                attn_gnn = lambda n_in: ChebyGIN(in_features=n_in, out_features=0, filters=[32, 32, 1], K=np.min((K, 2)), n_hidden=0, graph_layer_fn=graph_layer_fn)\n        graph_layers = []\n        for layer, f in enumerate(filters + [None]):\n            n_in = in_features if layer == 0 else filters[layer - 1]\n            if self.pool is not None and len(self.pool) > len(filters) + layer and (self.pool[layer + 3] != 'skip'):\n                graph_layers.append(AttentionPooling(in_features=n_in, in_features_prev=n_prev, pool_type=self.pool[:3] + [self.pool[layer + 3]], pool_arch=self.pool_arch, large_graph=large_graph, kl_weight=kl_weight, attn_gnn=attn_gnn, init=init, scale=scale, debug=debug))\n            if f is not None:\n                graph_layers.append(graph_layer_fn(n_in, f, K, n_hidden, None if self.out_features == 0 and layer == len(filters) - 1 else nn.ReLU(True)))\n                n_prev = n_in\n        if self.out_features > 0:\n            graph_layers.append(GraphReadout(readout))\n        self.graph_layers = nn.Sequential(*graph_layers)\n        if self.out_features > 0:\n            self.fc = nn.Sequential(*([nn.Dropout(p=dropout)] if dropout > 0 else []) + [nn.Linear(filters[-1], out_features)])\n\n    def forward(self, data):\n        data = self.graph_layers(data)\n        if self.out_features > 0:\n            y = self.fc(data[0])\n        else:\n            y = data[0]\n        return (y, data[4])\n\n# File: attention_pooling.py\nimport numpy as np\nimport torch\nimport torch.sparse\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom utils import *\n\nclass AttentionPooling(nn.Module):\n    \"\"\"\n    Graph pooling layer implementing top-k and threshold-based pooling.\n    \"\"\"\n\n    def __init__(self, in_features, in_features_prev, pool_type, pool_arch, large_graph, attn_gnn=None, kl_weight=None, drop_nodes=True, init='normal', scale=None, debug=False):\n        super(AttentionPooling, self).__init__()\n        self.pool_type = pool_type\n        self.pool_arch = pool_arch\n        self.large_graph = large_graph\n        self.kl_weight = kl_weight\n        self.proj = None\n        self.drop_nodes = drop_nodes\n        self.is_topk = self.pool_type[2].lower() == 'topk'\n        self.scale = scale\n        self.init = init\n        self.debug = debug\n        self.clamp_value = 60\n        self.torch = torch.__version__\n        if self.is_topk:\n            self.topk_ratio = float(self.pool_type[3])\n            assert self.topk_ratio > 0 and self.topk_ratio <= 1, ('invalid top-k ratio', self.topk_ratio, self.pool_type)\n        else:\n            self.threshold = float(self.pool_type[3])\n            assert self.threshold >= 0 and self.threshold <= 1, ('invalid pooling threshold', self.threshold, self.pool_type)\n        if self.pool_type[1] in ['unsup', 'sup']:\n            assert self.pool_arch not in [None, 'None'], self.pool_arch\n            n_in = in_features_prev if self.pool_arch[1] == 'prev' else in_features\n            if self.pool_arch[0] == 'fc':\n                p_optimal = torch.from_numpy(np.pad(np.array([0, 1]), (0, n_in - 2), 'constant')).float().view(1, n_in)\n                if len(self.pool_arch) == 2:\n                    self.proj = nn.Linear(n_in, 1, bias=False)\n                    p = self.proj.weight.data\n                    if scale is not None:\n                        if init == 'normal':\n                            p = torch.randn(n_in)\n                        elif init == 'uniform':\n                            p = torch.rand(n_in) * 2 - 1\n                        else:\n                            raise NotImplementedError(init)\n                        p *= scale\n                    else:\n                        print('Default PyTorch init is used for layer %s, std=%.3f' % (str(p.shape), p.std()))\n                    self.proj.weight.data = p.view_as(self.proj.weight.data)\n                    p = self.proj.weight.data.view(1, n_in)\n                else:\n                    filters = list(map(int, self.pool_arch[2:]))\n                    self.proj = []\n                    for layer in range(len(filters)):\n                        self.proj.append(nn.Linear(in_features=n_in if layer == 0 else filters[layer - 1], out_features=filters[layer]))\n                        if layer == 0:\n                            p = self.proj[0].weight.data\n                            if scale is not None:\n                                if init == 'normal':\n                                    p = torch.randn(filters[layer], n_in)\n                                elif init == 'uniform':\n                                    p = torch.rand(filters[layer], n_in) * 2 - 1\n                                else:\n                                    raise NotImplementedError(init)\n                                p *= scale\n                            else:\n                                print('Default PyTorch init is used for layer %s, std=%.3f' % (str(p.shape), p.std()))\n                            self.proj[0].weight.data = p.view_as(self.proj[0].weight.data)\n                            p = self.proj[0].weight.data.view(-1, n_in)\n                            self.proj.append(nn.ReLU(True))\n                    self.proj.append(nn.Linear(filters[-1], 1))\n                    self.proj = nn.Sequential(*self.proj)\n                cos_sim = self.cosine_sim(p[:, :-1], p_optimal[:, :-1])\n                if p.shape[0] == 1:\n                    print('p values', p[0].data.cpu().numpy())\n                    print('cos_sim', cos_sim.item())\n                else:\n                    for fn in [torch.max, torch.min, torch.mean, torch.std]:\n                        print('cos_sim', fn(cos_sim).item())\n            elif self.pool_arch[0] == 'gnn':\n                self.proj = attn_gnn(n_in)\n            else:\n                raise ValueError('invalid pooling layer architecture', self.pool_arch)\n        elif self.pool_type[1] == 'gt':\n            if not self.is_topk and self.threshold > 0:\n                print('For ground truth attention threshold should be 0, but it is %f' % self.threshold)\n        else:\n            raise NotImplementedError(self.pool_type[1])\n\n    def __repr__(self):\n        return 'AttentionPooling(pool_type={}, pool_arch={}, topk={}, kl_weight={}, init={}, scale={}, proj={})'.format(self.pool_type, self.pool_arch, self.is_topk, self.kl_weight, self.init, self.scale, self.proj)\n\n    def cosine_sim(self, a, b):\n        return torch.mm(a, b.t()) / (torch.norm(a, dim=1, keepdim=True) * torch.norm(b, dim=1, keepdim=True))\n\n    def mask_out(self, x, mask):\n        return x.view_as(mask) * mask\n\n    def drop_nodes_edges(self, x, A, mask):\n        N_nodes = torch.sum(mask, dim=1).long()\n        N_nodes_max = N_nodes.max()\n        idx = None\n        if N_nodes_max > 0:\n            B, N, C = x.shape\n            mask, idx = torch.topk(mask.byte(), N_nodes_max, dim=1, largest=True, sorted=False)\n            x = torch.gather(x, dim=1, index=idx.unsqueeze(2).expand(-1, -1, C))\n            A = torch.gather(A, dim=1, index=idx.unsqueeze(2).expand(-1, -1, N))\n            A = torch.gather(A, dim=2, index=idx.unsqueeze(1).expand(-1, N_nodes_max, -1))\n        return (x, A, mask, N_nodes, idx)\n\n    def forward(self, data):\n        KL_loss = None\n        x, A, mask, _, params_dict = data[:5]\n        mask_float = mask.float()\n        N_nodes_float = params_dict['N_nodes'].float()\n        B, N, C = x.shape\n        A = A.view(B, N, N)\n        alpha_gt = None\n        if 'node_attn' in params_dict:\n            if not isinstance(params_dict['node_attn'], list):\n                params_dict['node_attn'] = [params_dict['node_attn']]\n            alpha_gt = params_dict['node_attn'][-1].view(B, N)\n        if 'node_attn_eval' in params_dict:\n            if not isinstance(params_dict['node_attn_eval'], list):\n                params_dict['node_attn_eval'] = [params_dict['node_attn_eval']]\n        if (self.pool_type[1] == 'gt' or (self.pool_type[1] == 'sup' and self.training)) and alpha_gt is None:\n            raise ValueError('ground truth node attention values node_attn required for %s' % self.pool_type)\n        if self.pool_type[1] in ['unsup', 'sup']:\n            attn_input = data[-1] if self.pool_arch[1] == 'prev' else x.clone()\n            if self.pool_arch[0] == 'fc':\n                alpha_pre = self.proj(attn_input).view(B, N)\n            else:\n                input = [attn_input]\n                input.extend(data[1:])\n                alpha_pre = self.proj(input)[0].view(B, N)\n            alpha_pre = torch.clamp(alpha_pre, -self.clamp_value, self.clamp_value)\n            alpha = normalize_batch(self.mask_out(torch.exp(alpha_pre), mask_float).view(B, N))\n            if self.pool_type[1] == 'sup' and self.training:\n                if self.torch.find('1.') == 0:\n                    KL_loss_per_node = self.mask_out(F.kl_div(torch.log(alpha + 1e-14), alpha_gt, reduction='none'), mask_float.view(B, N))\n                else:\n                    KL_loss_per_node = self.mask_out(F.kl_div(torch.log(alpha + 1e-14), alpha_gt, reduce=False), mask_float.view(B, N))\n                KL_loss = self.kl_weight * torch.mean(KL_loss_per_node.sum(dim=1) / (N_nodes_float + 1e-07))\n        else:\n            alpha = alpha_gt\n        x = x * alpha.view(B, N, 1)\n        if self.large_graph:\n            x = x * N_nodes_float.view(B, 1, 1)\n        if self.is_topk:\n            N_remove = torch.round(N_nodes_float * (1 - self.topk_ratio)).long()\n            idx = torch.sort(alpha, dim=1, descending=False)[1]\n            mask = mask.clone().view(B, N)\n            for b in range(B):\n                idx_b = idx[b, mask[b, idx[b]]]\n                mask[b, idx_b[:N_remove[b]]] = 0\n        else:\n            mask = (mask & (alpha.view_as(mask) > self.threshold)).view(B, N)\n        if self.drop_nodes:\n            x, A, mask, N_nodes_pooled, idx = self.drop_nodes_edges(x, A, mask)\n            if idx is not None and 'node_attn' in params_dict:\n                params_dict['node_attn'].append(normalize_batch(self.mask_out(torch.gather(alpha_gt, dim=1, index=idx), mask.float())))\n            if idx is not None and 'node_attn_eval' in params_dict:\n                params_dict['node_attn_eval'].append(normalize_batch(self.mask_out(torch.gather(params_dict['node_attn_eval'][-1], dim=1, index=idx), mask.float())))\n        else:\n            N_nodes_pooled = torch.sum(mask, dim=1).long()\n            if 'node_attn' in params_dict:\n                params_dict['node_attn'].append(self.mask_out(params_dict['node_attn'][-1], mask.float()))\n            if 'node_attn_eval' in params_dict:\n                params_dict['node_attn_eval'].append(self.mask_out(params_dict['node_attn_eval'][-1], mask.float()))\n        params_dict['N_nodes'] = N_nodes_pooled\n        mask_matrix = mask.unsqueeze(2) & mask.unsqueeze(1)\n        A = A * mask_matrix.float()\n        if KL_loss is not None:\n            if 'reg' not in params_dict:\n                params_dict['reg'] = []\n            params_dict['reg'].append(KL_loss)\n        for key, value in zip(['alpha', 'mask'], [alpha, mask]):\n            if key not in params_dict:\n                params_dict[key] = []\n            params_dict[key].append(value.detach())\n        if self.debug and alpha_gt is not None:\n            idx_correct_pool = alpha_gt > 0\n            idx_correct_drop = alpha_gt == 0\n            alpha_correct_pool = alpha[idx_correct_pool].sum() / N_nodes_float.sum()\n            alpha_correct_drop = alpha[idx_correct_drop].sum() / N_nodes_float.sum()\n            ratio_avg = (N_nodes_pooled.float() / N_nodes_float).mean()\n            for key, values in zip(['alpha_correct_pool_debug', 'alpha_correct_drop_debug', 'ratio_avg_debug'], [alpha_correct_pool, alpha_correct_drop, ratio_avg]):\n                if key not in params_dict:\n                    params_dict[key] = []\n                params_dict[key].append(values.detach())\n        output = [x, A, mask]\n        output.extend(data[3:])\n        return output\n\n# File: train_test.py\nimport time\nimport torch\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom chebygin import *\nfrom utils import *\nfrom graphdata import *\nimport torch.multiprocessing as mp\nimport multiprocessing\ntry:\n    import ax\n    from ax.service.managed_loop import optimize\nexcept Exception as e:\n    print('AX is not available: %s' % str(e))\n\ndef set_pool(pool_thresh, args_pool):\n    pool = copy.deepcopy(args_pool)\n    for i, s in enumerate(pool):\n        try:\n            thresh = float(s)\n            pool[i] = str(pool_thresh)\n        except:\n            continue\n    return pool\n\ndef train_evaluate(datareader, args, collate_fn, loss_fn, feature_stats, parameterization, folds=10, threads=5):\n    print('parameterization', parameterization)\n    pool_thresh, kl_weight = (parameterization['pool'], parameterization['kl_weight'])\n    pool = args.pool\n    if args.tune_init:\n        scale, init = (parameterization['scale'], parameterization['init'])\n    else:\n        scale, init = (args.scale, args.init)\n    n_hidden_attn, layer = (parameterization['n_hidden_attn'], 1)\n    if layer == 0:\n        pool = copy.deepcopy(args.pool)\n        del pool[3]\n    pool = set_pool(pool_thresh, pool)\n    manager = multiprocessing.Manager()\n    val_acc = manager.dict()\n    assert threads <= folds, (threads, folds)\n    n_it = int(np.ceil(float(folds) / threads))\n    for i in range(n_it):\n        processes = []\n        if threads <= 1:\n            single_job(i * threads, datareader, args, collate_fn, loss_fn, pool, kl_weight, feature_stats, val_acc, scale=scale, init=init, n_hidden_attn=n_hidden_attn)\n        else:\n            for fold in range(threads):\n                p = mp.Process(target=single_job, args=(i * threads + fold, datareader, args, collate_fn, loss_fn, pool, kl_weight, feature_stats, val_acc, scale, init, n_hidden_attn))\n                p.start()\n                processes.append(p)\n            for p in processes:\n                p.join()\n    print(val_acc)\n    val_acc = list(val_acc.values())\n    print('average and std over {} folds: {} +- {}'.format(folds, np.mean(val_acc), np.std(val_acc)))\n    metric = np.mean(val_acc) - np.std(val_acc)\n    print('metric: avg acc - std: {}'.format(metric))\n    return metric\n\ndef ax_optimize(datareader, args, collate_fn, loss_fn, feature_stats, folds=10, threads=5, n_trials=30):\n    parameters = [{'name': 'pool', 'type': 'range', 'bounds': [0.0001, 0.02], 'log_scale': False}, {'name': 'kl_weight', 'type': 'range', 'bounds': [0.1, 10.0], 'log_scale': False}, {'name': 'n_hidden_attn', 'type': 'choice', 'values': [0, 32]}]\n    if args.tune_init:\n        parameters.extend([{'name': 'scale', 'type': 'range', 'bounds': [0.1, 2.0], 'log_scale': False}, {'name': 'init', 'type': 'choice', 'values': ['normal', 'uniform']}])\n    best_parameters, values, experiment, model = optimize(parameters=parameters, evaluation_function=lambda parameterization: train_evaluate(datareader, args, collate_fn, loss_fn, feature_stats, parameterization, folds=folds, threads=threads), total_trials=n_trials, objective_name='accuracy')\n    print('best_parameters', best_parameters)\n    print('values', values)\n    return best_parameters\n\ndef train(model, train_loader, optimizer, epoch, args, loss_fn, feature_stats=None, log=True):\n    model.train()\n    optimizer.zero_grad()\n    n_samples, correct, train_loss = (0, 0, 0)\n    alpha_pred, alpha_GT = ({}, {})\n    start = time.time()\n    for batch_idx, data in enumerate(train_loader):\n        data = data_to_device(data, args.device)\n        if feature_stats is not None:\n            data[0] = (data[0] - feature_stats[0]) / feature_stats[1]\n        if batch_idx == 0 and epoch <= 1:\n            sanity_check(model.eval(), data)\n            model.train()\n        optimizer.zero_grad()\n        mask = [data[2].view(len(data[2]), -1)]\n        output, other_outputs = model(data)\n        other_losses = other_outputs['reg'] if 'reg' in other_outputs else []\n        alpha = other_outputs['alpha'] if 'alpha' in other_outputs else []\n        mask.extend(other_outputs['mask'] if 'mask' in other_outputs else [])\n        targets = data[3]\n        loss = loss_fn(output, targets)\n        for l in other_losses:\n            loss += l\n        loss_item = loss.item()\n        train_loss += loss_item\n        n_samples += len(targets)\n        loss.backward()\n        optimizer.step()\n        time_iter = time.time() - start\n        correct += count_correct(output.detach(), targets.detach())\n        update_attn(data, alpha, alpha_pred, alpha_GT, mask)\n        acc = 100.0 * correct / n_samples\n        train_loss_avg = train_loss / (batch_idx + 1)\n        if log and (batch_idx > 0 and batch_idx % args.log_interval == 0 or batch_idx == len(train_loader) - 1):\n            print('Train set (epoch {}): [{}/{} ({:.0f}%)]\\tLoss: {:.4f} (avg: {:.4f}), other losses: {}\\tAcc metric: {}/{} ({:.2f}%)\\t AttnAUC: {}\\t avg sec/iter: {:.4f}'.format(epoch, n_samples, len(train_loader.dataset), 100.0 * n_samples / len(train_loader.dataset), loss_item, train_loss_avg, ['%.4f' % l.item() for l in other_losses], correct, n_samples, acc, ['%.2f' % a for a in attn_AUC(alpha_GT, alpha_pred)], time_iter / (batch_idx + 1)))\n    assert n_samples == len(train_loader.dataset), (n_samples, len(train_loader.dataset))\n    return (train_loss, acc)\n\ndef test(model, test_loader, epoch, loss_fn, split, args, feature_stats=None, noises=None, img_noise_level=None, eval_attn=False, alpha_WS_name=''):\n    model.eval()\n    n_samples, correct, test_loss = (0, 0, 0)\n    pred, targets, N_nodes = ([], [], [])\n    start = time.time()\n    alpha_pred, alpha_GT = ({}, {})\n    if eval_attn:\n        alpha_pred[0] = []\n        print('testing with evaluation of attention: takes longer time')\n    if args.debug:\n        debug_data = {}\n    with torch.no_grad():\n        for batch_idx, data in enumerate(test_loader):\n            data = data_to_device(data, args.device)\n            if feature_stats is not None:\n                assert feature_stats[0].shape[2] == feature_stats[1].shape[2] == data[0].shape[2], (feature_stats[0].shape, feature_stats[1].shape, data[0].shape)\n                data[0] = (data[0] - feature_stats[0]) / feature_stats[1]\n            if batch_idx == 0 and epoch <= 1:\n                sanity_check(model, data)\n            if noises is not None:\n                noise = noises[n_samples:n_samples + len(data[0])].to(args.device) * img_noise_level\n                if len(noise.shape) == 2:\n                    noise = noise.unsqueeze(2)\n                data[0][:, :, :3] = data[0][:, :, :3] + noise\n            mask = [data[2].view(len(data[2]), -1)]\n            N_nodes.append(data[4]['N_nodes'].detach())\n            targets.append(data[3].detach())\n            output, other_outputs = model(data)\n            other_losses = other_outputs['reg'] if 'reg' in other_outputs else []\n            alpha = other_outputs['alpha'] if 'alpha' in other_outputs else []\n            mask.extend(other_outputs['mask'] if 'mask' in other_outputs else [])\n            if args.debug:\n                for key in other_outputs:\n                    if key.find('debug') >= 0:\n                        if key not in debug_data:\n                            debug_data[key] = []\n                        debug_data[key].append([d.data.cpu().numpy() for d in other_outputs[key]])\n            if args.torch.find('1.') == 0:\n                loss = loss_fn(output, data[3], reduction='sum')\n            else:\n                loss = loss_fn(output, data[3], reduce=False).sum()\n            for l in other_losses:\n                loss += l\n            test_loss += loss.item()\n            pred.append(output.detach())\n            update_attn(data, alpha, alpha_pred, alpha_GT, mask)\n            if eval_attn:\n                assert len(alpha) == 0, 'invalid mode, eval_attn should be false for this type of pooling'\n                alpha_pred[0].extend(attn_heatmaps(model, args.device, data, output.data, test_loader.batch_size, constant_mask=args.dataset == 'mnist'))\n            n_samples += len(data[0])\n            if eval_attn and (n_samples % 100 == 0 or n_samples == len(test_loader.dataset)):\n                print('{}/{} samples processed'.format(n_samples, len(test_loader.dataset)))\n    assert n_samples == len(test_loader.dataset), (n_samples, len(test_loader.dataset))\n    pred = torch.cat(pred)\n    targets = torch.cat(targets)\n    N_nodes = torch.cat(N_nodes)\n    if args.dataset.find('colors') >= 0:\n        correct = count_correct(pred, targets, N_nodes=N_nodes, N_nodes_min=0, N_nodes_max=25)\n        if pred.shape[0] > 2500:\n            correct += count_correct(pred[2500:5000], targets[2500:5000], N_nodes=N_nodes[2500:5000], N_nodes_min=26, N_nodes_max=200)\n            correct += count_correct(pred[5000:], targets[5000:], N_nodes=N_nodes[5000:], N_nodes_min=26, N_nodes_max=200)\n    elif args.dataset == 'triangles':\n        correct = count_correct(pred, targets, N_nodes=N_nodes, N_nodes_min=0, N_nodes_max=25)\n        if pred.shape[0] > 5000:\n            correct += count_correct(pred, targets, N_nodes=N_nodes, N_nodes_min=26, N_nodes_max=100)\n    else:\n        correct = count_correct(pred, targets, N_nodes=N_nodes, N_nodes_min=0, N_nodes_max=100000.0)\n    time_iter = time.time() - start\n    test_loss_avg = test_loss / n_samples\n    acc = 100.0 * correct / n_samples\n    print('{} set (epoch {}): Avg loss: {:.4f}, Acc metric: {}/{} ({:.2f}%)\\t AttnAUC: {}\\t avg sec/iter: {:.4f}\\n'.format(split.capitalize(), epoch, test_loss_avg, correct, n_samples, acc, ['%.2f' % a for a in attn_AUC(alpha_GT, alpha_pred)], time_iter / (batch_idx + 1)))\n    if args.debug:\n        for key in debug_data:\n            for layer in range(len(debug_data[key][0])):\n                print('{} (layer={}): {:.5f}'.format(key, layer, np.mean([d[layer] for d in debug_data[key]])))\n    if eval_attn:\n        alpha_pred = alpha_pred[0]\n        if args.results in [None, 'None', ''] or alpha_WS_name == '':\n            print('skip saving alpha values, invalid results dir (%s) or alpha_WS_name (%s)' % (args.results, alpha_WS_name))\n        else:\n            file_path = pjoin(args.results, '%s_alpha_WS_%s_seed%d_%s.pkl' % (args.dataset, split, args.seed, alpha_WS_name))\n            if os.path.isfile(file_path):\n                print('WARNING: file %s exists and will be overwritten' % file_path)\n            with open(file_path, 'wb') as f:\n                pickle.dump(alpha_pred, f, protocol=2)\n    return (test_loss, acc, alpha_pred, pred)\n\ndef update_attn(data, alpha, alpha_pred, alpha_GT, mask):\n    key = 'node_attn_eval'\n    for layer in range(len(mask)):\n        mask[layer] = mask[layer].data.cpu().numpy() > 0\n    if key in data[4]:\n        if not isinstance(data[4][key], list):\n            data[4][key] = [data[4][key]]\n        for layer in range(len(data[4][key])):\n            if layer not in alpha_GT:\n                alpha_GT[layer] = []\n            alpha_GT[layer].extend(masked_alpha(data[4][key][layer].data.cpu().numpy(), mask[layer]))\n    for layer in range(len(alpha)):\n        if layer not in alpha_pred:\n            alpha_pred[layer] = []\n        alpha_pred[layer].extend(masked_alpha(alpha[layer].data.cpu().numpy(), mask[layer]))\n\ndef masked_alpha(alpha, mask):\n    alpha_lst = []\n    for i in range(len(alpha)):\n        alpha_lst.append(alpha[i][mask[i]])\n    return alpha_lst\n\ndef attn_heatmaps(model, device, data, output_org, batch_size=1, constant_mask=False):\n    labels = torch.argmax(output_org, dim=1)\n    B, N_nodes_max, C = data[0].shape\n    alpha_WS = []\n    if N_nodes_max > 1000:\n        print('WARNING: graph is too large (%d nodes) and not supported by this function (evaluation will be incorrect for graphs in this batch).' % N_nodes_max)\n        for b in range(B):\n            n = data[2][b].sum().item()\n            alpha_WS.append(np.zeros((1, n)) + 1.0 / n)\n        return alpha_WS\n    if constant_mask:\n        mask = torch.ones(N_nodes_max, N_nodes_max - 1).to(device)\n    node_ids = torch.arange(start=0, end=N_nodes_max, device=device).view(1, -1).repeat(N_nodes_max, 1)\n    node_ids[np.diag_indices(N_nodes_max, 2)] = -1\n    node_ids = node_ids[node_ids >= 0].view(N_nodes_max, N_nodes_max - 1).long()\n    with torch.no_grad():\n        for b in range(B):\n            x = torch.gather(data[0][b].unsqueeze(0).expand(N_nodes_max, -1, -1), dim=1, index=node_ids.unsqueeze(2).expand(-1, -1, C))\n            if not constant_mask:\n                mask = torch.gather(data[2][b].unsqueeze(0).expand(N_nodes_max, -1), dim=1, index=node_ids)\n            A = torch.gather(data[1][b].unsqueeze(0).expand(N_nodes_max, -1, -1), dim=1, index=node_ids.unsqueeze(2).expand(-1, -1, N_nodes_max))\n            A = torch.gather(A, dim=2, index=node_ids.unsqueeze(1).expand(-1, N_nodes_max - 1, -1))\n            output = torch.zeros(N_nodes_max).to(device)\n            n_chunks = int(np.ceil(N_nodes_max / float(batch_size)))\n            for i in range(n_chunks):\n                idx = np.arange(i * batch_size, (i + 1) * batch_size) if i < n_chunks - 1 else np.arange(i * batch_size, N_nodes_max)\n                output[idx] = model([x[idx], A[idx], mask[idx], None, {}])[0][:, labels[b]].data\n            alpha = torch.abs(output - output_org[b, labels[b]]).view(1, N_nodes_max)\n            if not constant_mask:\n                alpha = alpha[data[2][b].view(1, N_nodes_max)]\n            alpha_WS.append(normalize(alpha).data.cpu().numpy())\n    return alpha_WS\n\ndef save_checkpoint(model, scheduler, optimizer, args, epoch):\n    if args.results in [None, 'None']:\n        print('skip saving checkpoint, invalid results dir: %s' % args.results)\n        return\n    file_path = '%s/checkpoint_%s_%s_epoch%d_seed%07d.pth.tar' % (args.results, args.dataset, args.experiment_ID, epoch, args.seed)\n    try:\n        print('saving the model to %s' % file_path)\n        state = {'epoch': epoch, 'args': args, 'state_dict': model.state_dict(), 'scheduler': scheduler.state_dict(), 'optimizer': optimizer.state_dict()}\n        if os.path.isfile(file_path):\n            print('WARNING: file %s exists and will be overwritten' % file_path)\n        torch.save(state, file_path)\n    except Exception as e:\n        print('error saving the model', e)\n\ndef load_checkpoint(model, optimizer, scheduler, file_path):\n    print('loading the model from %s' % file_path)\n    state = torch.load(file_path)\n    model.load_state_dict(state['state_dict'])\n    optimizer.load_state_dict(state['optimizer'])\n    scheduler.load_state_dict(state['scheduler'])\n    print('loading from epoch %d done' % state['epoch'])\n    return state['epoch'] + 1\n\ndef create_model_optimizer(in_features, out_features, pool, kl_weight, args, scale=None, init=None, n_hidden_attn=None):\n    set_seed(args.seed, seed_data=None)\n    model = ChebyGIN(in_features=in_features, out_features=out_features, filters=args.filters, K=args.filter_scale, n_hidden=args.n_hidden, aggregation=args.aggregation, dropout=args.dropout, readout=args.readout, pool=pool, pool_arch=args.pool_arch if n_hidden_attn in [None, 0] else args.pool_arch[:2] + ['%d' % n_hidden_attn], large_graph=args.dataset.lower() == 'mnist', kl_weight=float(kl_weight), init=args.init if init is None else init, scale=args.scale if scale is None else scale, debug=args.debug)\n    print(model)\n    print('model capacity: %d' % np.sum([np.prod(p.size()) if p.requires_grad else 0 for p in model.parameters()]))\n    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wdecay, betas=(0.5, 0.999))\n    scheduler = lr_scheduler.MultiStepLR(optimizer, args.lr_decay_step, gamma=0.1)\n    epoch = 1\n    if args.resume not in [None, 'None']:\n        epoch = load_checkpoint(model, optimizer, scheduler, args.resume)\n        if epoch < args.epochs + 1:\n            print('resuming training for epoch %d' % epoch)\n    model.to(args.device)\n    return (epoch, model, optimizer, scheduler)\n\ndef single_job(fold, datareader, args, collate_fn, loss_fn, pool, kl_weight, feature_stats, val_acc, scale=None, init=None, n_hidden_attn=None):\n    set_seed(args.seed, seed_data=None)\n    wsup = args.pool[1] == 'sup'\n    train_loader = DataLoader(GraphData(datareader, fold, 'train'), batch_size=args.batch_size, shuffle=True, num_workers=args.threads, collate_fn=collate_fn)\n    val_loader = DataLoader(GraphData(datareader, fold, 'val'), batch_size=args.test_batch_size, shuffle=False, num_workers=args.threads, collate_fn=collate_fn)\n    start_epoch, model, optimizer, scheduler = create_model_optimizer(train_loader.dataset.num_features, train_loader.dataset.num_classes, None if wsup else pool, kl_weight, args, scale=scale, init=init, n_hidden_attn=n_hidden_attn)\n    for epoch in range(start_epoch, args.epochs + 1):\n        scheduler.step()\n        train(model, train_loader, optimizer, epoch, args, loss_fn, feature_stats, log=False)\n    if wsup:\n        train_loader_test = DataLoader(GraphData(datareader, fold, 'train'), batch_size=args.test_batch_size, shuffle=False, num_workers=args.threads, collate_fn=collate_fn)\n        train_loss, train_acc, attn_WS = test(model, train_loader_test, epoch, loss_fn, 'train', args, feature_stats, eval_attn=True)[:3]\n        train_loader = DataLoader(GraphData(datareader, fold, 'train', attn_labels=attn_WS), batch_size=args.batch_size, shuffle=True, num_workers=args.threads, collate_fn=collate_fn)\n        val_loader = DataLoader(GraphData(datareader, fold, 'val'), batch_size=args.test_batch_size, shuffle=False, num_workers=args.threads, collate_fn=collate_fn)\n        start_epoch, model, optimizer, scheduler = create_model_optimizer(train_loader.dataset.num_features, train_loader.dataset.num_classes, pool, kl_weight, args, scale=scale, init=init, n_hidden_attn=n_hidden_attn)\n        for epoch in range(start_epoch, args.epochs + 1):\n            scheduler.step()\n            train(model, train_loader, optimizer, epoch, args, loss_fn, feature_stats, log=False)\n    acc = test(model, val_loader, epoch, loss_fn, 'val', args, feature_stats)[1]\n    val_acc[fold] = acc\n\ndef cross_validation(datareader, args, collate_fn, loss_fn, pool, kl_weight, feature_stats, n_hidden_attn=None, folds=10, threads=5):\n    print('%d-fold cross-validation' % folds)\n    manager = multiprocessing.Manager()\n    val_acc = manager.dict()\n    assert threads <= folds, (threads, folds)\n    n_it = int(np.ceil(float(folds) / threads))\n    for i in range(n_it):\n        processes = []\n        if threads <= 1:\n            single_job(i * threads, datareader, args, collate_fn, loss_fn, pool, kl_weight, feature_stats, val_acc, scale=args.scale, init=args.init, n_hidden_attn=n_hidden_attn)\n        else:\n            for fold in range(threads):\n                p = mp.Process(target=single_job, args=(i * threads + fold, datareader, args, collate_fn, loss_fn, pool, kl_weight, feature_stats, val_acc, args.scale, args.init, n_hidden_attn))\n                p.start()\n                processes.append(p)\n            for p in processes:\n                p.join()\n    print(val_acc)\n    val_acc = list(val_acc.values())\n    print('average and std over {} folds: {} +- {}'.format(folds, np.mean(val_acc), np.std(val_acc)))\n    metric = np.mean(val_acc) - np.std(val_acc)\n    print('metric: avg acc - std: {}'.format(metric))\n    return metric\n\n# File: main.py\nimport argparse\nimport random\nimport datetime\nfrom torchvision import transforms\nfrom graphdata import *\nfrom train_test import *\nimport warnings\nwarnings.filterwarnings('once')\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Run experiments with Graph Neural Networks')\n    parser.add_argument('-D', '--dataset', type=str, default='colors-3', choices=['colors-3', 'colors-4', 'colors-8', 'colors-16', 'colors-32', 'triangles', 'mnist', 'mnist-75sp', 'TU'], help='colors-n means the colors dataset with n-dimensional features; TU is any dataset from https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets')\n    parser.add_argument('-d', '--data_dir', type=str, default='./data', help='path to the dataset')\n    parser.add_argument('--epochs', type=int, default=None, help='# of the epochs')\n    parser.add_argument('--batch_size', type=int, default=32, help='batch size for training data')\n    parser.add_argument('--lr', type=float, default=0.001, help='Learning Rate')\n    parser.add_argument('--lr_decay_step', type=str, default=None, help='number of epochs after which to reduce learning rate')\n    parser.add_argument('--wdecay', type=float, default=0.0001, help='weight decay')\n    parser.add_argument('--dropout', type=float, default=0, help='dropout rate')\n    parser.add_argument('-f', '--filters', type=str, default='64,64,64', help='number of filters in each graph layer')\n    parser.add_argument('-K', '--filter_scale', type=int, default=1, help='filter scale (receptive field size), must be > 0; 1 for GCN or GIN')\n    parser.add_argument('--n_hidden', type=int, default=0, help='number of hidden units inside the graph layer')\n    parser.add_argument('--aggregation', type=str, default='mean', choices=['mean', 'sum'], help='neighbors aggregation inside the graph layer')\n    parser.add_argument('--readout', type=str, default=None, choices=['mean', 'sum', 'max'], help='type of global pooling over all nodes')\n    parser.add_argument('--kl_weight', type=float, default=100, help='weight of the KL term in the loss')\n    parser.add_argument('--pool', type=str, default=None, help='type of pooling between layers, None for global pooling only')\n    parser.add_argument('--pool_arch', type=str, default=None, help='pooling layers architecture defining whether to use fully-connected layers or GNN and to which layer to attach (e.g.: fc_prev, gnn_prev, fc_curr, gnn_curr, fc_prev_32)')\n    parser.add_argument('--init', type=str, default='normal', choices=['normal', 'uniform'], help='distribution used for initialization for the attention model')\n    parser.add_argument('--scale', type=str, default='1', help='initialized weights scale for the attention model, set to None to use PyTorch default init')\n    parser.add_argument('--degree_feature', action='store_true', default=False, help='use degree features (only for the Triangles dataset)')\n    parser.add_argument('--n_nodes', type=int, default=25, help='maximum number of nodes in the training set for collab, proteins and dd (35 for collab, 25 for proteins, 200 or 300 for dd)')\n    parser.add_argument('--cv_folds', type=int, default=5, help='number of folds for cross-validating hyperparameters for collab, proteins and dd (5 or 10 shows similar results, 5 is faster)')\n    parser.add_argument('--cv_threads', type=int, default=5, help='number of parallel threads for cross-validation')\n    parser.add_argument('--tune_init', action='store_true', default=False, help='do not tune initialization hyperparameters')\n    parser.add_argument('--ax', action='store_true', default=False, help='use AX for hyperparameter optimization (recommended)')\n    parser.add_argument('--ax_trials', type=int, default=30, help='number of AX trials (hyperparameters optimization steps)')\n    parser.add_argument('--cv', action='store_true', default=False, help='run in the cross-validation mode')\n    parser.add_argument('--seed_data', type=int, default=111, help='random seed for data splits')\n    parser.add_argument('--img_features', type=str, default='mean,coord', help='image features to use as node features')\n    parser.add_argument('--img_noise_levels', type=str, default=None, help='Gaussian noise standard deviations for grayscale and color image features')\n    parser.add_argument('--validation', action='store_true', default=False, help='run in the validation mode')\n    parser.add_argument('--debug', action='store_true', default=False, help='evaluate on the test set after each epoch (only for visualization purposes)')\n    parser.add_argument('--eval_attn_train', action='store_true', default=False, help='evaluate attention and save coefficients on the training set for models without learnable attention')\n    parser.add_argument('--eval_attn_test', action='store_true', default=False, help='evaluate attention and save coefficients on the test set for models without learnable attention')\n    parser.add_argument('--test_batch_size', type=int, default=100, help='batch size for test data')\n    parser.add_argument('--alpha_ws', type=str, default=None, help='attention labels that will be used for (weak)supervision')\n    parser.add_argument('--log_interval', type=int, default=400, help='print interval')\n    parser.add_argument('--results', type=str, default='./results', help='directory to save model checkpoints and other results, set to None to prevent saving anything')\n    parser.add_argument('--resume', type=str, default=None, help='checkpoint to load the model and optimzer states from and continue training')\n    parser.add_argument('--device', type=str, default='cuda', choices=['cuda', 'cpu'], help='cuda/cpu')\n    parser.add_argument('--seed', type=int, default=111, help='random seed for model parameters')\n    parser.add_argument('--threads', type=int, default=0, help='number of threads for data loader')\n    args = parser.parse_args()\n    if args.readout in [None, 'None']:\n        args.readout = 'max'\n    set_default_lr_decay_step = args.lr_decay_step in [None, 'None']\n    if args.epochs in [None, 'None']:\n        if args.dataset.find('mnist') >= 0:\n            args.epochs = 30\n            if set_default_lr_decay_step:\n                args.lr_decay_step = '20,25'\n        elif args.dataset == 'triangles':\n            args.epochs = 100\n            if set_default_lr_decay_step:\n                args.lr_decay_step = '85,95'\n        elif args.dataset == 'TU':\n            args.epochs = 50\n            if set_default_lr_decay_step:\n                args.lr_decay_step = '25,35,45'\n        elif args.dataset.find('color') >= 0:\n            if args.readout in [None, 'None']:\n                args.readout = 'sum'\n            if args.pool in [None, 'None']:\n                args.epochs = 100\n                if set_default_lr_decay_step:\n                    args.lr_decay_step = '90'\n            else:\n                args.epochs = 300\n                if set_default_lr_decay_step:\n                    args.lr_decay_step = '280'\n        else:\n            raise NotImplementedError(args.dataset)\n    args.lr_decay_step = list(map(int, args.lr_decay_step.split(',')))\n    args.filters = list(map(int, args.filters.split(',')))\n    args.img_features = args.img_features.split(',')\n    args.img_noise_levels = None if args.img_noise_levels in [None, 'None'] else list(map(float, args.img_noise_levels.split(',')))\n    args.pool = None if args.pool in [None, 'None'] else args.pool.split('_')\n    args.pool_arch = None if args.pool_arch in [None, 'None'] else args.pool_arch.split('_')\n    try:\n        args.scale = float(args.scale)\n    except:\n        args.scale = None\n    args.torch = torch.__version__\n    for arg in vars(args):\n        print(arg, getattr(args, arg))\n    return args\n\ndef load_synthetic(args):\n    train_dataset = SyntheticGraphs(args.data_dir, args.dataset, 'train', degree_feature=args.degree_feature, attn_coef=args.alpha_ws)\n    test_dataset = SyntheticGraphs(args.data_dir, args.dataset, 'val' if args.validation else 'test', degree_feature=args.degree_feature)\n    loss_fn = mse_loss\n    collate_fn = collate_batch\n    in_features = train_dataset.feature_dim\n    out_features = 1\n    return (train_dataset, test_dataset, loss_fn, collate_fn, in_features, out_features)\n\ndef load_mnist(args):\n    use_mean_px = 'mean' in args.img_features\n    use_coord = 'coord' in args.img_features\n    assert use_mean_px, ('this mode is not well supported', use_mean_px)\n    gt_attn_threshold = 0 if args.pool is not None and args.pool[1] in ['gt'] and (args.filter_scale > 1) else 0.5\n    if args.dataset == 'mnist':\n        train_dataset = MNIST(args.data_dir, train=True, download=True, transform=transforms.ToTensor(), attn_coef=args.alpha_ws)\n    else:\n        train_dataset = MNIST75sp(args.data_dir, split='train', use_mean_px=use_mean_px, use_coord=use_coord, gt_attn_threshold=gt_attn_threshold, attn_coef=args.alpha_ws)\n    noises, color_noises = (None, None)\n    if args.validation:\n        n_val = 5000\n        if args.dataset == 'mnist':\n            train_dataset.train_data = train_dataset.train_data[:-n_val]\n            train_dataset.train_labels = train_dataset.train_labels[:-n_val]\n            test_dataset = MNIST(args.data_dir, train=True, download=True, transform=transforms.ToTensor())\n            test_dataset.train_data = train_dataset.train_data[-n_val:]\n            test_dataset.train_labels = train_dataset.train_labels[-n_val:]\n        else:\n            train_dataset.train_val_split(np.arange(0, train_dataset.n_samples - n_val))\n            test_dataset = MNIST75sp(args.data_dir, split='train', use_mean_px=use_mean_px, use_coord=use_coord, gt_attn_threshold=gt_attn_threshold)\n            test_dataset.train_val_split(np.arange(train_dataset.n_samples - n_val, train_dataset.n_samples))\n    else:\n        noise_file = pjoin(args.data_dir, '%s_noise.pt' % args.dataset.replace('-', '_'))\n        color_noise_file = pjoin(args.data_dir, '%s_color_noise.pt' % args.dataset.replace('-', '_'))\n        if args.dataset == 'mnist':\n            test_dataset = MNIST(args.data_dir, train=False, download=True, transform=transforms.ToTensor())\n            noise_shape = (len(test_dataset.test_labels), 28 * 28)\n        else:\n            test_dataset = MNIST75sp(args.data_dir, split='test', use_mean_px=use_mean_px, use_coord=use_coord, gt_attn_threshold=gt_attn_threshold)\n            noise_shape = (len(test_dataset.labels), 75)\n        noises = load_save_noise(noise_file, noise_shape)\n        color_noises = load_save_noise(color_noise_file, (noise_shape[0], noise_shape[1], 3))\n    if args.dataset == 'mnist':\n        A, coord, mask = precompute_graph_images(train_dataset.train_data.shape[1])\n        collate_fn = lambda batch: collate_batch_images(batch, A, mask, use_mean_px=use_mean_px, coord=coord if use_coord else None, gt_attn_threshold=gt_attn_threshold, replicate_features=args.img_noise_levels is not None)\n    else:\n        train_dataset.precompute_graph_data(replicate_features=args.img_noise_levels is not None, threads=12)\n        test_dataset.precompute_graph_data(replicate_features=args.img_noise_levels is not None, threads=12)\n        collate_fn = collate_batch\n    loss_fn = F.cross_entropy\n    in_features = 0 if args.img_noise_levels is None else 2\n    for features in args.img_features:\n        if features == 'mean':\n            in_features += 1\n        elif features == 'coord':\n            in_features += 2\n        else:\n            raise NotImplementedError(features)\n    in_features = np.max((in_features, 1))\n    out_features = 10\n    return (train_dataset, test_dataset, loss_fn, collate_fn, in_features, out_features, noises, color_noises)\n\ndef load_TU(args, cv_folds=5):\n    loss_fn = F.cross_entropy\n    collate_fn = collate_batch\n    scale, init = (args.scale, args.init)\n    n_hidden_attn = float(args.pool_arch[2]) if args.pool_arch is not None and len(args.pool_arch) > 2 else 0\n    if args.pool is None:\n        datareader = DataReader(data_dir=args.data_dir, N_nodes=args.n_nodes, rnd_state=rnd_data, folds=0)\n        train_dataset = GraphData(datareader, None, 'train_val')\n        test_dataset = GraphData(datareader, None, 'test')\n        in_features = train_dataset.num_features\n        out_features = train_dataset.num_classes\n        pool = args.pool\n        kl_weight = args.kl_weight\n    elif args.pool[1] == 'gt':\n        raise ValueError('ground truth attention for TU datasets is not available')\n    elif args.pool[1] in ['sup', 'unsup']:\n        datareader = DataReader(data_dir=args.data_dir, N_nodes=args.n_nodes, rnd_state=rnd_data, folds=cv_folds)\n        if args.ax:\n            best_parameters = ax_optimize(datareader, args, collate_fn, loss_fn, None, folds=cv_folds, threads=args.cv_threads, n_trials=args.ax_trials)\n            pool = args.pool\n            kl_weight = best_parameters['kl_weight']\n            if args.tune_init:\n                scale, init = (best_parameters['scale'], best_parameters['init'])\n            n_hidden_attn, layer = (best_parameters['n_hidden_attn'], 1)\n            if layer == 0:\n                pool = copy.deepcopy(args.pool)\n                del pool[3]\n            pool = set_pool(best_parameters['pool'], pool)\n        else:\n            if not args.cv:\n                pool_thresh_values = np.array([float(args.pool[-1])])\n                n_hiddens = [n_hidden_attn]\n                layers = [1]\n            elif args.debug:\n                pool_thresh_values = np.array([0.0001, 0.1])\n                n_hiddens = [n_hidden_attn]\n                layers = [1]\n            else:\n                if args.data_dir.lower().find('proteins') >= 0:\n                    pool_thresh_values = np.array([0.002, 0.005, 0.01, 0.03, 0.05])\n                elif args.data_dir.lower().find('dd') >= 0:\n                    pool_thresh_values = np.array([0.0001, 0.001, 0.002, 0.005, 0.01, 0.03, 0.05, 0.1])\n                elif args.data_dir.lower().find('collab') >= 0:\n                    pool_thresh_values = np.array([0.001, 0.002, 0.005, 0.01, 0.03, 0.05, 0.1])\n                else:\n                    raise NotImplementedError('this dataset is not supported currently')\n                n_hiddens = np.array([0, 32])\n                layers = np.array([0, 1])\n            if args.pool[1] == 'sup' and (not args.debug) and args.cv:\n                kl_weight_values = np.array([0.25, 1, 2, 10])\n            else:\n                kl_weight_values = np.array([args.kl_weight])\n            if len(pool_thresh_values) > 1 or len(kl_weight_values) > 1 or len(n_hiddens) > 1 or (len(layers) > 1):\n                val_acc = np.zeros((len(layers), len(n_hiddens), len(pool_thresh_values), len(kl_weight_values)))\n                for i_, layer in enumerate(layers):\n                    if layer == 0:\n                        pool = copy.deepcopy(args.pool)\n                        del pool[3]\n                    else:\n                        pool = args.pool\n                    for j_, n_hidden_attn in enumerate(n_hiddens):\n                        for k_, pool_thresh in enumerate(pool_thresh_values):\n                            for m_, kl_weight in enumerate(kl_weight_values):\n                                val_acc[i_, j_, k_, m_] = cross_validation(datareader, args, collate_fn, loss_fn, set_pool(pool_thresh, pool), kl_weight, None, n_hidden_attn=n_hidden_attn, folds=cv_folds, threads=args.cv_threads)\n                ind1, ind2, ind3, ind4 = np.where(val_acc == np.max(val_acc))\n                print(val_acc)\n                print(ind1, ind2, ind3, ind4, layers[ind1], n_hiddens[ind2], pool_thresh_values[ind3], kl_weight_values[ind4], val_acc[ind1[0], ind2[0], ind3[0], ind4[0]])\n                layer = layers[ind1[0]]\n                if layer == 0:\n                    pool = copy.deepcopy(args.pool)\n                    del pool[3]\n                else:\n                    pool = args.pool\n                n_hidden_attn = n_hiddens[ind2[0]]\n                pool = set_pool(pool_thresh_values[ind3[0]], pool)\n                kl_weight = kl_weight_values[ind4[0]]\n            else:\n                pool = args.pool\n                kl_weight = args.kl_weight\n        train_dataset = GraphData(datareader, None, 'train_val')\n        test_dataset = GraphData(datareader, None, 'test')\n        in_features = train_dataset.num_features\n        out_features = train_dataset.num_classes\n        if args.pool[1] == 'sup':\n            train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.threads, collate_fn=collate_fn)\n            train_loader_test = DataLoader(train_dataset, batch_size=args.test_batch_size, shuffle=False, num_workers=args.threads, collate_fn=collate_fn)\n            start_epoch, model, optimizer, scheduler = create_model_optimizer(in_features, out_features, None, kl_weight, args, scale=scale, init=init, n_hidden_attn=n_hidden_attn)\n            for epoch in range(start_epoch, args.epochs + 1):\n                scheduler.step()\n                train_loss, acc = train(model, train_loader, optimizer, epoch, args, loss_fn, None)\n            train_loss, train_acc, attn_WS = test(model, train_loader_test, epoch, loss_fn, 'train', args, None, eval_attn=True)[:3]\n            train_dataset = GraphData(datareader, None, 'train_val', attn_labels=attn_WS)\n    else:\n        raise NotImplementedError(args.pool)\n    return (train_dataset, test_dataset, loss_fn, collate_fn, in_features, out_features, pool, kl_weight, scale, init, n_hidden_attn)\nif __name__ == '__main__':\n    dt = datetime.datetime.now()\n    print('start time:', dt)\n    args = parse_args()\n    args.experiment_ID = '%06d' % dt.microsecond\n    print('experiment_ID: ', args.experiment_ID)\n    if args.cv_threads > 1 and args.dataset == 'TU':\n        torch.multiprocessing.set_start_method('spawn')\n    print('gpus: ', torch.cuda.device_count())\n    if args.results not in [None, 'None'] and (not os.path.isdir(args.results)):\n        os.mkdir(args.results)\n    rnd, rnd_data = set_seed(args.seed, args.seed_data)\n    pool = args.pool\n    kl_weight = args.kl_weight\n    scale = args.scale\n    init = args.init\n    n_hidden_attn = float(args.pool_arch[2]) if args.pool_arch is not None and len(args.pool_arch) > 2 else 0\n    if args.dataset.find('colors') >= 0 or args.dataset == 'triangles':\n        train_dataset, test_dataset, loss_fn, collate_fn, in_features, out_features = load_synthetic(args)\n    elif args.dataset in ['mnist', 'mnist-75sp']:\n        train_dataset, test_dataset, loss_fn, collate_fn, in_features, out_features, noises, color_noises = load_mnist(args)\n    else:\n        train_dataset, test_dataset, loss_fn, collate_fn, in_features, out_features, pool, kl_weight, scale, init, n_hidden_attn = load_TU(args, cv_folds=args.cv_folds)\n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.threads, collate_fn=collate_fn)\n    train_loader_test = DataLoader(train_dataset, batch_size=args.test_batch_size, shuffle=False, num_workers=args.threads, collate_fn=collate_fn)\n    print('test_dataset', test_dataset.split)\n    test_loader = DataLoader(test_dataset, batch_size=args.test_batch_size, shuffle=False, num_workers=args.threads, collate_fn=collate_fn)\n    start_epoch, model, optimizer, scheduler = create_model_optimizer(in_features, out_features, pool, kl_weight, args, scale=scale, init=init, n_hidden_attn=n_hidden_attn)\n    feature_stats = None\n    if args.dataset in ['mnist', 'mnist-75sp']:\n        feature_stats = compute_feature_stats(model, train_loader, args.device, n_batches=1000)\n\n    def test_fn(loader, epoch, split, eval_attn):\n        test_loss, acc, _, _ = test(model, loader, epoch, loss_fn, split, args, feature_stats, noises=None, img_noise_level=None, eval_attn=eval_attn, alpha_WS_name='orig')\n        if args.dataset in ['mnist', 'mnist-75sp'] and split == 'test' and (args.img_noise_levels is not None):\n            test(model, loader, epoch, loss_fn, split, args, feature_stats, noises=noises, img_noise_level=args.img_noise_levels[0], eval_attn=eval_attn, alpha_WS_name='noisy')\n            test(model, loader, epoch, loss_fn, split, args, feature_stats, noises=color_noises, img_noise_level=args.img_noise_levels[1], eval_attn=eval_attn, alpha_WS_name='noisy-c')\n        return (test_loss, acc)\n    if start_epoch > args.epochs:\n        print('evaluating the model')\n        test_fn(test_loader, start_epoch - 1, 'val' if args.validation else 'test', args.eval_attn_test)\n    else:\n        for epoch in range(start_epoch, args.epochs + 1):\n            eval_epoch = epoch <= 1 or epoch == args.epochs\n            scheduler.step()\n            train_loss, acc = train(model, train_loader, optimizer, epoch, args, loss_fn, feature_stats)\n            if eval_epoch:\n                save_checkpoint(model, scheduler, optimizer, args, epoch)\n                test_fn(train_loader_test, epoch, 'train', epoch == args.epochs and args.eval_attn_train)\n            if args.validation:\n                test_fn(test_loader, epoch, 'val', epoch == args.epochs and args.eval_attn_test)\n            elif eval_epoch or args.debug:\n                test_fn(test_loader, epoch, 'test', epoch == args.epochs and args.eval_attn_test)\n    print('done in {}'.format(datetime.datetime.now() - dt))\n\n# File: generate_data.py\nimport os\nimport numpy as np\nimport pickle\nimport argparse\nimport networkx as nx\nimport datetime\nimport random\nimport multiprocessing as mp\nfrom utils import *\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Generate synthetic graph datasets')\n    parser.add_argument('-D', '--dataset', type=str, default='colors', choices=['colors', 'triangles'])\n    parser.add_argument('-o', '--out_dir', type=str, default='./data', help='path where to save superpixels')\n    parser.add_argument('--N_train', type=int, default=500, help='number of training graphs (500 for colors and 30000 for triangles)')\n    parser.add_argument('--N_val', type=int, default=2500, help='number of graphs in the validation set (2500 for colors and 5000 for triangles)')\n    parser.add_argument('--N_test', type=int, default=2500, help='number of graphs in each test subset (2500 for colors and 5000 for triangles)')\n    parser.add_argument('--label_min', type=int, default=0, help='smallest label value for a graph (i.e. smallest number of green nodes); 1 for triangles')\n    parser.add_argument('--label_max', type=int, default=10, help='largest label value for a graph (i.e. largest number of green nodes)')\n    parser.add_argument('--N_min', type=int, default=4, help='minimum number of nodes')\n    parser.add_argument('--N_max', type=int, default=200, help='maximum number of nodes (default: 200 for colors and 100 for triangles')\n    parser.add_argument('--N_max_train', type=int, default=25, help='maximum number of nodes in the training set')\n    parser.add_argument('--dim', type=int, default=3, help='node feature dimensionality')\n    parser.add_argument('--green_ch_index', type=int, default=1, help='index of non-zero value in a one-hot node feature vector, i.e. [0, 1, 0] in case green_channel_index=1 and dim=3')\n    parser.add_argument('--seed', type=int, default=111, help='seed for shuffling nodes')\n    parser.add_argument('--threads', type=int, default=0, help='only for triangles')\n    args = parser.parse_args()\n    for arg in vars(args):\n        print(arg, getattr(args, arg))\n    return args\n\ndef check_graph_duplicates(Adj_matrices, node_features=None):\n    n_graphs = len(Adj_matrices)\n    print('check for duplicates for %d graphs' % n_graphs)\n    n_duplicates = 0\n    for i in range(n_graphs):\n        if node_features is not None:\n            assert Adj_matrices[i].shape[0] == node_features[i].shape[0], ('invalid data', i, Adj_matrices[i].shape[0], node_features[i].shape[0])\n        for j in range(i + 1, n_graphs):\n            if Adj_matrices[i].shape[0] == Adj_matrices[j].shape[0]:\n                if np.allclose(Adj_matrices[i], Adj_matrices[j]):\n                    if node_features is None or np.allclose(node_features[i], node_features[j]):\n                        n_duplicates += 1\n                        print('duplicates %d/%d' % (n_duplicates, n_graphs * (n_graphs - 1) / 2))\n    if n_duplicates > 0:\n        raise ValueError('%d duplicates found in the dataset' % n_duplicates)\n    print('no duplicated graphs')\n\ndef get_node_features_Colors(N_nodes, N_green, dim, green_ch_index=1, new_colors=False):\n    node_features = np.zeros((N_nodes, dim))\n    idx_not_green = rnd.randint(0, dim - 1, size=N_nodes - N_green)\n    idx_non_zero = np.concatenate((idx_not_green, np.zeros(N_green, np.int) + dim - 1))\n    idx_non_zero_cp = idx_non_zero.copy()\n    idx_non_zero[idx_non_zero_cp == dim - 1] = green_ch_index\n    idx_non_zero[idx_non_zero_cp == green_ch_index] = dim - 1\n    rnd.shuffle(idx_non_zero)\n    node_features[np.arange(N_nodes), idx_non_zero] = 1\n    if new_colors:\n        for ind in np.where(idx_non_zero != green_ch_index)[0]:\n            node_features[ind] = rnd.randint(0, 2, size=dim)\n            node_features[ind, green_ch_index] = 0\n    label = np.sum((np.sum(node_features, 1) == node_features[:, green_ch_index]) & (node_features[:, green_ch_index] == 1))\n    gt_attn = (idx_non_zero == green_ch_index).reshape(-1, 1)\n    label2 = np.sum(gt_attn)\n    assert N_green == label == label2, ('invalid node features', N_green, label, label2)\n    return (node_features, idx_non_zero, gt_attn)\n\ndef generate_graphs_Colors(N_graphs, N_min, N_max, dim, args, rnd, new_colors=False):\n    Adj_matrices, node_features, GT_attn, graph_labels, N_edges = ([], [], [], [], [])\n    n_labels = args.label_max - args.label_min + 1\n    n_graphs_per_shape = int(np.ceil(N_graphs / (N_max - N_min + 1) / n_labels) * n_labels)\n    for n_nodes in np.array(range(N_min, N_max + 1)):\n        c = 0\n        while True:\n            labels = np.arange(args.label_min, n_labels)\n            labels = labels[labels <= n_nodes]\n            rnd.shuffle(labels)\n            for lbl in labels:\n                features, idx_non_zero, gt_attn = get_node_features_Colors(N_nodes=n_nodes, N_green=lbl, dim=dim, green_ch_index=args.green_ch_index, new_colors=new_colors)\n                n_edges = int((rnd.rand() + 1) * n_nodes)\n                A = nx.to_numpy_array(nx.gnm_random_graph(n_nodes, n_edges))\n                add = True\n                for k in range(len(Adj_matrices)):\n                    if A.shape[0] == Adj_matrices[k].shape[0] and np.allclose(A, Adj_matrices[k]):\n                        if np.allclose(node_features[k], features):\n                            add = False\n                            break\n                if add:\n                    Adj_matrices.append(A.astype(np.bool))\n                    graph_labels.append(lbl)\n                    node_features.append(features.astype(np.bool))\n                    GT_attn.append(gt_attn)\n                    N_edges.append(n_edges)\n                    c += 1\n                    if c >= n_graphs_per_shape:\n                        break\n            if c >= n_graphs_per_shape:\n                break\n    graph_labels = np.array(graph_labels, np.int32)\n    N_edges = np.array(N_edges, np.int32)\n    print(N_graphs, len(graph_labels))\n    return {'Adj_matrices': Adj_matrices, 'GT_attn': GT_attn, 'graph_labels': graph_labels, 'node_features': node_features, 'N_edges': N_edges}\n\ndef get_gt_atnn_triangles(args):\n    G, N = args\n    node_ids = []\n    if G is not None:\n        for clq in nx.enumerate_all_cliques(G):\n            if len(clq) == 3:\n                node_ids.extend(clq)\n    node_ids = np.array(node_ids)\n    gt_attn = np.zeros((N, 1), np.int32)\n    for i in np.unique(node_ids):\n        gt_attn[i] = int(np.sum(node_ids == i))\n    return gt_attn\n\ndef get_graph_triangles(args):\n    N_nodes, rnd = args\n    N_edges = int((rnd.rand() + 1) * N_nodes)\n    G = nx.dense_gnm_random_graph(N_nodes, N_edges, seed=None)\n    A = nx.to_numpy_array(G)\n    A_cube = A.dot(A).dot(A)\n    label = int(np.trace(A_cube) / 6.0)\n    return (A.astype(np.bool), label, N_edges, G)\n\ndef generate_graphs_Triangles(N_graphs, N_min, N_max, args, rnd):\n    N_nodes = rnd.randint(N_min, N_max + 1, size=int(N_graphs * 10))\n    print('generating %d graphs with %d-%d nodes' % (N_graphs * 10, N_min, N_max))\n    if args.threads > 0:\n        with mp.Pool(processes=args.threads) as pool:\n            data = pool.map(get_graph_triangles, [(N_nodes[i], rnd) for i in range(len(N_nodes))])\n    else:\n        data = [get_graph_triangles((N_nodes[i], rnd)) for i in range(len(N_nodes))]\n    labels = np.array([data[i][1] for i in range(len(data))], np.int32)\n    Adj_matrices, node_features, G, graph_labels, N_edges, node_degrees = ([], [], [], [], [], [])\n    for lbl in range(args.label_min, args.label_max + 1):\n        idx = np.where(labels == lbl)[0]\n        c = 0\n        for i in idx:\n            add = True\n            for k in range(len(Adj_matrices)):\n                if data[i][0].shape[0] == Adj_matrices[k].shape[0] and labels[i] == graph_labels[k] and np.allclose(data[i][0], Adj_matrices[k]):\n                    add = False\n                    break\n            if add:\n                Adj_matrices.append(data[i][0])\n                graph_labels.append(labels[i])\n                G.append(data[i][3])\n                N_edges.append(data[i][2])\n                node_degrees.append(data[i][0].astype(np.int32).sum(1).max())\n                c += 1\n                if c >= int(N_graphs / (args.label_max - args.label_min + 1)):\n                    break\n        print('label={}, number of graphs={}/{}, total number of generated graphs={}'.format(lbl, c, len(idx), len(Adj_matrices)))\n        assert c == int(N_graphs / (args.label_max - args.label_min + 1)), ('invalid data', c, int(N_graphs / (args.label_max - args.label_min + 1)))\n    print('computing GT attention for %d graphs' % len(Adj_matrices))\n    if args.threads > 0:\n        with mp.Pool(processes=args.threads) as pool:\n            GT_attn = pool.map(get_gt_atnn_triangles, [(G[i], Adj_matrices[i].shape[0]) for i in range(len(Adj_matrices))])\n    else:\n        GT_attn = [get_gt_atnn_triangles((G[i], Adj_matrices[i].shape[0])) for i in range(len(Adj_matrices))]\n    graph_labels = np.array(graph_labels, np.int32)\n    N_edges = np.array(N_edges, np.int32)\n    return {'Adj_matrices': Adj_matrices, 'GT_attn': GT_attn, 'graph_labels': graph_labels, 'N_edges': N_edges, 'Max_degree': np.max(node_degrees)}\nif __name__ == '__main__':\n    dt = datetime.datetime.now()\n    print('start time:', dt)\n    args = parse_args()\n    if not os.path.isdir(args.out_dir):\n        os.mkdir(args.out_dir)\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    rnd = np.random.RandomState(args.seed)\n\n    def print_stats(data, split_name):\n        print('%s: %d graphs' % (split_name, len(data['graph_labels'])))\n        for lbl in np.unique(data['graph_labels']):\n            print('%s: label=%d, %d graphs' % (split_name, lbl, np.sum(data['graph_labels'] == lbl)))\n    if args.dataset.lower() == 'colors':\n        data_test_combined, Adj_matrices, node_features = ([], [], [])\n        for N_graphs, N_nodes_min, N_nodes_max, dim, name in zip([args.N_train + args.N_val + args.N_test, args.N_test, args.N_test], [args.N_min, args.N_max_train + 1, args.N_max_train + 1], [args.N_max_train, args.N_max, args.N_max], [args.dim, args.dim, args.dim + 1], ['test orig', 'test large', 'test large-c']):\n            data = generate_graphs_Colors(N_graphs, N_nodes_min, N_nodes_max, dim, args, rnd, new_colors=dim == args.dim + 1)\n            if name.find('orig') >= 0:\n                idx = rnd.permutation(len(data['graph_labels']))\n                data_train = copy_data(data, idx[:args.N_train])\n                print_stats(data_train, name.replace('test', 'train'))\n                node_features += data_train['node_features']\n                Adj_matrices += data_train['Adj_matrices']\n                data_val = copy_data(data, idx[args.N_train:args.N_train + args.N_val])\n                print_stats(data_val, name.replace('test', 'val'))\n                node_features += data_val['node_features']\n                Adj_matrices += data_val['Adj_matrices']\n                data_test = copy_data(data, idx[args.N_train + args.N_val:args.N_train + args.N_val + args.N_test])\n            else:\n                data_test = copy_data(data, rnd.permutation(len(data['graph_labels']))[:args.N_test])\n            Adj_matrices += data_test['Adj_matrices']\n            node_features += data_test['node_features']\n            data_test_combined.append(data_test)\n            print_stats(data_test, name)\n        check_graph_duplicates(Adj_matrices, node_features)\n        with open('%s/random_graphs_colors_dim%d_train.pkl' % (args.out_dir, args.dim), 'wb') as f:\n            pickle.dump(data_train, f, protocol=2)\n        with open('%s/random_graphs_colors_dim%d_val.pkl' % (args.out_dir, args.dim), 'wb') as f:\n            pickle.dump(data_val, f, protocol=2)\n        with open('%s/random_graphs_colors_dim%d_test.pkl' % (args.out_dir, args.dim), 'wb') as f:\n            pickle.dump(concat_data(data_test_combined), f, protocol=2)\n    elif args.dataset.lower() == 'triangles':\n        data = generate_graphs_Triangles(args.N_train + args.N_val + args.N_test, args.N_min, args.N_max_train, args, rnd)\n        idx_train, idx_val, idx_test = ([], [], [])\n        classes = np.unique(data['graph_labels'])\n        n_classes = len(classes)\n        for lbl in classes:\n            idx = np.where(data['graph_labels'] == lbl)[0]\n            rnd.shuffle(idx)\n            n_train = int(args.N_train / n_classes)\n            n_val = int(args.N_val / n_classes)\n            n_test = int(args.N_test / n_classes)\n            idx_train.append(idx[:n_train])\n            idx_val.append(idx[n_train:n_train + n_val])\n            idx_test.append(idx[n_train + n_val:n_train + n_val + n_test])\n        data_train = copy_data(data, np.concatenate(idx_train))\n        print_stats(data_train, 'train orig')\n        data_val = copy_data(data, np.concatenate(idx_val))\n        print_stats(data_val, 'val orig')\n        data_test = copy_data(data, np.concatenate(idx_test))\n        print_stats(data_test, 'test orig')\n        data = generate_graphs_Triangles(args.N_test, args.N_max_train + 1, args.N_max, args, rnd)\n        data_test_large = copy_data(data, rnd.permutation(len(data['graph_labels']))[:args.N_test])\n        print_stats(data_test_large, 'test large')\n        check_graph_duplicates(data_train['Adj_matrices'] + data_val['Adj_matrices'] + data_test['Adj_matrices'] + data_test_large['Adj_matrices'])\n        max_degree = np.max(np.array([d['Max_degree'] for d in (data_train, data_val, data_test, data_test_large)]))\n        data_train['Max_degree'] = max_degree\n        with open('%s/random_graphs_triangles_train.pkl' % args.out_dir, 'wb') as f:\n            pickle.dump(data_train, f, protocol=2)\n        data_val['Max_degree'] = max_degree\n        with open('%s/random_graphs_triangles_val.pkl' % args.out_dir, 'wb') as f:\n            pickle.dump(data_val, f, protocol=2)\n        data_test = concat_data((data_test, data_test_large))\n        data_test['Max_degree'] = max_degree\n        with open('%s/random_graphs_triangles_test.pkl' % args.out_dir, 'wb') as f:\n            pickle.dump(data_test, f, protocol=2)\n    else:\n        raise NotImplementedError('unsupported dataset: ' + args.dataset)\n    print('done in {}'.format(datetime.datetime.now() - dt))\n\n# File: graphdata.py\nimport numpy as np\nimport os\nfrom os.path import join as pjoin\nimport pickle\nimport copy\nimport torch\nimport torch.utils\nimport torch.utils.data\nimport torch.nn.functional as F\nimport torchvision\nfrom scipy.spatial.distance import cdist\nfrom utils import *\n\ndef compute_adjacency_matrix_images(coord, sigma=0.1):\n    coord = coord.reshape(-1, 2)\n    dist = cdist(coord, coord)\n    A = np.exp(-dist / (sigma * np.pi) ** 2)\n    A[np.diag_indices_from(A)] = 0\n    return A\n\ndef precompute_graph_images(img_size):\n    col, row = np.meshgrid(np.arange(img_size), np.arange(img_size))\n    coord = np.stack((col, row), axis=2) / img_size\n    A = torch.from_numpy(compute_adjacency_matrix_images(coord)).float().unsqueeze(0)\n    coord = torch.from_numpy(coord).float().unsqueeze(0).view(1, -1, 2)\n    mask = torch.ones(1, img_size * img_size, dtype=torch.uint8)\n    return (A, coord, mask)\n\ndef collate_batch_images(batch, A, mask, use_mean_px=True, coord=None, gt_attn_threshold=0, replicate_features=True):\n    B = len(batch)\n    C, H, W = batch[0][0].shape\n    N_nodes = H * W\n    params_dict = {'N_nodes': torch.zeros(B, dtype=torch.long) + N_nodes, 'node_attn_eval': None}\n    has_WS_attn = len(batch[0]) > 2\n    if has_WS_attn:\n        WS_attn = torch.from_numpy(np.stack([batch[b][2].reshape(N_nodes) for b in range(B)]).astype(np.float32)).view(B, N_nodes)\n        WS_attn = normalize_batch(WS_attn)\n        params_dict.update({'node_attn': WS_attn})\n    if use_mean_px:\n        x = torch.stack([batch[b][0].view(C, N_nodes).t() for b in range(B)]).float()\n        if gt_attn_threshold == 0:\n            GT_attn = (x > 0).view(B, N_nodes).float()\n        else:\n            GT_attn = x.view(B, N_nodes).float().clone()\n            GT_attn[GT_attn < gt_attn_threshold] = 0\n        GT_attn = normalize_batch(GT_attn)\n        params_dict.update({'node_attn_eval': GT_attn})\n        if not has_WS_attn:\n            params_dict.update({'node_attn': GT_attn})\n    else:\n        raise NotImplementedError('this case is not well supported')\n    if coord is not None:\n        if use_mean_px:\n            x = torch.cat((x, coord.expand(B, -1, -1)), dim=2)\n        else:\n            x = coord.expand(B, -1, -1)\n    if x is None:\n        x = torch.ones(B, N_nodes, 1)\n    if replicate_features:\n        x = F.pad(x, (2, 0), 'replicate')\n    try:\n        labels = torch.Tensor([batch[b][1] for b in range(B)]).long()\n    except:\n        labels = torch.stack([batch[b][1] for b in range(B)]).long()\n    return [x, A.expand(B, -1, -1), mask.expand(B, -1), labels, params_dict]\n\ndef collate_batch(batch):\n    \"\"\"\n    Creates a batch of same size graphs by zero-padding node features and adjacency matrices up to\n    the maximum number of nodes in the current batch rather than in the entire dataset.\n    \"\"\"\n    B = len(batch)\n    N_nodes = [batch[b][2] for b in range(B)]\n    C = batch[0][0].shape[1]\n    N_nodes_max = int(np.max(N_nodes))\n    mask = torch.zeros(B, N_nodes_max, dtype=torch.bool)\n    A = torch.zeros(B, N_nodes_max, N_nodes_max)\n    x = torch.zeros(B, N_nodes_max, C)\n    has_GT_attn = len(batch[0]) > 4 and batch[0][4] is not None\n    if has_GT_attn:\n        GT_attn = torch.zeros(B, N_nodes_max)\n    has_WS_attn = len(batch[0]) > 5 and batch[0][5] is not None\n    if has_WS_attn:\n        WS_attn = torch.zeros(B, N_nodes_max)\n    for b in range(B):\n        x[b, :N_nodes[b]] = batch[b][0]\n        A[b, :N_nodes[b], :N_nodes[b]] = batch[b][1]\n        mask[b][:N_nodes[b]] = 1\n        if has_GT_attn:\n            GT_attn[b, :N_nodes[b]] = batch[b][4].squeeze()\n        if has_WS_attn:\n            WS_attn[b, :N_nodes[b]] = batch[b][5].squeeze()\n    N_nodes = torch.from_numpy(np.array(N_nodes)).long()\n    params_dict = {'N_nodes': N_nodes}\n    if has_WS_attn:\n        params_dict.update({'node_attn': WS_attn})\n    if has_GT_attn:\n        params_dict.update({'node_attn_eval': GT_attn})\n        if not has_WS_attn:\n            params_dict.update({'node_attn': GT_attn})\n    elif has_WS_attn:\n        params_dict.update({'node_attn_eval': WS_attn})\n    labels = torch.from_numpy(np.array([batch[b][3] for b in range(B)])).long()\n    return [x, A, mask, labels, params_dict]\n\nclass MNIST(torchvision.datasets.MNIST):\n    \"\"\"\n    Wrapper around MNIST to use predefined attention coefficients\n    \"\"\"\n\n    def __init__(self, root, train=True, transform=None, target_transform=None, download=False, attn_coef=None):\n        super(MNIST, self).__init__(root, train, transform, target_transform, download)\n        self.alpha_WS = None\n        if attn_coef is not None and train:\n            print('loading weakly-supervised labels from %s' % attn_coef)\n            with open(attn_coef, 'rb') as f:\n                self.alpha_WS = pickle.load(f)\n            print(train, len(self.alpha_WS))\n\n    def __getitem__(self, index):\n        img, target = super(MNIST, self).__getitem__(index)\n        if self.alpha_WS is None:\n            return (img, target)\n        else:\n            return (img, target, self.alpha_WS[index])\n\nclass MNIST75sp(torch.utils.data.Dataset):\n\n    def __init__(self, data_dir, split, use_mean_px=True, use_coord=True, gt_attn_threshold=0, attn_coef=None):\n        self.data_dir = data_dir\n        self.split = split\n        self.is_test = split.lower() in ['test', 'val']\n        with open(pjoin(data_dir, 'mnist_75sp_%s.pkl' % split), 'rb') as f:\n            self.labels, self.sp_data = pickle.load(f)\n        self.use_mean_px = use_mean_px\n        self.use_coord = use_coord\n        self.n_samples = len(self.labels)\n        self.img_size = 28\n        self.gt_attn_threshold = gt_attn_threshold\n        self.alpha_WS = None\n        if attn_coef is not None and (not self.is_test):\n            with open(attn_coef, 'rb') as f:\n                self.alpha_WS = pickle.load(f)\n            print('using weakly-supervised labels from %s (%d samples)' % (attn_coef, len(self.alpha_WS)))\n\n    def train_val_split(self, samples_idx):\n        self.sp_data = [self.sp_data[i] for i in samples_idx]\n        self.labels = self.labels[samples_idx]\n        self.n_samples = len(self.labels)\n\n    def precompute_graph_data(self, replicate_features, threads=0):\n        print('precompute all data for the %s set...' % self.split.upper())\n        self.Adj_matrices, self.node_features, self.GT_attn, self.WS_attn = ([], [], [], [])\n        for index, sample in enumerate(self.sp_data):\n            mean_px, coord = sample[:2]\n            coord = coord / self.img_size\n            A = compute_adjacency_matrix_images(coord)\n            N_nodes = A.shape[0]\n            x = None\n            if self.use_mean_px:\n                x = mean_px.reshape(N_nodes, -1)\n            if self.use_coord:\n                coord = coord.reshape(N_nodes, 2)\n                if self.use_mean_px:\n                    x = np.concatenate((x, coord), axis=1)\n                else:\n                    x = coord\n            if x is None:\n                x = np.ones(N_nodes, 1)\n            if replicate_features:\n                x = np.pad(x, ((0, 0), (2, 0)), 'edge')\n            if self.gt_attn_threshold == 0:\n                gt_attn = (mean_px > 0).astype(np.float32)\n            else:\n                gt_attn = mean_px.copy()\n                gt_attn[gt_attn < self.gt_attn_threshold] = 0\n            self.GT_attn.append(normalize(gt_attn))\n            if self.alpha_WS is not None:\n                self.WS_attn.append(normalize(self.alpha_WS[index]))\n            self.node_features.append(x)\n            self.Adj_matrices.append(A)\n\n    def __len__(self):\n        return self.n_samples\n\n    def __getitem__(self, index):\n        data = [self.node_features[index], self.Adj_matrices[index], self.Adj_matrices[index].shape[0], self.labels[index], self.GT_attn[index]]\n        if self.alpha_WS is not None:\n            data.append(self.WS_attn[index])\n        data = list_to_torch(data)\n        return data\n\nclass SyntheticGraphs(torch.utils.data.Dataset):\n\n    def __init__(self, data_dir, dataset, split, degree_feature=True, attn_coef=None, threads=12):\n        self.is_test = split.lower() in ['test', 'val']\n        self.split = split\n        self.degree_feature = degree_feature\n        if dataset.find('colors') >= 0:\n            dim = int(dataset.split('-')[1])\n            data_file = 'random_graphs_colors_dim%d_%s.pkl' % (dim, split)\n            is_triangles = False\n            self.feature_dim = dim + 1\n        if dataset.find('triangles') >= 0:\n            data_file = 'random_graphs_triangles_%s.pkl' % split\n            is_triangles = True\n        else:\n            NotImplementedError(dataset)\n        with open(pjoin(data_dir, data_file), 'rb') as f:\n            data = pickle.load(f)\n        for key in data:\n            if not isinstance(data[key], list) and (not isinstance(data[key], np.ndarray)):\n                print(split, key, data[key])\n            else:\n                print(split, key, len(data[key]))\n        self.Node_degrees = [np.sum(A, 1).astype(np.int32) for A in data['Adj_matrices']]\n        if is_triangles:\n            self.feature_dim = data['Max_degree'] + 1\n            self.node_features = []\n            for i in range(len(data['Adj_matrices'])):\n                N = data['Adj_matrices'][i].shape[0]\n                if degree_feature:\n                    D_onehot = np.zeros((N, self.feature_dim))\n                    D_onehot[np.arange(N), self.Node_degrees[i]] = 1\n                else:\n                    D_onehot = np.zeros((N, 1))\n                self.node_features.append(D_onehot)\n            if not degree_feature:\n                self.feature_dim = 1\n        else:\n            self.node_features = []\n            for i in range(len(data['node_features'])):\n                features = data['node_features'][i]\n                if features.shape[1] < self.feature_dim:\n                    features = np.pad(features, ((0, 0), (0, 1)), 'constant')\n                self.node_features.append(features)\n        self.alpha_WS = None\n        if attn_coef is not None and (not self.is_test):\n            with open(attn_coef, 'rb') as f:\n                self.alpha_WS = pickle.load(f)\n            print('using weakly-supervised labels from %s (%d samples)' % (attn_coef, len(self.alpha_WS)))\n            self.WS_attn = []\n            for index in range(len(self.alpha_WS)):\n                self.WS_attn.append(normalize(self.alpha_WS[index]))\n        N_nodes = np.array([A.shape[0] for A in data['Adj_matrices']])\n        self.Adj_matrices = data['Adj_matrices']\n        self.GT_attn = data['GT_attn']\n        for i in range(len(self.GT_attn)):\n            self.GT_attn[i] = normalize(self.GT_attn[i])\n        self.labels = data['graph_labels'].astype(np.int32)\n        self.classes = np.unique(self.labels)\n        self.n_classes = len(self.classes)\n        R = np.corrcoef(self.labels, N_nodes)[0, 1]\n        degrees = []\n        for i in range(len(self.Node_degrees)):\n            degrees.extend(list(self.Node_degrees[i]))\n        degrees = np.array(degrees, np.int32)\n        print('N nodes avg/std/min/max: \\t{:.2f}/{:.2f}/{:d}/{:d}'.format(*stats(N_nodes)))\n        print('N edges avg/std/min/max: \\t{:.2f}/{:.2f}/{:d}/{:d}'.format(*stats(data['N_edges'])))\n        print('Node degree avg/std/min/max: \\t{:.2f}/{:.2f}/{:d}/{:d}'.format(*stats(degrees)))\n        print('Node features dim: \\t\\t%d' % self.feature_dim)\n        print('N classes: \\t\\t\\t%d' % self.n_classes)\n        print('Correlation of labels with graph size: \\t%.2f' % R)\n        print('Classes: \\t\\t\\t%s' % str(self.classes))\n        for lbl in self.classes:\n            idx = self.labels == lbl\n            print('Class {}: \\t\\t\\t{} samples, N_nodes: avg/std/min/max: \\t{:.2f}/{:.2f}/{:d}/{:d}'.format(lbl, np.sum(idx), *stats(N_nodes[idx])))\n\n    def __len__(self):\n        return len(self.Adj_matrices)\n\n    def __getitem__(self, index):\n        data = [self.node_features[index], self.Adj_matrices[index], self.Adj_matrices[index].shape[0], self.labels[index], self.GT_attn[index]]\n        if self.alpha_WS is not None:\n            data.append(self.WS_attn[index])\n        data = list_to_torch(data)\n        return data\n\nclass GraphData(torch.utils.data.Dataset):\n\n    def __init__(self, datareader, fold_id, split, degree_feature=True, attn_labels=None):\n        self.fold_id = fold_id\n        self.split = split\n        self.w_sup_signal_attn = None\n        print('The degree_feature argument is ignored for this dataset. \\n        It will automatically be set to True if nodes do not have any features. Otherwise it will be set to False')\n        if attn_labels is not None:\n            if isinstance(attn_labels, str) and os.path.isfile(attn_labels):\n                with open(attn_labels, 'rb') as f:\n                    self.w_sup_signal_attn = pickle.load(f)\n            else:\n                self.w_sup_signal_attn = attn_labels\n            for i in range(len(self.w_sup_signal_attn)):\n                alpha = self.w_sup_signal_attn[i]\n                alpha[alpha < 0.001] = 0\n                self.w_sup_signal_attn[i] = normalize(alpha)\n            print(('!!!using weakly supervised labels (%d samples)!!!' % len(self.w_sup_signal_attn)).upper())\n        self.set_fold(datareader.data, fold_id)\n\n    def set_fold(self, data, fold_id):\n        self.total = len(data['targets'])\n        self.N_nodes_max = data['N_nodes_max']\n        self.num_classes = data['num_classes']\n        self.num_features = data['num_features']\n        if self.split in ['train', 'val']:\n            self.idx = data['splits'][self.split][fold_id]\n        else:\n            assert self.split in ['train_val', 'test'], ('unexpected split', self.split)\n            self.idx = data['splits'][self.split]\n        self.labels = np.array(copy.deepcopy([data['targets'][i] for i in self.idx]))\n        self.adj_list = copy.deepcopy([data['adj_list'][i] for i in self.idx])\n        self.features_onehot = copy.deepcopy([data['features_onehot'][i] for i in self.idx])\n        self.N_edges = np.array([A.sum() // 2 for A in self.adj_list])\n        print('%s: %d/%d' % (self.split.upper(), len(self.labels), len(data['targets'])))\n        classes = np.unique(self.labels)\n        for lbl in classes:\n            print('Class %d: \\t\\t\\t%d samples' % (lbl, np.sum(self.labels == lbl)))\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, index):\n        if isinstance(index, str):\n            if index == 'Adj_matrices':\n                return self.adj_list\n            elif index == 'GT_attn':\n                print('Ground truth attention is unavailable for this dataset: weakly-supervised labels will be returned')\n                return self.w_sup_signal_attn\n            elif index == 'graph_labels':\n                return self.labels\n            elif index == 'node_features':\n                return self.features_onehot\n            elif index == 'N_edges':\n                return self.N_edges\n            else:\n                raise KeyError(index)\n        else:\n            data = [self.features_onehot[index], self.adj_list[index], self.adj_list[index].shape[0], self.labels[index], None]\n            if self.w_sup_signal_attn is not None:\n                data.append(self.w_sup_signal_attn[index])\n            data = list_to_torch(data)\n            return data\n\nclass DataReader:\n    \"\"\"\n    Class to read the txt files containing all data of the dataset\n    Should work for any dataset from https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets\n    \"\"\"\n\n    def __init__(self, data_dir, N_nodes=None, rnd_state=None, use_cont_node_attr=False, folds=10, fold_id=None):\n        self.data_dir = data_dir\n        self.rnd_state = np.random.RandomState() if rnd_state is None else rnd_state\n        self.use_cont_node_attr = use_cont_node_attr\n        self.N_nodes = N_nodes\n        if os.path.isfile('%s/data.pkl' % data_dir):\n            print('loading data from %s/data.pkl' % data_dir)\n            with open('%s/data.pkl' % data_dir, 'rb') as f:\n                data = pickle.load(f)\n        else:\n            files = os.listdir(self.data_dir)\n            data = {}\n            nodes, graphs = self.read_graph_nodes_relations(list(filter(lambda f: f.find('graph_indicator') >= 0, files))[0])\n            lst = list(filter(lambda f: f.find('node_labels') >= 0, files))\n            if len(lst) > 0:\n                data['features'] = self.read_node_features(lst[0], nodes, graphs, fn=lambda s: int(s.strip()))\n            else:\n                data['features'] = None\n            data['adj_list'] = self.read_graph_adj(list(filter(lambda f: f.find('_A') >= 0, files))[0], nodes, graphs)\n            data['targets'] = np.array(self.parse_txt_file(list(filter(lambda f: f.find('graph_labels') >= 0, files))[0], line_parse_fn=lambda s: int(float(s.strip()))))\n            if self.use_cont_node_attr:\n                data['attr'] = self.read_node_features(list(filter(lambda f: f.find('node_attributes') >= 0, files))[0], nodes, graphs, fn=lambda s: np.array(list(map(float, s.strip().split(',')))))\n            features, n_edges, degrees = ([], [], [])\n            for sample_id, adj in enumerate(data['adj_list']):\n                N = len(adj)\n                if data['features'] is not None:\n                    assert N == len(data['features'][sample_id]), (N, len(data['features'][sample_id]))\n                n = np.sum(adj)\n                n_edges.append(int(n / 2))\n                if not np.allclose(adj, adj.T):\n                    print(sample_id, 'not symmetric')\n                degrees.extend(list(np.sum(adj, 1)))\n                if data['features'] is not None:\n                    features.append(np.array(data['features'][sample_id]))\n            if data['features'] is not None:\n                features_all = np.concatenate(features)\n                features_min = features_all.min()\n                num_features = int(features_all.max() - features_min + 1)\n                features_onehot = []\n                for i, x in enumerate(features):\n                    feature_onehot = np.zeros((len(x), num_features))\n                    for node, value in enumerate(x):\n                        feature_onehot[node, value - features_min] = 1\n                    if self.use_cont_node_attr:\n                        feature_onehot = np.concatenate((feature_onehot, np.array(data['attr'][i])), axis=1)\n                    features_onehot.append(feature_onehot)\n                if self.use_cont_node_attr:\n                    num_features = features_onehot[0].shape[1]\n            else:\n                degree_max = int(np.max([np.sum(A, 1).max() for A in data['adj_list']]))\n                num_features = degree_max + 1\n                features_onehot = []\n                for A in data['adj_list']:\n                    n = A.shape[0]\n                    D = np.sum(A, 1).astype(np.int)\n                    D_onehot = np.zeros((n, num_features))\n                    D_onehot[np.arange(n), D] = 1\n                    features_onehot.append(D_onehot)\n            shapes = [len(adj) for adj in data['adj_list']]\n            labels = data['targets']\n            labels -= np.min(labels)\n            classes = np.unique(labels)\n            num_classes = len(classes)\n            if not np.all(np.diff(classes) == 1):\n                print('making labels sequential, otherwise pytorch might crash')\n                labels_new = np.zeros(labels.shape, dtype=labels.dtype) - 1\n                for lbl in range(num_classes):\n                    labels_new[labels == classes[lbl]] = lbl\n                labels = labels_new\n                classes = np.unique(labels)\n                assert len(np.unique(labels)) == num_classes, np.unique(labels)\n\n            def stats(x):\n                return (np.mean(x), np.std(x), np.min(x), np.max(x))\n            print('N nodes avg/std/min/max: \\t%.2f/%.2f/%d/%d' % stats(shapes))\n            print('N edges avg/std/min/max: \\t%.2f/%.2f/%d/%d' % stats(n_edges))\n            print('Node degree avg/std/min/max: \\t%.2f/%.2f/%d/%d' % stats(degrees))\n            print('Node features dim: \\t\\t%d' % num_features)\n            print('N classes: \\t\\t\\t%d' % num_classes)\n            print('Classes: \\t\\t\\t%s' % str(classes))\n            for lbl in classes:\n                print('Class %d: \\t\\t\\t%d samples' % (lbl, np.sum(labels == lbl)))\n            if data['features'] is not None:\n                for u in np.unique(features_all):\n                    print('feature {}, count {}/{}'.format(u, np.count_nonzero(features_all == u), len(features_all)))\n            N_graphs = len(labels)\n            assert N_graphs == len(data['adj_list']) == len(features_onehot), 'invalid data'\n            data['features_onehot'] = features_onehot\n            data['targets'] = labels\n            data['N_nodes_max'] = np.max(shapes)\n            data['num_features'] = num_features\n            data['num_classes'] = num_classes\n            with open('%s/data.pkl' % data_dir, 'wb') as f:\n                pickle.dump(data, f, protocol=2)\n        labels = data['targets']\n        N_graphs = len(labels)\n        shapes = np.array([len(adj) for adj in data['adj_list']])\n        train_ids, val_ids, train_val_ids, test_ids = self.split_ids_shape(np.arange(N_graphs), shapes, N_nodes, folds=folds)\n        splits = {'train': [], 'val': [], 'train_val': train_val_ids, 'test': test_ids}\n        for fold in range(folds):\n            splits['train'].append(train_ids[fold])\n            splits['val'].append(val_ids[fold])\n        data['splits'] = splits\n        self.data = data\n\n    def split_ids_shape(self, ids_all, shapes, N_nodes, folds=1, fold_id=0):\n        if N_nodes > 0:\n            small_graphs_ind = np.where(shapes <= N_nodes)[0]\n            print('{}/{} graphs with at least {} nodes'.format(len(small_graphs_ind), len(shapes), N_nodes))\n            idx = self.rnd_state.permutation(len(small_graphs_ind))\n            if len(idx) > 1000:\n                n = 1000\n            else:\n                n = 500\n            train_val_ids = small_graphs_ind[idx[:n]]\n            test_ids = small_graphs_ind[idx[n:]]\n            large_graphs_ind = np.where(shapes > N_nodes)[0]\n            test_ids = np.concatenate((test_ids, large_graphs_ind))\n        else:\n            idx = self.rnd_state.permutation(len(ids_all))\n            n = len(ids_all) // folds\n            test_ids = ids_all[idx[fold_id * n:(fold_id + 1) * n if fold_id < folds - 1 else -1]]\n            train_val_ids = []\n            for i in ids_all:\n                if i not in test_ids:\n                    train_val_ids.append(i)\n            train_val_ids = np.array(train_val_ids)\n        assert np.all(np.unique(np.concatenate((train_val_ids, test_ids))) == sorted(ids_all)), 'some graphs are missing in the test sets'\n        if folds > 0:\n            print('generating %d-fold cross-validation splits' % folds)\n            train_ids, val_ids = self.split_ids(train_val_ids, folds=folds)\n            for fold in range(folds):\n                ind = np.concatenate((train_ids[fold], val_ids[fold]))\n                print(fold, len(train_ids[fold]), len(val_ids[fold]))\n                assert len(train_ids[fold]) + len(val_ids[fold]) == len(np.unique(ind)) == len(ind) == len(train_val_ids), 'invalid splits'\n        else:\n            train_ids, val_ids = ([], [])\n        return (train_ids, val_ids, train_val_ids, test_ids)\n\n    def split_ids(self, ids, folds=10):\n        n = len(ids)\n        stride = int(np.ceil(n / float(folds)))\n        test_ids = [ids[i:i + stride] for i in range(0, n, stride)]\n        assert np.all(np.unique(np.concatenate(test_ids)) == sorted(ids)), 'some graphs are missing in the test sets'\n        assert len(test_ids) == folds, 'invalid test sets'\n        train_ids = []\n        for fold in range(folds):\n            train_ids.append(np.array([e for e in ids if e not in test_ids[fold]]))\n            assert len(train_ids[fold]) + len(test_ids[fold]) == len(np.unique(list(train_ids[fold]) + list(test_ids[fold]))) == n, 'invalid splits'\n        return (train_ids, test_ids)\n\n    def parse_txt_file(self, fpath, line_parse_fn=None):\n        with open(pjoin(self.data_dir, fpath), 'r') as f:\n            lines = f.readlines()\n        data = [line_parse_fn(s) if line_parse_fn is not None else s for s in lines]\n        return data\n\n    def read_graph_adj(self, fpath, nodes, graphs):\n        edges = self.parse_txt_file(fpath, line_parse_fn=lambda s: s.split(','))\n        adj_dict = {}\n        for edge in edges:\n            node1 = int(edge[0].strip()) - 1\n            node2 = int(edge[1].strip()) - 1\n            graph_id = nodes[node1]\n            assert graph_id == nodes[node2], ('invalid data', graph_id, nodes[node2])\n            if graph_id not in adj_dict:\n                n = len(graphs[graph_id])\n                adj_dict[graph_id] = np.zeros((n, n))\n            ind1 = np.where(graphs[graph_id] == node1)[0]\n            ind2 = np.where(graphs[graph_id] == node2)[0]\n            assert len(ind1) == len(ind2) == 1, (ind1, ind2)\n            adj_dict[graph_id][ind1, ind2] = 1\n        adj_list = [adj_dict[graph_id] for graph_id in sorted(list(graphs.keys()))]\n        return adj_list\n\n    def read_graph_nodes_relations(self, fpath):\n        graph_ids = self.parse_txt_file(fpath, line_parse_fn=lambda s: int(s.rstrip()))\n        nodes, graphs = ({}, {})\n        for node_id, graph_id in enumerate(graph_ids):\n            if graph_id not in graphs:\n                graphs[graph_id] = []\n            graphs[graph_id].append(node_id)\n            nodes[node_id] = graph_id\n        graph_ids = np.unique(list(graphs.keys()))\n        for graph_id in graph_ids:\n            graphs[graph_id] = np.array(graphs[graph_id])\n        return (nodes, graphs)\n\n    def read_node_features(self, fpath, nodes, graphs, fn):\n        node_features_all = self.parse_txt_file(fpath, line_parse_fn=fn)\n        node_features = {}\n        for node_id, x in enumerate(node_features_all):\n            graph_id = nodes[node_id]\n            if graph_id not in node_features:\n                node_features[graph_id] = [None] * len(graphs[graph_id])\n            ind = np.where(graphs[graph_id] == node_id)[0]\n            assert len(ind) == 1, ind\n            assert node_features[graph_id][ind[0]] is None, node_features[graph_id][ind[0]]\n            node_features[graph_id][ind[0]] = x\n        node_features_lst = [node_features[graph_id] for graph_id in sorted(list(graphs.keys()))]\n        return node_features_lst",
        "experimental_info": ""
      }
    },
    {
      "title": "Classification-Based Anomaly Detection for General Data",
      "full_text": "Published as a conference paper at ICLR 2020 CLASSIFICATION -BASED ANOMALY DETECTION FOR GENERAL DATA Liron Bergman Yedid Hoshen School of Computer Science and Engineering The Hebrew University of Jerusalem, Israel ABSTRACT Anomaly detection, ﬁnding patterns that substantially deviate from those seen pre- viously, is one of the fundamental problems of artiﬁcial intelligence. Recently, classiﬁcation-based methods were shown to achieve superior results on this task. In this work, we present a unifying view and propose an open-set method, GOAD, to relax current generalization assumptions. Furthermore, we extend the appli- cability of transformation-based methods to non-image data using random afﬁne transformations. Our method is shown to obtain state-of-the-art accuracy and is applicable to broad data types. The strong performance of our method is exten- sively validated on multiple datasets from different domains. 1 I NTRODUCTION Detecting anomalies in perceived data is a key ability for humans and for artiﬁcial intelligence. Hu- mans often detect anomalies to give early indications of danger or to discover unique opportunities. Anomaly detection systems are being used by artiﬁcial intelligence to discover credit card fraud, for detecting cyber intrusion, alert predictive maintenance of industrial equipment and for discovering attractive stock market opportunities. The typical anomaly detection setting is a one class classi- ﬁcation task, where the objective is to classify data as normal or anomalous. The importance of the task stems from being able to raise an alarm when detecting a different pattern from those seen in the past, therefore triggering further inspection. This is fundamentally different from supervised learning tasks, in which examples of all data classes are observed. There are different possible scenarios for anomaly detection methods. In supervised anomaly de- tection, we are given training examples of normal and anomalous patterns. This scenario can be quite well speciﬁed, however obtaining such supervision may not be possible. For example in cyber security settings, we will not have supervised examples of new, unknown computer viruses making supervised training difﬁcult. On the other extreme, fully unsupervised anomaly detection, obtains a stream of data containing normal and anomalous patterns and attempts to detect the anomalous data. In this work we deal with the semi-supervised scenario. In this setting, we have a training set of normal examples (which contains no anomalies). After training the anomaly detector, we detect anomalies in the test data, containing both normal and anomalous examples. This supervision is easy to obtain in many practical settings and is less difﬁcult than the fully-unsupervised case. Many anomaly detection methods have been proposed over the last few decades. They can be broadly classiﬁed into reconstruction and statistically based methods.Recently, deep learning meth- ods based on classiﬁcation have achieved superior results. Most semi-supervised classiﬁcation- based methods attempt to solve anomaly detection directly, despite only having normal training data. One example is: Deep-SVDD (Ruff et al., 2018) - one-class classiﬁcation using a learned deep space. Another type of classiﬁcation-based methods is self-supervised i.e. methods that solve one or more classiﬁcation-based auxiliary tasks on the normal training data, and this is shown to be useful for solving anomaly detection, the task of interest e.g. (Golan & El-Yaniv, 2018). Self-supervised classiﬁcation-based methods have been proposed with the object of image anomaly detection, but we show that by generalizing the class of transformations they can apply to all data types. In this paper, we introduce a novel technique, GOAD, for anomaly detection which uniﬁes current state-of-the-art methods that use normal training data only and are based on classiﬁcation. Our method ﬁrst transforms the data into M subspaces, and learns a feature space such that inter-class 1 arXiv:2005.02359v1  [cs.LG]  5 May 2020Published as a conference paper at ICLR 2020 separation is larger than intra-class separation. For the learned features, the distance from the cluster center is correlated with the likelihood of anomaly. We use this criterion to determine if a new data point is normal or anomalous. We also generalize the class of transformation functions to include afﬁne transformation which allows our method to generalize to non-image data. This is signiﬁcant as tabular data is probably the most important for applications of anomaly detection. Our method is evaluated on anomaly detection on image and tabular datasets (cyber security and medical) and is shown to signiﬁcantly improve over the state-of-the-art. 1.1 P REVIOUS WORKS Anomaly detection methods can be generally divided into the following categories: Reconstruction Methods: Some of the most common anomaly detection methods are reconstruction- based. The general idea behind such methods is that every normal sample should be reconstructed accurately using a limited set of basis functions, whereas anomalous data should suffer from larger reconstruction costs. The choice of features, basis and loss functions differentiates between the different methods. Some of the earliest methods use: nearest neighbors (Eskin et al., 2002), low-rank PCA (Jolliffe, 2011; Cand`es et al., 2011) or K-means (Hartigan & Wong, 1979) as the reconstruction basis. Most recently, neural networks were used (Sakurada & Yairi, 2014; Xia et al., 2015) for learning deep basis functions for reconstruction. Another set of recent methods (Schlegl et al., 2017; Deecke et al., 2018) use GANs to learn a reconstruction basis function. GANs suffer from mode-collapse and are difﬁcult to invert, which limits the performance of such methods. Distributional Methods: Another set of commonly used methods are distribution-based. The main theme in such methods is to model the distribution of normal data. The expectation is that anomalous test data will have low likelihood under the probabilistic model while normal data will have higher likelihoods. Methods differ in the features used to describe the data and the probabilistic model used to estimate the normal distribution. Some early methods used Gaussian or Gaussian mixture models. Such models will only work if the data under the selected feature space satisﬁes the prob- abilistic assumptions implicied by the model. Another set of methods used non-parametric density estimate methods such as kernel density estimate (Parzen, 1962). Recently, deep learning methods (autoencoders or variational autoencoders) were used to learn deep features which are sometimes easier to model than raw features (Yang et al., 2017). DAGMM introduced by Zong et al. (2018) learn the probabilistic model jointly with the deep features therefore shaping the features space to better conform with the probabilistic assumption. Classiﬁcation-Based Methods: Another paradigm for anomaly detection is separation between space regions containing normal data from all other regions. An example of such approach is One-Class SVM (Scholkopf et al., 2000), which trains a classiﬁer to perform this separation. Learning a good feature space for performing such separation is performed both by the classic kernel methods as well as by the recent deep learning approach (Ruff et al., 2018). One of the main challenges in unsupervised (or semi-supervised) learning is providing an objective for learning features that are relevant to the task of interest. One method for learning good representations in a self-supervised way is by training a neural network to solve an auxiliary task for which obtaining data is free or at least very inexpensive. Auxiliary tasks for learning high-quality image features include: video frame prediction (Mathieu et al., 2016), image colorization (Zhang et al., 2016; Larsson et al., 2016), puzzle solving (Noroozi & Favaro, 2016) - predicting the correct order of random permuted image patches. Recently, Gidaris et al. (2018) used a set of image processing transformations (rotation by 0,90,180,270 degrees around the image axis, and predicted the true image orientation has been used to learn high-quality image features. Golan & El-Yaniv (2018), have used similar image-processing task prediction for detecting anomalies in images. This method has shown good performance on detecting images from anomalous classes. In this work, we overcome some of the limitations of previous classiﬁcation-based methods and extend their applicability of self-supervised methods to general data types. We also show that our method is more robust to adversarial attacks. 2 C LASSIFICATION -BASED ANOMALY DETECTION Classiﬁcation-based methods have dominated supervised anomaly detection. In this section we will analyse semi-supervised classiﬁcation-based methods: 2Published as a conference paper at ICLR 2020 Let us assume all data lies in spaceRL (where Lis the data dimension). Normal data lie in subspace X ⊂RL. We assume that all anomalies lie outsideX. To detect anomalies, we would therefore like to build a classiﬁer C, such that C(x) = 1if x∈X and C(x) = 0if x∈RL\\X. One-class classiﬁcation methods attempt to learn C directly as P(x ∈X). Classical approaches have learned a classiﬁer either in input space or in a kernel space. Recently, Deep-SVDD (Ruff et al., 2018) learned end-to-end to i) transform the data to an isotropic feature space f(x) ii) ﬁt the minimal hypersphere of radius Rand center c0 around the features of the normal training data. Test data is classiﬁed as anomalous if the following normality score is positive: ∥f(x) −c0∥2 −R2. Learning an effective feature space is not a simple task, as the trivial solution of f(x) = 0 ∀ x results in the smallest hypersphere, various tricks are used to avoid this possibility. Geometric-transformation classiﬁcation (GEOM), proposed by Golan & El-Yaniv (2018) ﬁrst trans- forms the normal data subspace X into M subspaces X1..XM . This is done by transforming each image x ∈X using M different geometric transformations (rotation, reﬂection, translation) into T(x,1)..T(x,M). Although these transformations are image speciﬁc, we will later extend the class of transformations to all afﬁne transformations making this applicable to non-image data. They set an auxiliary task of learning a classiﬁer able to predict the transformation labelmgiven transformed data point T(x,m). As the training set consists of normal data only, each sample is x ∈X and the transformed sample is in ∪mXm. The method attempts to estimate the following conditional probability: P(m′|T(x,m)) = P(T(x,m) ∈Xm′ )P(m′)∑ ˜m P(T(x,m) ∈X˜m)P( ˜m) = P(T(x,m) ∈Xm′ )∑ ˜m P(T(x,m) ∈X˜m) (1) Where the second equality follows by design of the training set, and where every training sample is transformed exactly once by each transformation leading to equal priors. For anomalous data x∈RL\\X, by construction of the subspace, if the transformations T are one- to-one, it follows that the transformed sample does not fall in the appropriate subspace: T(x,m) ∈ RL\\Xm. GEOM uses P(m|T(x,m)) as a score for determining if x is anomalous i.e. that x ∈ RL\\X. GEOM gives samples with low probabilities P(m|T(x,m)) high anomaly scores. A signiﬁcant issue with this methodology, is that the learned classiﬁer P(m′|T(x,m)) is only valid for samples x ∈X which were found in the training set. For x ∈RL\\X we should in fact have P(T(x,m) ∈Xm′ ) = 0for all m= 1..M (as the transformed xis not in any of the subsets). This makes the anomaly score P(m′|T(x,m)) have very high variance for anomalies. One way to overcome this issue is by using examples of anomaliesxa and training P(m|T(x,m)) = 1 M on anomalous data. This corresponds to the supervised scenario and was recently introduced as Outlier Exposure (Hendrycks et al., 2018). Although getting such supervision is possible for some image tasks (where large external datasets can be used) this is not possible in the general case e.g. for tabular data which exhibits much more variation between datasets. 3 D ISTANCE -BASED MULTIPLE TRANSFORMATION CLASSIFICATION We propose a novel method to overcome the generalization issues highlighted in the previous section by using ideas from open-set classiﬁcation (Bendale & Boult, 2016). Our approach uniﬁes one-class and transformation-based classiﬁcation methods. Similarly to GEOM, we transform Xto X1..XM . We learn a feature extractor f(x) using a neural network, which maps the original input data into a feature representation. Similarly to deep OC methods, we model each subspace Xm mapped to the feature space {f(x)|x∈Xm}as a sphere with center cm. The probability of data point xafter transformation mis parameterized by P(T(x,m) ∈X′ m) = 1 Z e−(f(T(x,m))−c′ m)2 . The classiﬁer predicting transformation mgiven a transformed point is therefore: P(m′|T(x,m)) = e−∥f(T(x,m))−cm′ ∥2 ∑ ˜m e−∥f(T(x,m))−c ˜m∥2 (2) The centers cm are given by the average feature over the training set for every transformation i.e. cm = 1 N ∑ x∈X f(T(x,m)). One option is to directly learn f by optimizing cross-entropy between 3Published as a conference paper at ICLR 2020 P(m′|T(x,m)) and the correct label on the normal training set. In practice we obtained better results by training f using the center triplet loss (He et al., 2018), which learns supervised clus- ters with low intra-class variation, and high-inter-class variation by optimizing the following loss function (where sis a margin regularizing the distance between clusters): L= ∑ i max(∥f(T(xi,m)) −cm∥2 + s−minm′̸=m∥f(T(xi,m)) −cm′ ∥2,0) (3) Having learned a feature space in which the different transformation subspaces are well separated, we use the probability in Eq. 2 as a normality score. However, for data far away from the normal distributions, the distances from the means will be large. A small difference in distance will make the classiﬁer unreasonably certain of a particular transformation. To add a general prior for uncer- tainty far from the training set, we add a small regularizing constant ϵ to the probability of each transformation. This ensures equal probabilities for uncertain regions: ˜P(m′|T(x,m)) = e−∥f(T(x,m))−cm′ ∥2 + ϵ∑ ˜m e−∥f(T(x,m))−c ˜m∥2 + M ·ϵ (4) At test time we transform each sample by the M transformations. By assuming independence be- tween transformations, the probability thatxis normal (i.e. x∈X) is the product of the probabilities that all transformed samples are in their respective subspace. For log-probabilities the total score is given by: Score(x) =−log P(x∈X) =− ∑ m log ˜P(T(x,m) ∈Xm) =− ∑ m log ˜P(m|T(x,m)) (5) The score computes the degree of anomaly of each sample. Higher scores indicate a more anomalous sample. Algorithm 1:GOAD: Training Algorithm Input: Normal training data x1,x2...xN Transformations T(,1),T(,2)...T(,M) Output: Feature extractor f, centers c1,c2...cM T(xi,1),T(xi,2)...T(xi,M) ←xi // Transform each sample by all transformations 1 to M Find f,c1,c2...cM that optimize the triplet loss in Eq. 3 Algorithm 2:GOAD: Evaluation Algorithm Input: Test sample: x, feature extractor: f, centers: c1,c2...cM , transformations: T(,1),T(,2)...T(,M) Output: Score(x) T(x,1),T(x,2)...T(x,M) ←x // Transform test sample by all transformations 1 to M P(m|T(x,m)) ←f(T(x,m)),c1,c2...cM // Likelihood of predicting the correct transformation (Eq. 4) Score(x) ←P(1|T(x,1)),P(2|T(x,2))...P(M|T(x,M)) // Aggregate probabilities to compute anomaly score (Eq. 5) 4 P ARAMETERIZING THE SET OF TRANSFORMATIONS Geometric transformations have been used previously for unsupervised feature learning by Gidaris et al. (2018) as well as by GEOM (Golan & El-Yaniv, 2018) for classiﬁcation-based anomaly de- tection. This set of transformations is hand-crafted to work well with convolutional neural networks (CNNs) which greatly beneﬁt from preserving neighborhood between pixels. This is however not a requirement for fully-connected networks. 4Published as a conference paper at ICLR 2020 Anomaly detection often deals with non-image datasets e.g. tabular data. Tabular data is very commonly used on the internet e.g. for cyber security or online advertising. Such data consists of both discrete and continuous attributes with no particular neighborhoods or order. The data is one- dimensional and rotations do not naturally generalize to it. To allow transformation-based methods to work on general data types, we therefore need to extend the class of transformations. We propose to generalize the set of transformations to the class of afﬁne transformations (where we have a total of M transformations): T(x,m) =Wmx+ bm (6) It is easy to verify that all geometric transformations in Golan & El-Yaniv (2018) (rotation by a multiple of 90 degrees, ﬂips and translations) are a special case of this class ( xin this case is the set of image pixels written as a vector). The afﬁne class is however much more general than mere permutations, and allows for dimensionality reduction, non-distance preservation and random trans- formation by sampling W, bfrom a random distribution. Apart from reduced variance across different dataset types where no apriori knowledge on the cor- rect transformation classes exists, random transformations are important for avoiding adversarial examples. Assume an adversary wishes to change the label of a particular sample from anomalous to normal or vice versa. This is the same as requiring that ˜P(m′|T(x,m)) has low or high proba- bility for m′= m. If T is chosen deterministically, the adversary may create adversarial examples against the known class of transformations (even if the exact network parameters are unknown). Conversely, if T is unknown, the adversary must create adversarial examples that generalize across different transformations, which reduces the effectiveness of the attack. To summarize, generalizing the set of transformations to the afﬁne class allows us to: generalize to non-image data, use an unlimited number of transformations and choose transformations randomly which reduces variance and defends against adversarial examples. 5 E XPERIMENTS We perform experiments to validate the effectiveness of our distance-based approach and the per- formance of the general class of transformations we introduced for non-image data. 5.1 I MAGE EXPERIMENTS Cifar10: To evaluate the performance of our method, we perform experiments on the Cifar10 dataset. We use the same architecture and parameter choices of Golan & El-Yaniv (2018), with our distance-based approach. We use the standard protocol of training on all training images of a single digit and testing on all test images. Results are reported in terms of AUC. In our method, we used a margin of s = 0.1 (we also run GOAD with s = 1, shown in the appendix). Similarly to He et al. (2018), to stabilize training, we added a softmax + cross entropy loss, as well as L2 norm regularization for the extracted features f(x). We compare our method with the deep one- class method of Ruff et al. (2018) as well as Golan & El-Yaniv (2018) without and with Dirichlet weighting. We believe the correct comparison is without Dirichlet post-processing, as we also do not use it in our method. Our distance based approach outperforms the SOTA approach by Golan & El-Yaniv (2018), both with and without Dirichlet (which seems to improve performance on a few classes). This gives evidence for the importance of considering the generalization behavior outside the normal region used in training. Note that we used the same geometric transformations as Golan & El-Yaniv (2018). Random afﬁne matrices did not perform competitively as they are not pixel order preserving, this information is effectively used by CNNs and removing this information hurts performance. This is a special property of CNN architectures and image/time series data. As a rule of thumb, fully-connected networks are not pixel order preserving and can fully utilize random afﬁne matrices. FasionMNIST: In Tab. 2, we present a comparison between our method (GOAD) and the strongest baseline methods (Deep SVDD and GEOM) on the FashionMNIST dataset. We used exactly the same setting as Golan & El-Yaniv (2018). GOAD was run with s = 1. OCSVM and GEOM 5Published as a conference paper at ICLR 2020 Table 1: Anomaly Detection Accuracy on Cifar10 (ROC-AUC%) Class Method Deep-SVDD GEOM (no Dirichlet) GEOM (w. Dirichlet) Ours 0 61.7 ±1.3 76.0 ±0.8 74.7 ±0.4 77.2 ±0.6 1 65.9 ±0.7 83.0 ±1.6 95.7 ±0.0 96.7 ±0.2 2 50.8 ±0.3 79.5 ±0.7 78.1 ±0.4 83.3 ±1.4 3 59.1 ±0.4 71.4 ±0.9 72.4 ±0.5 77.7 ±0.7 4 60.9 ±0.3 83.5 ±1.0 87.8 ±0.2 87.8 ±0.7 5 65.7 ±0.8 84.0 ±0.3 87.8 ±0.1 87.8 ±0.6 6 67.7 ±0.8 78.4 ±0.7 83.4 ±0.5 90.0 ±0.6 7 67.3 ±0.3 89.3 ±0.5 95.5 ±0.1 96.1 ±0.3 8 75.9 ±0.4 88.6 ±0.6 93.3 ±0.0 93.8 ±0.9 9 73.1 ±0.4 82.4 ±0.7 91.3 ±0.1 92.0 ±0.6 Average 64.8 81.6 86.0 88.2 Table 2: Anomaly Detection Accuracy on FashionMNIST (ROC-AUC%) Class Method Deep-SVDD GEOM (no Dirichlet) GEOM (w. Dirichlet) Ours 0 98.2 77.8 ±5.9 99.4 ±0.0 94.1 ±0.9 1 90.3 79.1 ±16.3 97.6 ±0.1 98.5 ±0.3 2 90.7 80.8 ±6.9 91.1 ±0.2 90.8 ±0.4 3 94.2 79.2 ±9.1 89.9 ±0.4 91.6 ±0.9 4 89.4 77.8 ±3.3 92.1 ±0.0 91.4 ±0.3 5 91.8 58.0 ±29.4 93.4 ±0.9 94.8 ±0.5 6 83.4 73.6 ±8.7 83.3 ±0.1 83.4 ±0.4 7 98.8 87.4 ±11.4 98.9 ±0.1 97.9 ±0.4 8 91.9 84.6 ±5.6 90.8 ±0.1 98.9 ±0.1 9 99.0 99.5 ±0.0 99.2 ±0.0 99.2 ±0.3 Average 92.8 79.8 93.5 94.1 with Dirichlet were copied from their paper. We run their method without Dirichlet and presented it in the table (we veriﬁed the implementation by running their code with Dirichlet and replicated the numbers in the paper). It appears that GEOM is quite dependent on Dirichlet for this dataset, whereas we do not use it at all. GOAD outperforms all the baseline methods. Adversarial Robustness:Let us assume an attack model where the attacker knows the architecture and the normal training data and is trying to minimally modify anomalies to look normal. We exam- ine the merits of two settings i) the adversary knows the transformations used (non-random) ii) the adversary uses another set of transformations. To measure the beneﬁt of the randomized transfor- mations, we train three networks A, B, C. Networks A and B use exactly the same transformations but random parameter initialization prior to training. Network C is trained using other randomly se- lected transformations. The adversary creates adversarial examples using PGD (Madry et al., 2017) based on network A (making anomalies appear like normal data). On Cifar10, we randomly selected 8 transformations from the full set of 72 for Aand B, another randomly selected 8 transformations are used for C. We measure the increase of false classiﬁcation rate on the adversarial examples us- ing the three networks. The average increase in performance of classifying transformation correctly on anomalies (causing lower anomaly scores) on the original network A was 12.8%, the transfer performance for B causes an increase by 5.0% on network Bwhich shared the same set of transfor- mation, and 3% on network C that used other rotations. This shows the beneﬁts of using random transformations. 6Published as a conference paper at ICLR 2020 Table 3: Anomaly Detection Accuracy (%) Method Dataset Arrhythmia Thyroid KDD KDDRev F1 Score σ F 1 Score σ F 1 Score σ F 1 Score σ OC-SVM 45.8 38.9 79.5 83.2 E2E-AE 45.9 11.8 0.3 74.5 LOF 50.0 0.0 52.7 0.0 83.8 5.2 81.6 3.6 DAGMM 49.8 47.8 93.7 93.8 FB-AE 51.5 1.6 75.0 0.8 92.7 0.3 95.9 0.4 GOAD(Ours) 52.0 2.3 74.5 1.1 98.4 0.2 98.9 0.3 5.2 T ABULAR DATA EXPERIMENTS Datasets: We evaluate on small-scale medical datasets Arrhythmia, Thyroid as well as large-scale cyber intrusion detection datasets KDD and KDDRev. Our conﬁguration follows that of Zong et al. (2018). Categorical attributes are encoded as one-hot vectors. For completeness the datasets are described in the appendix A.2. We train all compared methods on 50% of the normal data. The methods are evaluated on 50% of the normal data as well as all the anomalies. Baseline methods: The baseline methods evaluated are: One-Class SVM (OC-SVM, Scholkopf et al. (2000)), End-to-End Autoencoder (E2E-AE), Local Outlier Factor (LOF, Breunig et al. (2000)). We also evaluated deep distributional method DAGMM (Zong et al., 2018), choosing their strongest variant. To compare against ensemble methods e.g. Chen et al. (2017), we implemented the Fea- ture Bagging Autoencoder (FB-AE) with autoencoders as the base classiﬁer, feature bagging as the source of randomization, and average reconstruction error as the anomaly score. OC-SVM, E2E-AE and DAGMM results are directly taken from those reported by Zong et al. (2018). LOF and FB-AE were computed by us. Implementation of GOAD: We randomly sampled transformation matrices using the normal distri- bution for each element. Each matrix has dimensionality L×r, where Lis the data dimension and ris a reduced dimension. For Arryhthmia and Thyroid we used r = 32, for KDD and KDDrev we used r = 128and r = 64respectively, the latter due to high memory requirements. We used 256 tasks for all datasets apart from KDD ( 64) due to high memory requirements. We set the bias term to 0. For C we used fully-connected hidden layers and leaky-ReLU activations (8 hidden nodes for the small datasets, 128 and 32 for KDDRev and KDD). We optimized using ADAM with a learning rate of 0.001. Similarly to He et al. (2018), to stabilize the triplet center loss training, we added a softmax + cross entropy loss. We repeated the large-scale experiments 5 times, and the small scale GOAD experiments500 times (due to the high variance). We report the mean and standard deviation (σ). Following the protocol in Zong et al. (2018), the decision threshold value is chosen to result in the correct number of anomalies e.g. if the test set contains Na anomalies, the threshold is selected so that the highest Na scoring examples are classiﬁed as anomalies. True positives and negatives are evaluated in the usual way. Some experiments copied from other papers did not measure standard variation and we kept the relevant cell blank. Results Arrhythmia: The Arrhythmia dataset was the smallest examined. A quantitative comparison on this dataset can be seen in Tab. 3. OC-SVM and DAGMM performed reasonably well. Our method is comparable to FB-AE. A linear classiﬁer C performed better than deeper networks (which suffered from overﬁtting). Early stopping after a single epoch generated the best results. Thyroid: Thyroid is a small dataset, with a low anomaly to normal ratio and low feature dimension- ality. A quantitative comparison on this dataset can be seen in Tab. 3. Most baselines performed about equally well, probably due to the low dimensionality. On this dataset, we also found that early stopping after a single epoch gave the best results. The best results on this dataset, were obtained with a linear classiﬁer. Our method is comparable to FB-AE and beat all other baselines by a wide margin. 7Published as a conference paper at ICLR 2020 Figure 1: Left: Classiﬁcation error for our method and DAGMM as a function of percentage of the anomalous examples in the training set (on the KDDCUP99 dataset). Our method consistently outperforms the baseline. Right: Classiﬁcation error as a function of the number of transforma- tions (on the KDDRev dataset). The error and instability decrease as a function of the number of transformations. For both, lower is better. KDDCUP99: The UCI KDD10% dataset is the largest dataset examined. A quantitative comparison on this dataset can be seen in Tab. 3. The strongest baselines are FB-AE and DAGMM. Our method signiﬁcantly outperformed all baselines. We found that large datasets have different dynamics from very small datasets. On this dataset, deep networks performed the best. We also, did not need early stopping. The results are reported after 25 epochs. KDD-Rev: The KDD-Rev dataset is a large dataset, but smaller than KDDCUP99 dataset. A quanti- tative comparison on this dataset can be seen in Tab. 3. Similarly to KDDCUP99, the best baselines are FB-AE and DAGMM, where FB-AE signiﬁcantly outperforms DAGMM. Our method signiﬁ- cantly outperformed all baselines. Due to the size of the dataset, we did not need early stopping. The results are reported after 25 epochs. Adversarial Robustness: Due to the large number of transformations and relatively small networks, adversarial examples are less of a problem for tabular data. PGD generally failed to obtain adversar- ial examples on these datasets. On KDD, transformation classiﬁcation accuracy on anomalies was increased by 3.7% for the network the adversarial examples were trained on, 1.3% when transfer- ring to the network with the same transformation and only0.2% on the network with other randomly selected transformations. This again shows increased adversarial robustness due to random transfor- mations. Further Analysis Contaminated Data: This paper deals with the semi-supervised scenario i.e. when the training dataset contains only normal data. In some scenarios, such data might not be available but instead we might have a training dataset that contains a small percentage of anomalies. To evaluate the robustness of our method to this unsupervised scenario, we analysed the KDDCUP99 dataset, when X% of the training data is anomalous. To prepare the data, we used the same normal training data as before and added further anomalous examples. The test data consists of the same proportions as before. The results are shown in Fig. 1. Our method signiﬁcantly outperforms DAGMM for all impurity values, and degrades more graceful than the baseline. This attests to the effectiveness of our approach. Results for the other datasets are presented in Fig. 3, showing similar robustness to contamination. Number of Tasks: One of the advantages of GOAD, is the ability to generate any number of tasks. We present the anomaly detection performance on the KDD-Rev dataset with different numbers of tasks in Fig. 1. We note that a small number of tasks (less than 16) leads to poor results. From 16 tasks, the accuracy remains stable. We found that on the smaller datasets (Thyroid, Arrhythmia) using a larger number of transformations continued to reduce F1 score variance between differently initialized runs (Fig. 2). 8Published as a conference paper at ICLR 2020 6 D ISCUSSION Openset vs. Softmax: The openset-based classiﬁcation presented by GOAD resulted in performance improvement over the closed-set softmax approach on Cifar10 and FasionMNIST. In our experi- ments, it has also improved performance in KDDRev. Arrhythmia and Thyroid were comparable. As a negative result, performance of softmax was better on KDD (F1 = 0.99). Choosing the margin parameter s: GOAD is not particularly sensitive to the choice of margin parameter s, although choosing sthat is too small might cause some instability. We used a ﬁxed value of s= 1in our experiments, and recommend this value as a starting point. Other transformations: GOAD can also work with other types of transformations such as rotations or permutations for tabular data. In our experiments, we observed that these transformation types perform comparably but a little worse than afﬁne transformations. Unsupervised training: Although most of our results are semi-supervised i.e. assume that no anoma- lies exist in the training set, we presented results showing that our method is more robust than strong baselines to a small percentage of anomalies in the training set. We further presented results in other datasets showing that our method degrades gracefully with a small amount of contamination. Our method might therefore be considered in the unsupervised settings. Deep vs. shallow classiﬁers: Our experiments show that for large datasets deep networks are ben- eﬁcial (particularly for the full KDDCUP99), but are not needed for smaller datasets (indicating that deep learning has not beneﬁted the smaller datasets). For performance critical operations, our approach may be used in a linear setting. This may also aid future theoretical analysis of our method. 7 C ONCLUSION In this paper, we presented a method for detecting anomalies for general data. This was achieved by training a classiﬁer on a set of random auxiliary tasks. Our method does not require knowledge of the data domain, and we are able to generate an arbitrary number of random tasks. Our method signiﬁcantly improve over the state-of-the-art. REFERENCES Arthur Asuncion and David Newman. Uci machine learning repository, 2007. Abhijit Bendale and Terrance E Boult. Towards open set deep networks. InProceedings of the IEEE conference on computer vision and pattern recognition, pp. 1563–1572, 2016. Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J¨org Sander. Lof: identifying density- based local outliers. In ACM sigmod record, volume 29, pp. 93–104. ACM, 2000. Emmanuel J Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis? JACM, 2011. Jinghui Chen, Saket Sathe, Charu Aggarwal, and Deepak Turaga. Outlier detection with autoencoder ensembles. In ICDM, 2017. Lucas Deecke, Robert Vandermeulen, Lukas Ruff, Stephan Mandt, and Marius Kloft. Anomaly detection with generative adversarial networks. In ICLR, 2018. Eleazar Eskin, Andrew Arnold, Michael Prerau, Leonid Portnoy, and Sal Stolfo. A geometric frame- work for unsupervised anomaly detection. In Applications of data mining in computer security , pp. 77–101. Springer, 2002. Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. ICLR, 2018. Izhak Golan and Ran El-Yaniv. Deep anomaly detection using geometric transformations. In NeurIPS, 2018. 9Published as a conference paper at ICLR 2020 John A Hartigan and Manchek A Wong. Algorithm as 136: A k-means clustering algorithm.Journal of the Royal Statistical Society. Series C (Applied Statistics), 1979. Xinwei He, Yang Zhou, Zhichao Zhou, Song Bai, and Xiang Bai. Triplet-center loss for multi- view 3d object retrieval. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1945–1954, 2018. Dan Hendrycks, Mantas Mazeika, and Thomas G Dietterich. Deep anomaly detection with outlier exposure. arXiv preprint arXiv:1812.04606, 2018. Ian Jolliffe. Principal component analysis. Springer, 2011. Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Learning representations for auto- matic colorization. In ECCV, 2016. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. Michael Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond mean square error. ICLR, 2016. Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In ECCV, 2016. Emanuel Parzen. On estimation of a probability density function and mode. The annals of mathe- matical statistics, 1962. Shebuti Rayana. ODDS library http://odds.cs.stonybrook.edu, 2016. Lukas Ruff, Nico Gornitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Robert Vandermeulen, Alexan- der Binder, Emmanuel M¨uller, and Marius Kloft. Deep one-class classiﬁcation. In ICML, 2018. Mayu Sakurada and Takehisa Yairi. Anomaly detection using autoencoders with nonlinear dimen- sionality reduction. In MLSD. ACM, 2014. Thomas Schlegl, Philipp Seeb ¨ock, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In International Conference on Information Processing in Medical Imaging, 2017. Bernhard Scholkopf, Robert C Williamson, Alex J Smola, John Shawe-Taylor, and John C Platt. Support vector method for novelty detection. In NIPS, 2000. Yan Xia, Xudong Cao, Fang Wen, Gang Hua, and Jian Sun. Learning discriminative reconstructions for unsupervised outlier removal. In ECCV, 2015. Bo Yang, Xiao Fu, Nicholas D Sidiropoulos, and Mingyi Hong. Towards k-means-friendly spaces: Simultaneous deep learning and clustering. In ICML, 2017. Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016. Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, and Haifeng Chen. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. ICLR, 2018. A A PPENDIX A.1 I MAGE EXPERIMENTS Sensitive to margins: We run Cifar10 experiments with s = 0.1 and s = 1 and presented the results in Fig. 4. The results were not affected much by the margin parameter. This is in-line with the rest of our empirical observations that GOAD is not very sensitive to the margin parameter. 10Published as a conference paper at ICLR 2020 Table 4: Anomaly Detection Accuracy on Cifar10 (%) Class Method GEOM (w. Dirichlet) GOAD( s= 0.1) GOAD( 1.0) 0 74.7 ±0.4 77.2 ±0.6 77.9 ±0.7 1 95.7 ±0.0 96.7 ±0.2 96.4 ±0.9 2 78.1 ±0.4 83.3 ±1.4 81.8 ±0.8 3 72.4 ±0.5 77.7 ±0.7 77.0 ±0.7 4 87.8 ±0.2 87.8 ±0.7 87.7 ±0.5 5 87.8 ±0.1 87.8 ±0.6 87.8 ±0.7 6 83.4 ±0.5 90.0 ±0.6 90.9 ±0.5 7 95.5 ±0.1 96.1 ±0.3 96.1 ±0.2 8 93.3 ±0.0 93.8 ±0.9 93.3 ±0.1 9 91.3 ±0.1 92.0 ±0.6 92.4 ±0.3 Average 86.0 88.2 88.1 A.2 T ABULAR DATASETS Following the evaluation protocol of Zong et al. (2018), 4 datasets are used in this comparison: Arrhythmia: A cardiology dataset from the UCI repository (Asuncion & Newman, 2007) contain- ing attributes related to the diagnosis of cardiac arrhythmia in patients. The datasets consists of 16 classes: class 1 are normal patients, 2-15 contain different arrhythmia conditions, and class 16 con- tains undiagnosed cases. Following the protocol established by ODDS (Rayana, 2016), the smallest classes: 3,4,5,7,8,9,14,15 are taken to be anomalous and the rest normal. Also following ODDS, the categorical attributes are dropped, the ﬁnal attributes total 274. Thyroid: A medical dataset from the UCI repository (Asuncion & Newman, 2007), containing at- tributes related to whether a patient is hyperthyroid. Following ODDS (Rayana, 2016), from the 3 classes of the dataset, we designate hyperfunction as the anomalous class and the rest as normal. Also following ODDS only the 6 continuous attributes are used. KDD: The KDD Intrusion Detection dataset was created by an extensive simulation of a US Air Force LAN network. The dataset consists of the normal and 4 simulated attack types: denial of service, unauthorized access from a remote machine, unauthorized access from local superuser and probing. The dataset consists of around 5 million TCP connection records. Following the evaluation protocol in Zong et al. (2018), we use the UCI KDD 10% dataset, which is a subsampled version of the original dataset. The dataset contains 41 different attributes. 34 are continuous and 7 are categorical. Following Zong et al. (2018), we encode the categorical attributes using1-hot encoding. Following Zong et al. (2018), we evaluate two different settings for the KDD dataset: KDDCUP99: In this conﬁguration we use the entire UCI 10% dataset. As the non-attack class consists of only 20% of the dataset, it is treated as the anomaly in this case, while attacks are treated as normal. KDDCUP99-Rev: To better correspond to the actual use-case, in which the non-attack scenario is normal and attacks are anomalous, Zong et al. (2018) also evaluate on the reverse conﬁguration, in which the attack data is sub-sampled to consist of 25% of the number of non-attack samples. The attack data is in this case designated as anomalous (the reverse of the KDDCUP99 dataset). In all the above datasets, the methods are trained on 50% of the normal data. The methods are evaluated on 50% of the normal data as well as all the anomalies. A.3 N UMBER OF TASKS We provide plots of the number of auxiliary tasks vs. the anomaly detection accuracy (measured by F1) for all datasets. The results are presented in Fig. 2. Performance increases rapidly up to a certain number of tasks (around 16). Afterwards more tasks reduce the variance of F1 scores between runs. 11Published as a conference paper at ICLR 2020 a) ‘ b) c) ‘ d) Figure 2: Plots of the number of auxiliary tasks vs. the anomaly detection accuracy (measured by F1) a) Arrhythmia b) Thyroid c) KDDRev d) KDDCup99 Accuracy often increases with the number of tasks, although the rate diminishes with the number of tasks. Figure 3: Plots of the degree of contamination vs. the anomaly detection accuracy (measured byF1) (left) KDDRev (center) KDDCup99 (right) Arrhythmia. GOAD is generally robust to the degree of contamination. A.4 C ONTAMINATION EXPERIMENTS We conduct contamination experiments for 3 datasets. Thyroid was omitted due to not having a sufﬁcient number of anomalies. The protocol is different than that of KDDRev as we do not have unused anomalies for contamination. Instead, we split the anomalies into train and test. Train anomalies are used for contamination, test anomalies are used for evaluation. As DAGMM did not present results for the other datasets, we only present GOAD. GOAD was reasonably robust to contamination on KDD, KDDRev and Arrhythmia. The results are presented in Fig. 3 12",
      "references": [
        "Towards open set deep networks.",
        "Lof: identifying density- based local outliers.",
        "Robust principal component analysis?",
        "Outlier detection with autoencoder ensembles.",
        "Anomaly detection with generative adversarial networks.",
        "A geometric frame- work for unsupervised anomaly detection.",
        "Unsupervised representation learning by predicting image rotations.",
        "Deep anomaly detection using geometric transformations.",
        "Algorithm as 136: A k-means clustering algorithm.",
        "Triplet-center loss for multi- view 3d object retrieval.",
        "Deep anomaly detection with outlier exposure.",
        "Principal component analysis.",
        "Learning representations for auto- matic colorization.",
        "Towards deep learning models resistant to adversarial attacks.",
        "Deep multi-scale video prediction beyond mean square error.",
        "Unsupervised learning of visual representations by solving jigsaw puzzles.",
        "On estimation of a probability density function and mode.",
        "Deep one-class classification.",
        "Anomaly detection using autoencoders with nonlinear dimen- sionality reduction.",
        "Unsupervised anomaly detection with generative adversarial networks to guide marker discovery.",
        "Support vector method for novelty detection.",
        "Learning discriminative reconstructions for unsupervised outlier removal.",
        "Towards k-means-friendly spaces: Simultaneous deep learning and clustering.",
        "Colorful image colorization.",
        "Deep autoencoding gaussian mixture model for unsupervised anomaly detection."
      ],
      "meta_data": {
        "arxiv_id": "2005.02359v1",
        "authors": [
          "Liron Bergman",
          "Yedid Hoshen"
        ],
        "published_date": "2020-05-05T17:44:40Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces GOAD, a novel method for anomaly detection that unifies one-class and classification-based approaches using multiple data transformations. It advances the state-of-the-art by utilizing a distance-based framework with open-set classification principles and extends transformation-based methods to non-image data via random affine transformations.",
        "methodology": "GOAD transforms each input sample using M different transformations and maps them into a feature space using a neural network. It then models each transformed feature set as a sphere with a computed center. A center triplet loss is employed to achieve low intra-class and high inter-class variation among transformation-specific clusters, and anomaly scores are computed based on the aggregated log probabilities (with added regularization) of the transformed samples aligning with their respective clusters.",
        "experimental_setup": "The method is evaluated on both image datasets (CIFAR-10, FashionMNIST) and tabular datasets (Arrhythmia, Thyroid, KDD, and KDD-Rev). It is compared with several baselines including Deep SVDD, GEOM, one-class SVM, autoencoder-based methods, LOF, DAGMM, and FB-AE. Experiments also include evaluations under adversarial attacks (using PGD), analysis of model performance as a function of transformation count, and robustness tests under data contamination scenarios.",
        "limitations": "While GOAD demonstrates robustness and improved performance, it has several limitations: the method may become unstable if hyperparameters such as the margin parameter are set too low; its performance on image data can be sensitive to transformation choice (e.g., random affine transformations underperform compared to geometric transformations with CNNs); and deep network architectures may overfit on smaller datasets where simpler models could suffice.",
        "future_research_directions": "Potential research directions include extending GOAD to completely unsupervised settings, exploring alternative transformation sets or adaptive transformation strategies, improving adversarial robustness further by experimenting with randomized and dynamic transformations, refining the learned feature space with alternative loss functions or distance metrics, and applying the framework to other data domains beyond images and tabular data to assess its generalizability.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Explanation-based Data Augmentation for Image Classification",
      "full_text": "",
      "references": [],
      "meta_data": {
        "arxiv_id": "",
        "authors": [],
        "published_date": "",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "IDEAL: Inexact DEcentralized Accelerated Augmented Lagrangian Method",
      "full_text": "IDEAL: Inexact DEcentralized Accelerated Augmented Lagrangian Method Yossi Arjevani NYU yossia@nyu.edu Joan Bruna NYU bruna@cims.nyu.edu Bugra Can Rutgers University bc600@scarletmail.rutgers.edu Mert Gürbüzbalaban Rutgers University mg1366@rutgers.edu Stefanie Jegelka MIT stefje@csail.mit.edu Hongzhou Lin MIT hongzhou@mit.edu Abstract We introduce a framework for designing primal methods under the decentralized optimization setting where local functions are smooth and strongly convex. Our approach consists of approximately solving a sequence of sub-problems induced by the accelerated augmented Lagrangian method, thereby providing a systematic way for deriving several well-known decentralized algorithms including EXTRA [41] and SSDA [37]. When coupled with accelerated gradient descent, our framework yields a novel primal algorithm whose convergence rate is optimal and matched by recently derived lower bounds. We provide experimental results that demonstrate the effectiveness of the proposed algorithm on highly ill-conditioned problems. 1 Introduction Due to their rapidly increasing size, modern datasets are typically collected, stored and manipulated in a distributed manner. This, together with strict privacy requirements, has created a large demand for efﬁcient solvers for the decentralized setting in which models are trained locally at each agent, and only local parameter vectors are shared. This approach has become particularly appealing for applications such as edge computing [25, 42], cooperative multi-agent learning [6, 33] and federated learning [26, 43]. Clearly, the nature of the decentralized setting prevents a global synchronization, as only communication within the neighboring machines is allowed. The goal is then to arrive at a consensus on all local agents with a model that performs as well as in the centralized setting. Arguably, the simplest approach for addressing decentralized settings is to adapt the vanilla gradient descent method to the underlying network architecture [9, 16, 29, 47]. To this end, the connections between the agents are modeled through a mixing matrix, which dictates how agents average over their neighbors’ parameter vectors. Thus, the mixing matrix serves as a communication oracle which determines how information propagates throughout the network. Perhaps surprisingly, when the stepsizes are constant, simply averaging over the local iterates via the mixing matrix only converges to a neighborhood of the optimum [41, 50]. A recent line of works [15, 30, 31, 34, 40, 41] proposed a number of alternative methods that linearly converge to the global minimum. The overall complexity of solving decentralized optimization problems is typically determined by two factors: (i) the condition number of the objective function κf, which measures the ‘hardness’ of solving the underlying optimization problem, and (ii) the condition number of the mixing matrix κW, which quantiﬁes the severity of information ‘bottlenecks’ present in the network. Lower complexity bounds recently derived for distributed settings [1, 3, 37, 46] show that one cannot expect to have a better dependence on the condition numbers than √κf and √κW. Notably, despite the considerable Preprint. Under review. arXiv:2006.06733v1  [math.OC]  11 Jun 2020recent progress, none of the methods mentioned above is able to achieve accelerated rates, that is, a square root dependence for both κf and κW—simultaneously. An extensive effort has been devoted to obtaining acceleration for decentralized algorithms under various settings [10, 11, 14, 22, 37, 38, 45, 48, 51]. When a dual oracle is available, that is, access to the gradients of the dual functions is provided, optimal rates can be attained for smooth and strongly convex objectives [37]. However, having access to a dual oracle is a very restrictive assumption, and resorting to a direct ‘primalization’ through inexact approximation of the dual gradients leads to sub-optimal worst-case theoretical rates [45]. In this work, we propose a novel primal approach that leads to optimal rates in terms of dependency on κf and κW. Our contributions can be summarized as follows. • We introduce a novel framework based on the accelerated augmented Lagrangian method for designing primal decentralized methods. The framework provides a simple and systematic way for deriving several well-known decentralized algorithms [15, 40, 41], including EXTRA [41] and SSDA [37], and uniﬁes their convergence analyses. • Using accelerated gradient descent as a sub-routine, we derive a novel method for smooth and strongly convex local functions which achieves optimal accelerated rates on both the condition numbers of the problem, κf and κW, using primal updates, see Table 2. • We perform a large number of experiments, which conﬁrm our theoretical ﬁndings, and demon- strate a signiﬁcant improvement when the objective function is ill-conditioned and κf ≫κW. 2 Decentralized Optimization Setting We consider ncomputational agents and a network graph G= (V,E) which deﬁnes how the agents are linked. The set of vertices V= {1,··· ,n}represents the agents and the set of edges E∈V×V speciﬁes the connectivity in the network, i.e., a communication link between agents iand jexists if and only if (i,j) ∈E. Each agent has access to local information encoded by a loss function fi : Rd →R. The goal is to minimize the global objective over the entire network, min x∈Rd f(x) := n∑ i=1 fi(x). (1) In this paper, we assume that the local loss functions fi are differentiable, L-smooth and µ-strongly convex.1 Strong convexity of the component functions fi implies that the problem admits a unique solution, which we denote by x∗. We consider the following computation and communication models [37]: • Local computation: Each agent is able to compute the gradients of fi and the cost of this computation is one unit of time. • Communication: Communication is done synchronously, and each agent can only exchange information with its neighbors, where iis a neighbor of j if (i,j) ∈E. The ratio between the communication cost and computation cost per round is denoted by τ. We further assume that propagation of information is governed by a mixing matrix W ∈Rn×n [29, 37, 50]. Speciﬁcally, given a local copy of the decision variable xi ∈Rd at node i∈[1,n], one round of communication provides the following update xi ←∑n i=1 Wijxj. The following standard assumptions regarding the mixing matrix [37] are made throughout the paper. Assumption 1. The mixing matrix W satisﬁes the following: 1. Symmetry: W = WT. 2. Positiveness: W is positive semi-deﬁnite. 3. Decentralized property: If (i,j) /∈E and i̸= j, then Wij = Wji = 0. 4. Spectrum property:The kernel of W is given by the vector of all ones Ker(W) = R1n. 1 f is L-smooth if ∇f is L-Lipschitz; f is µ-strongly convex if f −µ 2 ∥x∥2 is convex. 2Algorithm 1Decentralized Augmented Lagrangian framework Input: mixing matrix W, regularization parameter ρ, stepsize η. 1: for k= 1,2,...,K do 2: Xk = arg min { Pk(X) := F(X) + ΛT kX + ρ 2 ∥X∥2 W } . 3: Λk+1 = Λk + ηWXk. 4: end for A typical choice of the mixing matrix is the (weighted) Laplacian matrix of the graph. Another common choice is to set W as I − ˜W where ˜W is a doubly stochastic matrix [ 5, 8, 41]. By Assumption 1.4, all the eigenvalues of W are strictly positive, except for the smallest one. We let λmax(W) denote the maximum eigenvalue, and let λ+ min(W) denote the smallest positive eigenvalue. The ratio between these two quantities plays an important role in quantifying the overall complexity of this problem. Theorem 1(Decentralized lower bound [37]). For any ﬁrst-order black-box decentralized method, the number of time units required to reach anϵ-optimal solution for (1) is lower bounded by Ω (√κf(1 + τ√κW) log (1 ϵ )) , (2) where κf = L/µis the condition number of the loss function and κW = λmax(W)/λ+ min(W) is the condition number of the mixing matrix. The lower bound decomposes as follows: a) computation cost, given by √κf log(1/ϵ), and b) communication cost, given by τ√κfκW log(1/ϵ). The computation cost matches lower bounds for centralized settings [2, 32], while the communication cost introduces an additional term which depends on κW and accounts for the ‘price’ of communication in decentralized models. It follows that the effective condition number of a given decentralized problem is κWκf. Clearly, the choice of the matrix W can strongly affect the optimal attainable performance. For example, κW can get as large as n2 in the line/cycle graph, or be constant in the complete graph. In this paper, we do not focus on optimizing over the choice of W for a given graph G; instead, following the approach taken by existing decentralized algorithms, we assume that the graph Gand the mixing matrix W are given and aim to achieve the optimal complexity (2) for this particular choice of W. 3 Related Work and the Dual Formulation A standard approach to adress problem (1) is to express it as a constrained optimization problem min X∈Rnd F(X) := 1 n n∑ i=1 fi(xi) such that x1 = x2 = ··· = xn ∈Rd, (P) where X = [x1; x2; ···xn] ∈Rnd is a concatenation of the vectors. To lighten the notation, we introduce the global mixing matrix W = W ⊗Id ∈Rnd×nd, where ⊗denotes the Kronecker product, and let ∥·∥ W denote the semi-norm induced by W, i.e. ∥X∥2 W = XTWX. With this notation in hand, we brieﬂy review existing literature on decentralized algorithms. Decentralized Gradient Descent The decentralized gradient method [29, 50] has the update rule Xk+1 = WXk −η∇F(Xk). (DGD) However, with constant stepsize, the algorithm does not converge to a global minimum of(P), but rather to a neighborhood of the solution [50]. A decreasing stepsize schedule may be used to ensure convergence, but this yields a sublinear convergence rate, even in the strongly convex case. Linearly convergent primal algorithms By and large, recent methods that achieve linear con- vergence in the strongly convex case [15, 30, 31, 34, 40, 41, 44] can be shown to follow a general framework based on the augmented Lagrangian method, see Algorithm 1; The main difference lies 3Algorithm 2Accelerated Decentralized Augmented Lagrangian framework Input: mixing matrix W, regularization parameter ρ, stepsize η, extrapolation parameters {βk}k∈N 1: Initialize dual variables Λ1 = Ω1 = 0 ∈Rnd. 2: for k= 1,2,...,K do 3: Xk = arg min { Pk(X) := F(X) + ΩT kX + ρ 2 ∥X∥2 W } . 4: Λk+1 = Ωk + ηWXk 5: Ωk+1 = Λk+1 + βk+1(Λk+1 −Λk) 6: end for Output: XK. Algorithm 3IDEAL: Inexact Acc-Decentralized Augmented Lagrangian framework Additional Input:A ﬁrst-order optimization algorithm A Apply Ato solve the subproblem Pk warm starting at Xk−1 to ﬁnd an approximate solution Xk ≈arg min { Pk(X) := F(X) + ΩT kX + ρ 2∥X∥2 W } , Option I: stop the algorithm when ∥Xk −X∗ k∥2 ≤ϵk, where X∗ k is the unique minimizer of Pk. Option II: stop the algorithm after a preﬁxed number of iterations Tk. in how subproblems Pk are solved. Shi et al. [40] apply an alternating directions method; in [41], the EXTRA algorithm takes a single gradient descent step to solve Pk, see Appendix B for details. Jakoveti´c et al. [15] use multi-step algorithms such as Jacobi/Gauss-Seidel methods. To the best of our knowledge, the complexity of these algorithms is not better than O ( (1 + τ)κfκW log(1 ϵ) ) , in other words, they are non-accelerated. The recently proposed algorithm APM-C [22] enjoys a square root dependence on κf and κW, but incurs an additional log(1/ϵ) factor compared to the optimal attainable rate. Optimal method based on the dual formulationBy Assumption 1.4, the constraint x1 = x2 = ··· = xn is equivalent to the identity W ·X = 0, which is again equivalent to √ W ·X = 0. Hence, the dual formulation of (P) is given by max Λ∈Rdn −F∗(− √ WΛ). (D) Since the primal function is convex and the constraints are linear, we can use strong duality and address the dual problem instead of the primal one. Using this approach, [37] proposed a dual method with optimal accelerated rates, using Nesterov’s accelerated gradient method for the dual problem (D). As mentioned earlier, the main drawback of this method is that it requires access to the gradient of the dual function which, unless the primal function has a relatively simple structure, is not available. One may apply a ﬁrst-order method to approximate the dual gradients inexactly at the expense of an additional √κf factor in the computation cost [45], but this woul make the algorithm no longer optimal. This indicates that achieving optimal rates when using primal updates is a rather challenging task in the decentralized setting. In the following sections, we provide a generic framework which allows us to derive a primal decentralized method with optimal complexity guarantees. 4 An Inexact Accelerated Augmented Lagrangian framework In this section, we introduce our inexact accelerated Augmented Lagrangian framework, and show how to combine it with Nesterov’s acceleration. To ease the presentation, we ﬁrst describe a conceptual algorithm, Algorithm 2, where subproblems are solved exactly, and only then introduce inexact inner-solvers. Similarly to Nesterov’s accelerated gradient method, we use an extrapolation step for the dual variable Λk. The component WXk in line 4 of Algorithm 2 is the negative gradient of the Moreau- envelope2 of the dual function. Hence our algorithm is equivalent to applying Nesterov’s method 2A proper deﬁnition of the Moreau-envelope is given in [36], readers that are not familiar with this concept could take it as an implicit function which shares the same optimum as the original function. 4on the Moreau-envelope of the dual function, or equivalently, an accelerated dual proximal point algorithm. This renders the optimal dual method proposed in [37] as a special case of our algorithmic framework (with ρset to 0). While Algorithm 2 is conceptually plausible, it requires an exact solution of the Augmented La- grangian problems, which can be too expensive in practice. To address this issue, we introduce an inexact version, shown in Algorithm 3, where the k-th subproblem Pk is solved up to a predeﬁned accuracy ϵk. The choice of ϵk is rather subtle. On the one hand, choosing a large ϵk may result in a non-converging algorithm. On the other hand, choosing a small ϵk can be exceedingly expensive as the optimal solution of the subproblem X∗ k is not the global optimum X∗. Intuitively, ϵk should be chosen to be of the same order of magnitude as ∥X∗ k −X∗∥, leading to the following result. Theorem 2. Consider the sequence of primal variables (Xk)k∈N generated by Algorithm 3 with the subproblem Pk solved up to ϵk accuracy in Option I. With parameters set to βk = √ Lρ −√µρ √ Lρ + √µρ , η = 1 Lρ , ϵ k = µρ 2λmax(W) ( 1 −1 2 √µρ Lρ )k ∆dual, (3) where Lρ = λmax(W) µ+ρλmax(W) , µρ = λ+ min(W) L+ρλ+ min(W) and ∆dual is the initial dual function gap, we obtain ∥Xk −X∗∥2 ≤Cρ ( 1 −1 2 √µρ Lρ )k ∆dual, (4) where X∗= 1n ⊗x∗and Cρ = 258Lρλmax(W) µ2µ2ρ . Corollary 3. The number of subproblems Pk to achieve ∥Xk −X∗∥2 ≤ϵin IDEAL is bounded by K = O (√ Lρ µρ log (Cρ∆dual ϵ )) . (5) We remark that inexact accelerated Augmented Lagrangian methods have been previously analyzed under different assumptions [19, 28, 49]. The main difference is that here, we are able to establish a linear convergence rate, whereas existing analyses only yield sublinear rates. One of the reasons for this discrepancy is that, although F∗is strongly convex, the dual problem (D) is not, as the mixing matrix W is singular. The key to obtaining a linear convergence rate is a ﬁne-grained analysis of the dual problem, showing that the dual variables always lie in the subspace where strong convexity holds. The proof of the theorem relies on the equivalence between Augmented Lagrangian methods and the dual proximal point algorithm [7, 35], which can be interpreted as applying an inexact accelerated proximal point algorithm [13, 23] to the dual problem. A complete convergence analysis is deferred to Section C in the appendix. Theorem 2 provides an accelerated convergence rate with respect to the ‘augmented’ condition number κρ := Lρ/µρ, as determined by the Augmented Lagrangian parameter ρin Algorithm 3. We have the following bounds: 1 ρ=∞ ≤κρ = L+ ρλ+ min(W) µ+ ρλmax(W) λmax(W) λ+ min(W) ≤L µ λmax(W) λ+ min(W)   ρ=0 = κfκW, (6) where we observe that the condition number κρ is a decreasing function of the regularization parameter ρ. When ρ= 0, the maximum value is attained at κρ = κfκW, the effective condition number of the decentralized problem. As ρgoes to inﬁnity, the augmented condition number κρ goes to 1. Naively, one may want to take ρas large as possible to get a fast convergence. However, one must also take into account the complexity of solving the subproblems. Indeed, since W is singular, the additional regularization term in Pk does not improve the strong convexity of the subproblems, yielding an increase in inner loops complexity as ρgrows. Hence, the optimal choice of ρrequires balancing the inner and outer complexity in a careful manner. To study the inner loop complexity, we introduce a warm-start strategy. Intuitively, the distance between Xk−1 and the k-th solution X∗ k to the subproblem Pk is roughly on the order of ϵk−1. More precisely, we have the following result. 5GD AGD SGD Tk ˜O ( L+ρλmax(W) µ ) ˜O (√ L+ρλmax(W) µ ) ˜O ( σ2 µ2ϵk ) ρ L λmax(W) L λmax(W) L λ+ min(W) K∑ k=1 Tk ˜O ( κf √κW log(1 ϵ) ) ˜O (√κfκW log(1 ϵ) ) ˜O ( σ2κfκW µ2ϵ ) Table 1: The ﬁrst row indicates the number of iterations required for different inner solvers to achieve ϵk accuracy for the k-th subproblem Pk; the ˜O notation hides logarithmic factors in the parameters ρ, κf and κW. The second row shows the theoretical choice of the regularization parameter ρ. The last row shows the total number of iterations according to the choice of ρ. Lemma 4. Given the parameter choice in Theorem 2, initializing the subproblem Pk at Xk−1 yields, ∥Xk−1 −X∗ k∥2 ≤8Cρ µρ ϵk−1. Consequently, the ratio between the initial gap at the k-th subproblem and the desired gap ϵk is bounded by ∥Xk−1 −X∗ k∥2 ϵk ≤8Cρ µρ ϵk−1 ϵk ≤16Cρ µρ = O(κfκWρ2), which is independent of k. In other words, the inner loop solver only needs to decrease the iterate gap by a constant factor for each Pk. If the algorithm enjoys a linear convergence rate, a constant number of iteration is sufﬁcient for that. If the algorithm enjoys a sublinear convergence, then the inner loop complexity grows with k. To illustrate the behaviour of different algorithms, we present the inner loop complexity Tk for gradient descent (GD), accelerated gradient descent (AGD) and stochastic gradient descent (SGD) in Table 1. Note that while the inner complexity of GD and AGD are independent of k, the inner complexity for SGD increases geometrically with k. Other possible choices for inner solvers are the alternating directions or Jacobi/Gauss-Seidel method, both of which yield accelerated variants for [40] and [15]. In fact, the theoretical upper bounds on the inner complexity also provide a more practical way to halt the inner optimization processes (see Option II in Algorithm 3). Indeed, one can predeﬁne the computational budget for each subproblem, for instance, 100 iterations of AGD. If this budget exceeds the theoretical inner complexity Tk in Table 1, then the desired accuracy ϵk is guaranteed to be reached. In particular, we do not need to evaluate the sub-optimality condition, it is automatically satisﬁed as long as the budget is chosen appropriately. Finally, the global complexity is obtained by summing ∑K k=1 Tk, where Kis the number of subprob- lems given in (5). Note that, so far, our analysis applies to any regularization parameter ρ. Since∑K k=1 Tk is a function of ρ, this implies that one can select the parameter ρsuch that the overall complexity is minimized, leading to the choices of ρdescribed in Table 1. Two-fold acceleration In our setting, acceleration seems to occur in two stages (when compared to the non-accelerated O ( κfκW log(1 ϵ) ) rates in [15, 30, 34, 40, 41]). First, combining IDEAL with GD improves the dependence on the condition of the mixing matrix κW. Secondly, when used as an inner solver, AGD improves the dependence on the condition number of the local functionsκf. This suggests that the two phenomena are independent; while one is related to the consensus between the agents, as governed by the mixing matrix, the other one is related to the respective centralized hardness of the optimization problem. Stochastic oracle Our framework also subsumes the stochastic setting, where only noisy gradients are available. In this case, since SGD is sublinear, the required iteration counters Tk for the subprob- lem must increase inversely proportional to ϵk. Also the stepsize at the k-th iteration needs to be decreased accordingly. The overall complexity is now given by ˜O ( σ2κfκW µ2ϵ ) . However, in this case, the resulting dependence on the graph condition number can be improved [11]. 6ρ Computation cost Communication cost SSDA+AGD 0 ˜O ( κf √κW log(1 ϵ) ) O ( τ√κfκW log(1 ϵ) ) IDEAL+AGD L λmax(W) ˜O (√κfκW ) log(1 ϵ) ˜O ( τ√κfκW log(1 ϵ) ) MSDA+AGD 0 ˜O ( κf log(1 ϵ) ) O ( τ√κfκW log(1 ϵ) ) MIDEAL+AGD L λmax(Q(W)) ˜O (√κf log(1 ϵ) ) ˜O ( τ√κfκW log(1 ϵ) ) Table 2: The communication cost of the presented algorithms are all optimal, but the computation cost differs. An additional factor of √κf is introduced in SSDA/MSDA compared to their original rate in [37], due to the gradient approximation. The optimal computation cost is achieved by combining our multi-stage algorithm MIDEAL with AGD as an inner solver. Multi-stage variant (MIDEAL) We remark that the complexity presented in Table 2 is abbreviated, in the sense that it does not distinguish between communication cost and computation cost. To provide a more ﬁne-grained analysis, it sufﬁces to note that performing a gradient step of the subproblem ∇Pk(X) = ∇F(X) + Ωk + ρWX requires one local computation to evaluate ∇F, and one round of communication to obtain WX. This implies that when GD/AGD/SGD is combined with IDEAL, the number of local computation rounds is roughly the number of communication rounds, leading to a sub-optimal computation cost, as shown in Table 2. To achieve optimal accelerated rates, we enforce multiple communication rounds after one evaluation of ∇F. This is achieved by substituting the regularization metric ∥·∥ 2 W with ∥·∥ 2 Q(W), where Qis a well-chosen polynomial. In this case, the gradient of the subproblem becomes ∇Pk(X) = ∇F(X) + Ωk + ρ Q(W)X, which requires deg(Q) rounds of communication. The choice of the polynomial Qrelies on Chebyshev acceleration, which is introduced in [ 4, 37]. More concretely, the Chebyshev polynomials are deﬁned by the recursion relation T0(x) = 1 , T1(x) = x, Tj+1(x) = 2xTj(x) −Tj−1(x), and Qis deﬁned by Q(x) = 1 −TjW(c(1 −x)) TjW(c) with jW = ⌊√κW⌋, c = κW + 1 κW −1. (7) Applying this speciﬁc choice of Qto the mixing matrix W reduces its condition number by the maxi- mum amount [4, 37], yielding a graph independent bound κQ(W) = λmax(Q(W))/λ+ min(Q(W)) ≤ 4. Moreover, the symmetry, positiveness and spectrum property in Assumption 1 are maintained by Q(W). Even though Q(W) no longer satisﬁes the decentralized property, it can be implemented using ⌊√κW⌋rounds of communications with respect to W. The implementation details of the resulting algorithm are similar to Algorithm 2, and follow by substituting the mixing matrix W by Q(W) (Algorithm 5 in Appendix E). Comparison with inexact SSDA/MSDA [37]Recall that SSDA/MSDA are special cases of our algorithmic framework with the degenerate regularization parameterρ= 0. Therefore, our complexity analysis naturally extends to an inexact anlysis of SSDA/MSDA, as shown in Table 2. although the resulting communication costs are optimal, the computation cost is not, due to the additional√κf factor introduced by solving the subproblems inexactly. In contrast, our multi-stage framework achieves the optimal computation cost. •Low communication cost regime:τ√κW <1, the computation cost dominates the communica- tion cost, a √κf improvement is obtained by MIDEAL comparing to MSDA. •Ill conditioned regime:1 <τ √κW <√κf, the complexity of MSDA is dominated by the com- putation cost ˜O ( κf log(1 ϵ) ) while the complexity MIDEAL is dominated by the communication cost ˜O ( τ√κfκW log(1 ϵ) ) . The improvement is proportional to the ratio √κf/τ√κW. •High communication cost regime:√κf < τ√κW, the communication cost dominates, and MIDEAL and MSDA are comparable. 70 500 1000 1500 2000 Time -20 -15 -10 -5 0 Circular Graph (n= 16,  = 0.1, f = 1000, W = 26.3) IDEAL+AGD MIDEAL+AGD APM-C SSDA+AGD MSDA+AGD EXTRA 0 1000 2000 3000 4000 5000 6000 Time -20 -15 -10 -5 0 Circular Graph (n= 16,  = 1.0, f = 1000, W = 26.3) IDEAL+AGD MIDEAL+AGD APM-C SSDA+AGD MSDA+AGD EXTRA 0 5000 10000 15000 Time -20 -15 -10 -5 0 Circular Graph (n= 16,  = 10.0, f = 1000, W = 26.3) IDEAL+AGD MIDEAL+AGD APM-C SSDA+AGD MSDA+AGD EXTRA 0 500 1000 1500 2000 Time -20 -15 -10 -5 0 Barbell Graph (n= 16,  = 0.1, f = 1000, W = 47.1) IDEAL+AGD MIDEAL+AGD APM-C SSDA+AGD MSDA+AGD EXTRA 0 1000 2000 3000 4000 5000 6000 Time -20 -15 -10 -5 0 Barbell Graph (n= 16,  = 1.0, f = 1000, W = 47.1) IDEAL+AGD MIDEAL+AGD APM-C SSDA+AGD MSDA+AGD EXTRA 0 5000 10000 15000 Time -20 -15 -10 -5 0 Barbell Graph (n= 16,  = 10.0, f = 1000, W = 47.1) IDEAL+AGD MIDEAL+AGD APM-C SSDA+AGD MSDA+AGD EXTRA Figure 1: We evaluate the empirical performance of existing state-of-the-art algorithms, where the underlying network is a circular graph (top) and a barbell graph (bottom). We consider the following regimes: low communication cost (left), Ill-condition problems (middle) and High communication cost (right). The x-axis is the time counter, i.e. the sum of the communication cost and the computation cost; the y-axis is the log scale suboptimality. We observe that our algorithms IDEAL/MIDEAL are optimal under various regimes, validating our theoretical ﬁndings. 5 Experiments Having described the IDEAL/MIDEAL algorithms for decentralized optimization problem (1), we now turn to presenting various empirical results which corroborate our theoretical analysis. To facilitate a simple comparison between existing state-of-the-art algorithms, we consider an ℓ2- regularized logistic regression task over two classes of the MNIST [ 21] benchmark dataset. The smoohtness parameter (assuming normalized feature vectors) can be shown to be bounded by 1/4, which together with a regularization parameter µ≈1e−3, yields a relatively high 1e3-bound on the condition number of the loss function. Further empirical results which demonstrate the robustness of IDEAL/MIDEAL under wide range of parameter choices are provided in Appendix G. We compare the performance of IDEAL/MIDEAL with the state-of-the-art algorithms EXTRA [41], APM-C [22] and the inexact dual method SSDA/MSDA [ 37]. We set the inner iteration counter to be Tk = 100 for all algorithms, and use the theoretical stepsize schedule. The decentralized environment is modelled in a synthetic setting, where the communication time is steady and no latency is encountered. To demonstrate the effect of the underlying network architecture, we consider: a) a circular graph, where the agents form a cycle; b) a Barbell graph, where the agents are split into two complete subgraphs, connected by a single bridge (shown in Figure 2 in the appendix). As shown in Figure 1, our multi-stage algorithm MIDEAL is optimal in the regime where the communication cost τ is small, and the single-stage variant IDEAL is optimal when τ is large. As expected, the inexactness mechanism signiﬁcantly slows down the dual method SSDA/MSDA in the low communication cost regime. In contrast, the APM-C algorithm performs reasonably well in the low communication regime, but performs relatively poorly when the communication cost is high. 6 Conclusions We propose a novel framework of decentralized algorithms for smooth and strongly convex objectives. The framework provides a uniﬁed viewpoint of several well-known decentralized algorithms and, when instantiated with AGD, achieves optimal convergence rates in theory and state-of-the-art performance in practice. We leave further generalization to (non-strongly) convex and non-smooth objectives to future work. 8Acknowledgements YA and JB acknowledge support from the Sloan Foundation and Samsung Research. BC and MG acknowledge support from the grants NSF DMS-1723085 and NSF CCF-1814888. HL and SJ acknowledge support by The Defense Advanced Research Projects Agency (grant number YFA17 N66001-17-1-4039). The views, opinions, and/or ﬁndings contained in this article are those of the author and should not be interpreted as representing the ofﬁcial views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of Defense. Broader impact Centralization of data is not always possible because of security and legacy concerns [12]. Our work proposes a new optimization algorithm in the decentralized setting, which can learn a model without revealing the privacy sensitive data. Potential applications include data coming from healthcare, environment, safety, etc, such as personal medical information [17, 18], keyboard input history [20, 27] and beyond. References [1] Y . Arjevani and O. Shamir. Communication complexity of distributed convex learning and optimization. In Proceedings of Advances in Neural Information Processing Systems (NIPS), 2015. [2] Y . Arjevani and O. Shamir. On the iteration complexity of oblivious ﬁrst-order optimization algorithms. In International Conferences on Machine Learning (ICML), 2016. [3] Y . Arjevani, O. Shamir, and N. Srebro. A tight convergence analysis for stochastic gradient de- scent with delayed updates. In Proceedings of the 31st International Conference on Algorithmic Learning Theory, volume 117, pages 111–132, 2020. [4] W. Auzinger and J. Melenk. Iterative solution of large linear systems. Lecture Note, 2011. [5] N. S. Aybat and M. Gürbüzbalaban. Decentralized computation of effective resistances and ac- celeration of consensus algorithms. In 2017 IEEE Global Conference on Signal and Information Processing (GlobalSIP), pages 538–542. IEEE, 2017. [6] D. S. Bernstein, R. Givan, N. Immerman, and S. Zilberstein. The complexity of decentralized control of markov decision processes. Mathematics of operations research, 27(4):819–840, 2002. [7] D. P. Bertsekas. Constrained optimization and Lagrange multiplier methods. Academic press, 2014. [8] B. Can, S. Soori, N. S. Aybat, M. M. Dehvani, and M. Gürbüzbalaban. Decentralized compu- tation of effective resistances and acceleration of distributed optimization algorithms. arXiv preprint arXiv:1907.13110, 2019. [9] J. C. Duchi, A. Agarwal, and M. J. Wainwright. Dual averaging for distributed optimization: Convergence analysis and network scaling. IEEE Transactions on Automatic control, 57(3): 592–606, 2011. [10] D. Dvinskikh and A. Gasnikov. Decentralized and parallelized primal and dual accelerated methods for stochastic convex programming problems. arXiv preprint arXiv:1904.09015, 2019. [11] A. Fallah, M. Gürbüzbalaban, A. Ozdaglar, U. Simsekli, and L. Zhu. Robust distributed acceler- ated stochastic gradient methods for multi-agent networks. arXiv preprint arXiv:1910.08701, 2019. [12] GDPR. The eu general data protection regulation (gdpr). 2016. [13] O. Güler. New proximal point algorithms for convex minimization. SIAM Journal on Optimiza- tion, 2(4):649–664, 1992. 9[14] H. Hendrikx, F. Bach, and L. Massoulie. An optimal algorithm for decentralized ﬁnite sum optimization, 2020. [15] D. Jakoveti´c, J. M. Moura, and J. Xavier. Linear convergence rate of a class of distributed augmented lagrangian algorithms. IEEE Transactions on Automatic Control, 60(4):922–936, 2014. [16] D. Jakoveti´c, J. Xavier, and J. M. Moura. Fast distributed gradient methods. IEEE Transactions on Automatic Control, 59(5):1131–1146, 2014. [17] A. Jochems, T. M. Deist, J. Van Soest, M. Eble, P. Bulens, P. Coucke, W. Dries, P. Lambin, and A. Dekker. Distributed learning: developing a predictive model based on data from multiple hospitals without data leaving the hospital–a real life proof of concept. Radiotherapy and Oncology, 121(3):459–467, 2016. [18] A. Jochems, T. M. Deist, I. El Naqa, M. Kessler, C. Mayo, J. Reeves, S. Jolly, M. Matuszak, R. Ten Haken, J. van Soest, et al. Developing and validating a survival prediction model for nsclc patients through distributed learning across 3 countries.International Journal of Radiation Oncology* Biology* Physics, 99(2):344–352, 2017. [19] M. Kang, M. Kang, and M. Jung. Inexact accelerated augmented lagrangian methods. Compu- tational Optimization and Applications, 62(2):373–404, 2015. [20] J. Konevcný, H. B. McMahan, F. X. Yu, P. Richtarik, A. T. Suresh, and D. Bacon. Federated learning: Strategies for improving communication efﬁciency. In NIPS Workshop on Private Multi-Party Machine Learning, 2016. URL https://arxiv.org/abs/1610.05492. [21] Y . LeCun, C. Cortes, and C. Burges. Mnist handwritten digit database.ATT Labs [Online], 2, 2010. URL http://yann.lecun.com/exdb/mnist. [22] H. Li, C. Fang, W. Yin, and Z. Lin. A sharp convergence rate analysis for distributed accelerated gradient methods. arXiv preprint arXiv:1810.01053, 2018. [23] H. Lin, J. Mairal, and Z. Harchaoui. Catalyst acceleration for ﬁrst-order convex optimization: from theory to practice. The Journal of Machine Learning Research, 18(1):7854–7907, 2017. [24] J. Mairal. End-to-end kernel learning with supervised convolutional kernel networks. In Proceedings of Advances in Neural Information Processing Systems (NIPS), 2016. [25] Y . Mao, C. You, J. Zhang, K. Huang, and K. B. Letaief. A survey on mobile edge computing: The communication perspective. IEEE Communications Surveys & Tutorials, 19(4):2322–2358, 2017. [26] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efﬁcient learning of deep networks from decentralized data. In Artiﬁcial Intelligence and Statistics , pages 1273–1282, 2017. [27] H. B. McMahan, E. Moore, D. Ramage, S. Hampson, et al. Communication-efﬁcient learning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629, 2016. [28] V . Nedelcu, I. Necoara, and Q. Tran-Dinh. Computational complexity of inexact gradient augmented lagrangian methods: application to constrained mpc. SIAM Journal on Control and Optimization, 52(5):3109–3134, 2014. [29] A. Nedic and A. Ozdaglar. Distributed subgradient methods for multi-agent optimization. IEEE Transactions on Automatic Control, 54(1):48–61, 2009. [30] A. Nedic, A. Olshevsky, and W. Shi. Achieving geometric convergence for distributed optimiza- tion over time-varying graphs. SIAM Journal on Optimization, 27(4):2597–2633, 2017. [31] A. Nedi ´c, A. Olshevsky, W. Shi, and C. A. Uribe. Geometrically convergent distributed optimization with uncoordinated step-sizes. In 2017 American Control Conference (ACC) , pages 3950–3955. IEEE, 2017. 10[32] Y . Nesterov.Introductory lectures on convex optimization, volume 87. Springer Science & Business Media, 2004. [33] L. Panait and S. Luke. Cooperative multi-agent learning: The state of the art. Autonomous agents and multi-agent systems, 11(3):387–434, 2005. [34] G. Qu and N. Li. Harnessing smoothness to accelerate distributed optimization. IEEE Transac- tions on Control of Network Systems, 5(3):1245–1260, 2017. [35] R. T. Rockafellar. Augmented lagrangians and applications of the proximal point algorithm in convex programming. Mathematics of operations research, 1(2):97–116, 1976. [36] R. T. Rockafellar and R. J.-B. Wets. Variational analysis, volume 317. Springer Science & Business Media, 2009. [37] K. Scaman, F. Bach, S. Bubeck, Y . T. Lee, and L. Massoulié. Optimal algorithms for smooth and strongly convex distributed optimization in networks. In International Conferences on Machine Learning (ICML), 2017. [38] K. Scaman, F. Bach, S. Bubeck, L. Massoulié, and Y . T. Lee. Optimal algorithms for non- smooth distributed optimization in networks. In Proceedings of Advances in Neural Information Processing Systems (NIPS), 2018. [39] M. Schmidt, N. L. Roux, and F. R. Bach. Convergence rates of inexact proximal-gradient methods for convex optimization. In Proceedings of Advances in Neural Information Processing Systems (NIPS), 2011. [40] W. Shi, Q. Ling, K. Yuan, G. Wu, and W. Yin. On the linear convergence of the ADMM in decentralized consensus optimization. IEEE Transactions on Signal Processing , 62(7): 1750–1761, 2014. [41] W. Shi, Q. Ling, G. Wu, and W. Yin. Extra: An exact ﬁrst-order algorithm for decentralized consensus optimization. SIAM Journal on Optimization, 25(2):944–966, 2015. [42] W. Shi, J. Cao, Q. Zhang, Y . Li, and L. Xu. Edge computing: Vision and challenges. IEEE internet of things journal, 3(5):637–646, 2016. [43] R. Shokri and V . Shmatikov. Privacy-preserving deep learning. In Proceedings of the 22nd ACM SIGSAC conference on computer and communications security, pages 1310–1321, 2015. [44] Y . Sun, A. Daneshmand, and G. Scutari. Convergence rate of distributed optimization algorithms based on gradient tracking. arXiv preprint arXiv:1905.02637, 2019. [45] C. A. Uribe, S. Lee, A. Gasnikov, and A. Nedi ´c. A dual approach for optimal algorithms in distributed optimization over networks. Optimization Methods and Software, pages 1–40, 2020. [46] B. E. Woodworth, J. Wang, A. Smith, B. McMahan, and N. Srebro. Graph oracle models, lower bounds, and gaps for parallel stochastic optimization. In Proceedings of Advances in Neural Information Processing Systems (NIPS), 2018. [47] L. Xiao, S. Boyd, and S.-J. Kim. Distributed average consensus with least-mean-square deviation. Journal of parallel and distributed computing, 67(1):33–46, 2007. [48] J. Xu, Y . Tian, Y . Sun, and G. Scutari. Accelerated primal-dual algorithms for distributed smooth convex optimization over networks. International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2020. [49] S. Yan and N. He. Bregman augmented lagrangian and its acceleration, 2020. [50] K. Yuan, Q. Ling, and W. Yin. On the convergence of decentralized gradient descent. SIAM Journal on Optimization, 26(3):1835–1854, 2016. [51] J. Zhang, C. A. Uribe, A. Mokhtari, and A. Jadbabaie. Achieving acceleration in distributed op- timization via direct discretization of the heavy-ball ode. In 2019 American Control Conference (ACC), pages 3408–3413. IEEE, 2019. 11A Remark on the choice of the mixing matrix In the main paper, the mixing matrix W is deﬁned following the convention used in [37], where the kernel of W is the vector of all ones. It is worth noting that the term mixing matrix is also used in the literature to denote a doubly stochastic matrix WDS (see e.g. [8, 15, 30, 31, 34, 40, 41]). These two approaches are equivalent as given a doubly stochastic matrix WDS, the matrix I−WDS is a mixing matrix under Deﬁnition 1. In the following discussion, we will use WDS to draw the connection when necessary. B Recovering EXTRA under the augmented Lagrangian framework The goal of this section is to show that EXTRA algorithm [41] is a special case of the non-accelerated Augmented Lagrangian framework in Algorithm 1. Proposition 5. The EXTRA algorithm is equivalent to applying one step of gradient descent to solve the subproblem in Algorithm 1. Proof. Taking a single step of gradient descent in the subproblem Pk in Algorithm 1 warm starting at Xk−1 yields the update Xk = Xk−1 −α(∇F(Xk−1) + Λk + ρWXk−1). (8) Λk+1 = Λk + ηWXk. Using the (k+ 1)-th update, Xk+1 = Xk −α(∇F(Xk) + Λk+1 + ρWXk). (9) and subtracting (8) from (9) gives Xk+1 = (2 −α(ρ+ η)W)Xk −(1 −αρW)Xk−1 −α(∇F(Xk) −∇F(Xk−1)). When incorporating with the mixing matrix W = I−WDS and taking ρ= η= 1 2α gives, Xk+1 = (I+ WDS)Xk − ( I+ WDS 2 ) Xk−1 −α(∇F(Xk) −∇F(Xk−1)), which is the update rule of EXTRA [41]. Remark 6. When expressing the parameters in terms of ρ, the inner loop stepsize reads as α= 1 2ρ, and the outer-loop stepsize reads as η= ρ. C Proof of Theorem 3 Algorithm 4(Unscaled) Accelerated Decentralized Augmented Lagrangian framework Input: mixing matrix W, regularization parameter ρ, stepsize η, extrapolation parameters {βk}k∈N 1: Initialize dual variables Λ1 = Ω1 = 0 ∈Rnd. 2: for k= 1,2,...,K do 3: Xk ≈arg min { Pk(X) := F(X) + ( √ WΩk)TX + ρ 2 ∥X∥2 W } . 4: Λk+1 = Ωk + η √ WXk 5: Ωk+1 = Λk+1 + βk+1(Λk+1 −Λk) 6: end for Output: XK. We start by noting that Algorithm 2 is equivalent to the “unscaled\" version of Algorithm 4. More speciﬁcally, we recover Algorithm 2 by substituting the variables Λ ← √ WΛ, Ω ← √ WΩ. 12The unscaled version is computationally inefﬁcient since it requires the computation of the square root of W. This is the reason why we choose to present the scaled version Algorithm 2 in the main paper. However, the unscaled version is easier to work with for the analysis. In the following proof, the variablesΛ and Ω are referred to as in the unscaled version Algorithm 4. The key concept underlying our analysis on is the Moreau-envelope of the dual problem: Φρ(Λ) = min Γ∈Rnd { F∗(− √ WΓ) + 1 2ρ∥Γ −Λ∥2 } . (10) Similarly, we deﬁne the associated proximal operator proxΦρ(Λ) = arg min Γ∈Rnd { F∗(− √ WΓ) + 1 2ρ∥Γ −Λ∥2 } . (11) Note that when the inner problem is strongly convex, the proximal operator is unique (that is, a single-valued operator). The following is a list well known properties of the Moreau-envelope: Proposition 7. The Moreau envelope Φρ enjoys the following properties 1. Φρ is convex and it shares the same optimum as the dual problem (D). 2. Φρ is differentiable and the gradient of Φρ is given by ∇Φρ(Λ) = 1 ρ(Λ −proxΦρ(Λ)) 3. If F is twice differentiable, then its convex conjugate F∗is also twice differentiable. In this case, Φρ is also twice differentiable and the Hessian is given by ∇2Φρ(Λ) = 1 ρI− 1 ρ2 [1 ρI+ √ W∇2F∗(− √ W proxΦρ(Λ)) √ W ]−1 . Corollary 8. The Moreau envelope Φρ satisﬁes 1. Φρ is Lρ-smooth, where Lρ = λmax(W) µ+ρλmax(W) ≤1 ρ. 2. Φρ is µρ-strongly convex in the image space of √ W, where µρ = λ+ min(W) L+ρλ+ min(W) . Proof. These properties follow from the expressions for the Hessian of Φρ and by the fact that F∗is 1 µ-smooth and 1 L strongly convex. In particular, Φρ is only strongly convex on the image space of √ W, one of the keys to prove the linear convergence rate is the following lemma. Lemma 9. The variables Λk and Ωk in the un-scaled version Algorithm 4 all lie in the image space of √ W for any k. Proof. This can be easily derived by induction according to the update rule in line 4, 5 of Algorithm 4. Similar to the dual Moreau-envelope, we also deﬁne the weighted Moreau-envelope on the primal function Ψρ(Ω) = min X { F(X) + ΩTX + ρ 2∥X∥2 W } (12) and its associated proximal operator proxΨρ(Ω) = arg min X { F(X) + ΩTX + ρ 2∥X∥2 W } . (13) Indeed, this function corresponds exactly to the subproblem solved in the augmented Lagrangian framework (line 3 of Algorithm 2). Similar property holds for Ψρ: 13Proposition 10. The Moreau envelope Ψρ enjoys the following properties: 1. Ψρ is concave. 2. Ψρ is differentiable and the gradient of Ψρ is given by ∇Ψρ(Ω) = proxΨ(Ω). 3. If F is twice differentiable, then Ψρ is also twice differentiable and the Hessian is given by ∇2Ψρ(Ω) = − [ ∇2F(proxΨ(Ω)) + ρW ]−1 . In particular, Ψρ is 1 µ-smooth and 1 L+ρλmax(W) strongly concave. The dual Moreau-envelope Φρ and primal Moreau-envelope Ψρ are connected through the following relationship. Proposition 11. The gradient of the Moreau envelope Φρ is given by ∇Φρ(Λ) = − √ W∇Ψρ( √ WΛ). (14) Proof. To simplify the presentation, let us denote X(Λ) = arg min X { F(X) + ( √ WΛ)TX + ρ 2∥X∥2 W } = ∇Ψρ( √ WΛ). From the optimality of X(Λ), we have ∇F(X(Λ)) + √ WΛ + ρWX(Λ) = 0 From the fact that ∇F(x) = y⇔∇F∗(y) = x, we have X(Λ) = ∇F∗ ( − √ W [ Λ + ρ √ WX(Λ) ]) . Let Γ = Λ + ρ √ WX(Λ), then − √ W∇F∗(− √ WΓ) + 1 ρ(Γ −Λ) = 0. Therefore Γ is the minimizer of the function F∗(− √ WΓ) + 1 2ρ∥Γ −Λ∥2, namely proxΦρ(Λ) = Λ + ρ √ WX(Λ). Then based on the expression for the gradient in Prop 7, we obtain the desired equality (14). Proposition 14 demonstrates that solving the augmented Lagrangian subproblem could be viewed as evaluating the gradient of the Moreau-envelope. Hence applying gradient descent on the Moreau- envelope gives the non-accelerated augmented Lagrangian framework Algorithm 1. Even more, applying Nesterov’s accelerated gradient on the Moreau-envelopeΦρ yields accelerated Augmented Lagrangian Algorithm 4. In addition, when the subproblems are solved inexactly, this corresponds to an inexact evaluation on the gradient. This interpretation allows us to derive guarantees for the convergence rate of the dual variables. Before present the the convergence analysis in detail, we formally establish the connection between the primal solution and the dual solution. Lemma 12. Let x∗be the optimum of f and deﬁne X∗= 1n⊗x∗∈Rnd. Then there exists a unique Λ∗∈Im(W) such that Λ∗is the optimum of the dual problem (D). Moreover, it satisﬁes ∇F(X∗) = − √ WΛ∗. Proof. Since Ker(W) = R1n, we have Ker(W) = Ker(W ⊗Id) = Vect(1n ⊗ei,i = 1,··· ,d), 14where ei is the canonical basis with all entries 0 except the i-th equals to 1. By optimality, ∇f(x∗) =∑n i=1 ∇fi(x∗) = 0. This implies that ∇F(x∗)T(1n ⊗ei) = 0, for all i= 1,···d. In other words, ∇F(X∗) is orthogonal to the null space of W, namely ∇F(X∗) ∈Im(W). Therefore, there exists Λ such that ∇F(X∗) = −WΛ. By setting Λ∗ = √ WΛ, we have Λ∗ ∈Im(W) and ∇F(X∗) = − √ WΛ∗. In particular, since ∇F(x) = y⇔∇F∗(y) = x, we have, √ W∇F∗(− √ WΛ∗) = √ WX∗= 0. (15) Hence Λ∗is the solution of the dual problem (D) and it is the unique one lies in the Im(W). Throughout the rest of the paper, we use Λ∗to denote the unique solution as shown in the lemma above. We would like to emphasize that even though F∗is strongly convex, the dual problem (D) is not strongly convex, because W is singular. Hence, the solution of the dual problem is not unique unless we restrict to the image space of W. To derive the linear convergence rate, we need to show that the dual variable always lies in this subspace where the Moreau-envelope Φρ is strongly convex. Theorem 13. Consider the sequence of primal variables (Xk)k∈N generated by Algorithm 3 with the subproblem solved up to ϵk accuracy, i.e. Option I. Therefore, ∥Xk+1 −X∗∥2 ≤2ϵk+1 + C ( 1 − √µρ Lρ )k(√ µρ∆dual + Ak )2 (16) where X∗ = 1n ⊗x∗, Lρ = λmax(W) µ+ρλmax(W) , µρ = λ+ min(W) L+ρλ+ min(W) , C = 2λmax(W) µ2µ2ρ , ∆dual is the dual function gap deﬁned by ∆dual = F∗(− √ WΛ1) −F∗(− √ WΛ∗) and Ak = √ λmax(W) ∑k i=1 √ϵi ( 1 − √µρ Lρ )−i/2 . Proof. The proof builds on the concepts developed so far in this section. We start by showing that the dual variable Λk converges to the dual solution Λ∗in a linear rate. From the interpretation given in Prop 7 and Prop 11, the sequence (Λk)k∈N given in Algorithm 2 is equivalent to applying Nesterov’s accelerated gradient method on the Moreau-envelope Φρ. In the inexact variant, the inexactness on the solution directly translates to an inexact gradient of Φρ, where the inexactness is given by ∥ek∥= ∥ √ W(Xk −X∗ k)∥≤ √ λmax(W)∥Xk −X∗ k∥≤ √ λmax(W)ϵk. Hence (Λk)k∈N in Algorithm 4 is obtained by applying inexact accelerated gradient method on the Moreau-envelope Φρ. Note that by induction Λk and Ωk belong to the image space of √ W, in which the dual Moreau-envelope Φρ is strongly convex. Following the analysis on inexact accelerated gradient method Prop 4 in [39], we have µρ 2 ∥Λk+1 −Λ∗∥2 ≤ ( 1 − √µρ Lρ )k+1 (√ 2∆Φρ + √ 2 µρ Ak )2 (17) where ∆Φρ = Φρ(Λ1) −Φ∗ ρ and Ak is the accumulation of the errors given by Ak = k∑ i=1 ∥ei∥ ( 1 − √µρ Lρ )−i/2 ≤ k∑ i=1 √ λmax(W)ϵi ( 1 − √µρ Lρ )−i/2 . Based on the convergence on the dual variable, we could now derive the convergence on the primal variable. Let X∗ k+1 be the exact solution of the problem Pk+1. Then ∥X∗ k+1 −X∗∥= ∥∇Ψρ( √ WΛk+1) −∇Ψρ( √ WΛ∗)∥ ≤1 µ∥ √ W(Λk+1 −Λ∗)∥ (From Prop 10.3) ≤ √ λmax(W) µ ∥Λk+1 −Λ∗∥. (18) 15Finally, from triangle inequality ∥Xk+1 −X∗∥2 ≤2∥Xk+1 −X∗ k+1∥2 + 2∥X∗ k+1 −X∗∥2 ≤2ϵk+1 + 2λmax(W) µ2µρ (1 −√κρ)k+1 (√ 2∆Φρ + √ 2 µρ Ak )2 . The desired inequality follows from reorganizing the constant and the fact that ∆Φρ ≤∆dual. Proof of Theorem 2. Plugging in the choice of ϵk = µρ 2λmax(W) ( 1 −1 2 √µρ Lρ )k ∆dual in (16) yields the desired convergence rate. D Proof of Lemma 4 Lemma 14. With the parameter choice as Theorem 2, then warm starting the k-th subproblem Pk at the previous solution Xk−1 gives an initial gap ∥Xk−1 −X∗ k∥2 ≤8Cρ µρ ϵk−1. Proof. From triangle inequality, we have ∥Xk−1 −X∗ k∥2 ≤2(∥Xk−1 −X∗∥2 + ∥X∗ k −X∗∥2) The desired inequality follows from the convergence on the primal iterates and (18), i.e. ∥Xk−1 −X∗ k∥2 ≤2Cρ µρ ϵk−1, ∥X∗ k −X∗ k∥2 ≤2Cρ µρ ϵk. E Multi-stage algorithm: MIDEAL Algorithm 5MIDEAL: Multi-stage Inexact Acc-Decentralized Augmented Lagrangian framework Input: mixing matrix W, regularization parameter ρ, stepsize η, extrapolation parameters {βk}k∈N 1: Initialize dual variables Λ1 = Ω1 = 0 ∈Rnd and the polynomial Qaccording to (7). 2: for k= 1,2,...,K do 3: Xk ≈arg min { Pk(X) := F(X) + ΩT kX + ρ 2 ∥X∥2 Q(W) } . 4: Λk+1 = Ωk + ηQ(W)Xk 5: Ωk+1 = Λk+1 + βk+1(Λk+1 −Λk) 6: end for Output: XK. Algorithm 6AcceleratedGossip [37] Input: mixing matrix W, vector or matrix X. 1: Set parameters κW = λmax(W) λ+ min(W) , c2 = κW+1 κW−1 , c3 = 2 (κW+1)λ+ min(W) , # of iterations J = ⌊√κW⌋. 2: Initialize coefﬁcients a0 = 1, a1 = c2, iterates X0 = X, X1 = c2(I−c3W)X. 3: for j = 1,2,...,J −1 do 4: aj+1 = 2c2aj −aj−1 5: Xj+1 = 2c2(1 −c3W)Xj −Xj−1 6: end for Output: X0 −XJ aJ . 16Intuitively, we simply replace the mixing matrix W by Q(W), resulting in a better graph condi- tion number. However, each evaluation of the new mixing matrix Q(W) requires deg(Q) rounds of communication, given by the AcceleratedGossip algorithm introduced in [ 37]. For complete- ness of the discussion, we recall this procedure in Algorithm 6. In particular, given W and X, AcceleratedGossip(W,X) returns Q(W)X, based on the communication oracle W. F Implementation of Algorithms Algorithm 7Implementation: IDEAL+AGD solver Input: number of iterations K >0, gossip matrix W ∈Rn×n 1: ωi(0) = ⃗0, γi(0) = ⃗0, xi(0) = xi(0) = x0 for any i∈[1,n] 2: κinner = L+ρλmax(W) µ , βinner = √κinner−1√κinner+1 , κρ = L+ρλ+ min(W) µ+ρλmax(W) λmax(W) λ+ min(W) , βouter = √κouter−1√κouter+1 3: for k= 1,2,...,K do 4: Inner iteration: Approximately solve the augmented Lagrangian multiplier. 5: Set xi,k(0) = yi,k(0) = xi(k−1), xi,k(0) = yi,k(0) = ∑ j∼iWijxj,k(0) 6: for t= 0,1,...,T −1 do 7: xi,k(t+ 1) = yi,k(t) −η(γi(k) + ∇fi(yi,k(t)) + ρyi,k(t)) 8: yi,k(t+ 1) = xi,k(t+ 1) +βinner(xi,k(t+ 1) −xi,k(t)) 9: yi,k(t+ 1) = ∑ j∼iWijyj,k(t+ 1) 10: end for 11: Set xi(k) = xi,k(T), xi(k) = ∑ j∼iWijxj,k(T) 12: Outer iteration: Update the dual variables on each node 13: λi(k+ 1) = ωi(k) + ρxi(k) 14: ωi(k+ 1) = λi(k+ 1) +βouter(λi(k+ 1) −λi(k)) 15: end for Output: G Further Experimental Results Figure 2: Network Structures: Left:Circular graph with 4 nodes. Right:Barbell graph with 8 nodes. 170 500 1000 1500 2000 Time -20 -15 -10 -5 0 CIFAR Circular Graph (n= 16,  = 0.1, f = 1000, W = 26.3) IDEAL+AGD MIDEAL+AGD APM-C SSDA+AGD MSDA+AGD EXTRA 0 1000 2000 3000 4000 Time -20 -15 -10 -5 0 CIFAR Circular Graph (n= 16,  = 1.0, f = 1000, W = 26.3) IDEAL+AGD MIDEAL+AGD APM-C SSDA+AGD MSDA+AGD EXTRA 0 5000 10000 15000 Time -20 -15 -10 -5 0 CIFAR Circular Graph (n= 16,  = 10.0, f = 1000, W = 26.3) IDEAL+AGD MIDEAL+AGD APM-C SSDA+AGD MSDA+AGD EXTRA 0 500 1000 1500 2000 Time -20 -15 -10 -5 0 CIFAR Barbell Graph (n= 16,  = 0.1, f = 1000, W = 47.1) IDEAL+AGD MIDEAL+AGD APM-C SSDA+AGD MSDA+AGD EXTRA 0 1000 2000 3000 4000 Time -20 -15 -10 -5 0 CIFAR Barbell Graph (n= 16,  = 1.0, f = 1000, W = 47.1) IDEAL+AGD MIDEAL+AGD APM-C SSDA+AGD MSDA+AGD EXTRA 0 5000 10000 15000 Time -20 -15 -10 -5 0 CIFAR Barbell Graph (n= 16,  = 10.0, f = 1000, W = 47.1) IDEAL+AGD MIDEAL+AGD APM-C SSDA+AGD MSDA+AGD EXTRA Figure 3: CIFAR experiments: we conduct experiments on two classes of CIFAR dataset, where the feature representation of each image was computed using an unsupervised convolutional kernel net- work Mairal [24]. We observe similar phenomenon as in the MNIST experiment, that the multi-stage algorithm MIDEAL outperforms when the communication cost τ is low and the IDEAL outperforms in the other cases. 0 1000 2000 3000 4000 Time -20 -15 -10 -5 0 Circular Graph (n= 16,  = 1.0, f = 1000, W = 26.3) Ideal+AGD =0.01 default Ideal+AGD =0.1 default Ideal+AGD =0.5 default Ideal+AGD = default Ideal+AGD =2 default Ideal+AGD =10 default Ideal+AGD =100 default 0 1000 2000 3000 4000 Time -20 -15 -10 -5 0 Barbell Graph (n= 16,  = 1.0, f = 1000, W = 47.1) Ideal+AGD =0.01 default Ideal+AGD =0.1 default Ideal+AGD =0.5 default Ideal+AGD = default Ideal+AGD =2 default Ideal+AGD =10 default Ideal+AGD =100 default Figure 4: Ablation study on the regularization parameterρ in IDEAL framework. For all the experiments, we use AGD as inner loop solver and set the same parameters as predicted by theory. We observe that whenρis selected in the range[0.5ρdefault,10ρdefault], the perfomance of the algorithm is quite similar and robust. We also observe that using a small ρdegrades the performance of the algorithm, this phenomenon is consistent with the observation that the inexact SSD [ 37] does not perform well since it uses ρ= 0. Another observation is that with larger ρ, such as ρ= 2ρdefault or 10ρdefault, the algorithm is more stable with less zigzag oscillation, which is preferable in practice. 180 500 1000 1500 2000 Time -10 -8 -6 -4 -2 0 Circular Graph (n= 16,  = 1.0, f = 1000, W = 26.3) Ideal+AGD T=1 Ideal+AGD T=5 Ideal+AGD T=10 Ideal+AGD T=20 Ideal+AGD T=50 Ideal+AGD T=100 0 500 1000 1500 2000 Time -10 -8 -6 -4 -2 0 Barbell Graph (n= 16,  = 1.0, f = 1000, W = 47.1) Ideal+AGD T=1 Ideal+AGD T=5 Ideal+AGD T=10 Ideal+AGD T=20 Ideal+AGD T=50 Ideal+AGD T=100 Figure 5: Ablation study on the inner loop complexityTk in IDEAL framework. When the inner loop iteration is small, the algorithm becomes less stable, so we have decreased the momentum parameters to ensure the convergence. For these experiments, we use AGD solver with βin = 0.8 and βout = 0.4. As we can see, it is beneﬁcial to perform multiple iterations in the inner loop rather than taking T=1 as in the EXTRA algorithm [41]. 19",
      "references": [
        "Communication complexity of distributed convex learning and optimization.",
        "On the iteration complexity of oblivious ﬁrst-order optimization algorithms.",
        "A tight convergence analysis for stochastic gradient de- scent with delayed updates.",
        "Iterative solution of large linear systems.",
        "Decentralized computation of effective resistances and ac- celeration of consensus algorithms.",
        "The complexity of decentralized control of markov decision processes.",
        "Constrained optimization and Lagrange multiplier methods.",
        "Decentralized computation of effective resistances and acceleration of distributed optimization algorithms.",
        "Dual averaging for distributed optimization: Convergence analysis and network scaling.",
        "Decentralized and parallelized primal and dual accelerated methods for stochastic convex programming problems.",
        "Robust distributed accelerated stochastic gradient methods for multi-agent networks.",
        "The eu general data protection regulation (gdpr).",
        "New proximal point algorithms for convex minimization.",
        "An optimal algorithm for decentralized ﬁnite sum optimization.",
        "Linear convergence rate of a class of distributed augmented lagrangian algorithms.",
        "Fast distributed gradient methods.",
        "Distributed learning: developing a predictive model based on data from multiple hospitals without data leaving the hospital–a real life proof of concept.",
        "Developing and validating a survival prediction model for nsclc patients through distributed learning across 3 countries.",
        "Inexact accelerated augmented lagrangian methods.",
        "Federated learning: Strategies for improving communication efﬁciency.",
        "Mnist handwritten digit database.",
        "A sharp convergence rate analysis for distributed accelerated gradient methods.",
        "Catalyst acceleration for ﬁrst-order convex optimization: from theory to practice.",
        "End-to-end kernel learning with supervised convolutional kernel networks.",
        "A survey on mobile edge computing: The communication perspective.",
        "Communication-efﬁcient learning of deep networks from decentralized data.",
        "Computational complexity of inexact gradient augmented lagrangian methods: application to constrained mpc.",
        "Distributed subgradient methods for multi-agent optimization.",
        "Achieving geometric convergence for distributed optimiza- tion over time-varying graphs.",
        "Geometrically convergent distributed optimization with uncoordinated step-sizes.",
        "Introductory lectures on convex optimization.",
        "Cooperative multi-agent learning: The state of the art.",
        "Harnessing smoothness to accelerate distributed optimization.",
        "Augmented lagrangians and applications of the proximal point algorithm in convex programming.",
        "Variational analysis.",
        "Optimal algorithms for smooth and strongly convex distributed optimization in networks.",
        "Optimal algorithms for non- smooth distributed optimization in networks.",
        "Convergence rates of inexact proximal-gradient methods for convex optimization.",
        "On the linear convergence of the ADMM in decentralized consensus optimization.",
        "Extra: An exact ﬁrst-order algorithm for decentralized consensus optimization.",
        "Edge computing: Vision and challenges.",
        "Privacy-preserving deep learning.",
        "Convergence rate of distributed optimization algorithms based on gradient tracking.",
        "A dual approach for optimal algorithms in distributed optimization over networks.",
        "Graph oracle models, lower bounds, and gaps for parallel stochastic optimization.",
        "Distributed average consensus with least-mean-square deviation.",
        "Accelerated primal-dual algorithms for distributed smooth convex optimization over networks.",
        "Bregman augmented lagrangian and its acceleration.",
        "On the convergence of decentralized gradient descent.",
        "Achieving acceleration in distributed optimization via direct discretization of the heavy-ball ode."
      ],
      "meta_data": {
        "arxiv_id": "2006.06733v1",
        "authors": [
          "Yossi Arjevani",
          "Joan Bruna",
          "Bugra Can",
          "Mert Gürbüzbalaban",
          "Stefanie Jegelka",
          "Hongzhou Lin"
        ],
        "published_date": "2020-06-11T18:49:06Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces IDEAL, a novel framework for designing decentralized optimization algorithms via an inexact accelerated augmented Lagrangian method. It unifies several known decentralized algorithms (including EXTRA and SSDA) and, when combined with accelerated gradient descent, yields optimal convergence rates with respect to both the condition number of the objective function and the mixing matrix, particularly addressing challenges in highly ill-conditioned problems.",
        "methodology": "The authors develop an inexact accelerated augmented Lagrangian framework operating in the primal space. The approach formulates a sequence of subproblems and approximately solves them using techniques such as gradient descent (GD), accelerated gradient descent (AGD), and stochastic gradient descent (SGD). In addition, a multi-stage variant (MIDEAL) is introduced by incorporating Chebyshev acceleration via a polynomial Q(W) to improve the condition number of the mixing matrix, effectively balancing inner and outer loop complexities.",
        "experimental_setup": "Experimental evaluations are performed on logistic regression tasks using benchmarks such as MNIST and CIFAR datasets. The algorithms are assessed in synthetic decentralized environments over different network structures (e.g., circular graphs and barbell graphs) while comparing computation and communication costs under various regime settings (low communication cost, ill-conditioned problems, high communication cost). The performance is compared against state-of-the-art methods like EXTRA, APM-C, and SSDA/MSDA.",
        "limitations": "The approach is formulated for smooth and strongly convex objective functions, limiting its direct applicability to non-smooth or non-strongly convex problems. The method assumes a given network topology and particular mixing matrices, and it requires careful tuning of parameters (such as the regularization parameter ρ) to balance convergence speed with computational cost. In stochastic settings, additional factors may lead to sub-optimal rates, indicating potential sensitivity to noise and inner solver inaccuracies.",
        "future_research_directions": "Future work could extend the framework to handle non-strongly convex and non-smooth objectives, potentially broadening its application scope. Further research could also explore adaptive methods for parameter tuning to improve robustness in heterogeneous network settings, and investigate the integration with more complex stochastic or federated learning scenarios to address privacy and scalability challenges.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "The Primal-Dual method for Learning Augmented Algorithms",
      "full_text": "The Primal-Dual method for Learning Augmented Algorithms Etienne Bamas∗ EPFL, Lausanne, Switzerland etienne.bamas@epfl.ch Andreas Maggiori∗ EPFL, Lausanne, Switzerland andreas.maggiori@epfl.ch Ola Svensson∗ EPFL, Lausanne, Switzerland ola.svensson@epfl.ch Abstract The extension of classical online algorithms when provided with predictions is a new and active research area. In this paper, we extend the primal-dual method for online algorithms in order to incorporate predictions that advise the online algorithm about the next action to take. We use this framework to obtain novel algorithms for a variety of online covering problems. We compare our algorithms to the cost of the true and predicted ofﬂine optimal solutions and show that these algorithms outperform any online algorithm when the prediction is accurate while maintaining good guarantees when the prediction is misleading. 1 Introduction In the classical ﬁeld of online algorithms the input is presented in an online fashion and the algorithm is required to make irrevocable decisions without knowing the future. The performance is often measured in terms of worst-case guarantees with respect to an optimal ofﬂine solution. In this paper, we will consider minimization problems and formally, we will say that an online algorithm ALGis c-competitive if on any input I, the cost cALG(I) of the solution output by algorithm ALGon input Isatisﬁes cALG(I) ⩽ c·OPT(I), where OPT(I) denotes the cost of the ofﬂine optimum. Due to the uncertainty about the future, online algorithms tend to be overly cautious which sometimes causes their performance in real-world situations to be far from what a machine learning (ML) algorithm would have achieved. Indeed in many practical applications future events follow patterns which are easily predictable using ML methods. In [19] Lykouris and Vassilvitskii formalized a general framework for incorporating (ML) predictions into online algorithms and designed an extension of the marking algorithm to solve the online caching problem when provided with predictions. This work was quickly followed by many other papers studying different learning augmented online problems such as scheduling ([17]), caching ([2, 26]), ski rental ([10, 16, 25, 28]), clustering ([7]) and other problems ([12, 23]). The main challenge is to incorporate the prediction without knowing how the prediction was computed and in particular without making any assumption on the quality of the prediction. This setting is natural as in real-world situations, predictions are provided by ML algorithms that rarely come with worst-case guarantees on their accuracy. Thus, the difﬁculty in designing a learning augmented algorithm is to ﬁnd a good balance: on the one hand, following blindly the prediction might lead to a very bad solution if the prediction is misleading. On the other hand if the algorithm does not trust the prediction at all, it will simply never beneﬁt from an excellent prediction. The aforementioned results solve this issue by designing smart algorithms which exploit the problem structure to achieve a good trade-off between these two cases. In this ∗Equal Contribution. arXiv:2010.11632v1  [cs.LG]  22 Oct 2020paper we take a different perspective. Instead of focusing on a speciﬁc problem trying to integrate predictions, we show how to extend a very powerful algorithmic method, the Primal-Dual method, into the design of online learning augmented algorithms. We underline that despite the generality of our extension technique, it produces online learning augmented algorithms in a fairly simple and straightforward manner. The Primal-Dual method.The Primal-Dual (PD) method is a very powerful algorithmic technique to design online algorithms. It was ﬁrst introduced by Alon et al. [1] to design an online algorithm for the classical online set cover problem and later extended to many other problems such as weighted caching ([3]), revenue maximization in ad-auctions, TCP acknowledgement and ski rental [5]. We mention the survey of Buchbinder and Naor [4] for more references about this technique. In a few words, the technique consists in formulating the online problem as a linear program P complemented by its dual D. Subsequently, the algorithm builds online a feasible fractional solution to both the primal P and dual D. Every time an update of the primal and dual variables is made, the cost of the primal increases by some amount ∆P while the cost of the dual increases by some amount ∆D. The competitive ratio of the fractional solution is then obtained by upper bounding the ratio ∆P ∆D and using weak duality. The integral solution is then obtained by an online rounding scheme of the fractional solution. Preliminary notions for Learning Augmented (LA) algorithms.LA algorithms receive as input a prediction A, an instance Iwhich is revealed online, a robustness parameter λ, and output a solution of cost cALG(A,I,λ). Intuitively, λindicates our conﬁdence in the prediction with smaller values reﬂecting high conﬁdence. We denote by S(A,I) the cost of the output solution on input Iif the algorithm follows blindly the prediction A. We avoid deﬁning explicitly prediction Ato easily ﬁt different prediction cases. For instance if the prediction Ais a predicted solution (without necessarily revealing the predicted instance) then following blindly the solution would simply mean to output the predicted solution A. For each result presented in this paper, it will be clear what is the prediction Aand the cost S(A,I). Given this, we restate some useful deﬁnitions introduced in [19, 25] in our context. For any 0 <λ ⩽ 1, we will say that an LA algorithm is C(λ)-consistent and R(λ)-robust if the cost of the output solution satisﬁes: cALG(A,I,λ) ⩽ min {C(λ) ·S(A,I),R(λ) ·OPT(I)} (1) If Ais accurate (S(A,I) ≈OPT(I)) and at the same time we trust the prediction, we would like our performance to be close to the optimal ofﬂine. Thus, ideally C(λ) should approach 1 as λapproaches 0. On the same spirit, a value of λclose to 1 denotes no trust to the prediction, and in that case, our algorithm should not be much worse than the best pure online algorithm. Therefore, R(1) should be close to the competitive ratio of the best pure online algorithm. We also mention that in some other papers such as [25], a smoothness criterion on the consistency bound is required. In these papers the setting is slightly different. Indeed, prediction Ais a predicted instance Ipred and the error is deﬁned to describe how far Ipred is from the real instance I. With this in mind, an algorithm is said to be smooth if the performance degrades smoothly as the error increases. We emphasize that, in the applications considered in this paper, this smoothness property is implicitly included in the value of S(A,I) which degrades smoothly with the quality of the prediction. Our contributions. We show how to extend the Primal-Dual method (when predictions are pro- vided) for solving problems that can be formulated as covering problems. The algorithms designed using this technique receive as input a robustness parameter λand incorporate a prediction. If the prediction is accurate our algorithms can be arbitrarily close to the optimal ofﬂine (beating known lower bounds of the classical online algorithms) while being robust to failures of the predictor. We ﬁrst apply our Primal-Dual Learning Augmented (PDLA) technique to the online version of the weighted set cover problem, which constitutes the most canonical example of a covering Linear Program (LP). For that problem we show how we can easily modify the Primal-Dual algorithm to incorporate predictions. Even though in this case, prediction may not seem very natural, this result reveals that we can use PDLA to design learning augmented algorithms for the large class of problems that can be formulated as a covering LP. We then continue by addressing problems in which the prediction model is much more natural. Using the PDLA technique, we ﬁrst design an algorithm which recovers the results of Purohit et al. [25] for the ski rental problem, and we also prove that the consistency-robustness trade-off of that algorithm is optimal. We additionally design a learning augmented algorithm for a generalization of the ski rental, namely the Bahncard problem. 2Finally, we turn our attention to a problem which arises in network congestion control, the TCP acknowledgement problem. We design an LA algorithm for that problem and conduct experiments which conﬁrm our claims. We note that the analysis of the algorithms designed using PDLA is (arguably) simple and boils down to (1) proving robustness with (essentially) the same proof as in the original Primal-Dual technique and (2) proving consistency using a simple charging argument that, without making use of the dual, relates the cost incurred by our algorithms to the prediction. In addition to that, using PDLA, the design of online LA algorithms is almost automatic. We emphasize that the preexisting online rounding schemes to obtain an integral solution from a fractional solution still apply to our learning augmented algorithms. Hence in all the paper we focus only on building a fractional solution and provide appropriate references for the rounding scheme. 2 General PDLA method In this section we apply PDLA to solve the online weighted set cover problem when provided with predictions. Set cover is arguably the most canonical example of a covering problem and the framework that we develop readily applies to other covering problems. In particular, we use the framework to give tight or nearly-tight LA algorithms for ski rental, Bahncard, and dy- namic TCP acknowledgement, which are all problems that can be formulated as covering LPs. Primal minimize ∑ S∈F wSxS subject to: ∑ S∈F(e) xS ≥1 ∀e ∈U xS ≥0 ∀S ∈F Dual maximize ∑ e∈U ye subject to: ∑ e∈S ye ≤wS ∀S ∈F ye ≥0 ∀e ∈U Figure 1: Primal Dual formulation of weighted set cover The weighted set cover problem.In this problem, we are given a universe U= {e1,e2,...,e n}of nelements and a family Fof m sets over this universe, each set S ∈ Fhas a weight wS and each element e is cov- ered by any set in F(e) = {S ∈ F |e ∈ S}. Let d= maxe∈U|F(e)|denote the maximum number of sets that cover one element. Our goal is to select sets so as to cover all elements while minimizing the total weight. In its online version, elements are given one by one and it is unknown to the algorithm which elements will arrive and in which order. When a new element arrives, it is required to cover it by adding a new set if necessary. Removing a set from the current solution to decrease its cost is not allowed. Alon et al. in [1] ﬁrst studied the online version designing an almost optimal O(log nlog d)- competitive algorithm. We note that the O(log n) factor comes from the integrality gap of the linear program formulation of the problem (Figure 1) while the O(log d) is due to the online nature of the problem. Since Alon et al. [1] designed an online rounding scheme at a multiplicative cost of O(log n), we will focus on building an increasing fractional solution to the set cover problem (i.e. xS can only increase over time for all S). PDLA for weighted set cover.Algorithm 2 takes as input a predicted covering A⊂F and a robustness parameter λ∈[0,1]. While an instance Iis revealed in an online fashion, an increasing fractional solution {xS}S∈F∈[0,1]F is built. Note that F(e) ∩A are the sets which cover ein the prediction. To simplify the description, we assume that |F(e) ∩A| ⩾ 1,∀e, i.e. the prediction forms a feasible solution. The algorithm without this assumption can be found in appendix A. Algorithm Intuition. We ﬁrst turn our attention to the original online algorithm of Alon et. al. [1] described in Algorithm 1. To get an intuition assume that wS = 1 ,∀S and consider the very ﬁrst arrival of an element e. After the ﬁrst execution of the while loop, e is covered and xS = 1 |F(e)|,∀S ∈F(e). In other words, the online algorithm creates a uniform distribution over the sets in F(e), reﬂecting in such a way his unawareness about the future. On the contrary Algorithm 2 uses the prediction to adjust the increase rate of primal variables, augmenting more aggressively primal variables of sets which are predicted to be in the optimal ofﬂine solution. Indeed, after the ﬁrst execution of the while loop, sets which belong to Aget a value of λ |F(e)|+ 1−λ |F(e)∩A| while sets which are not chosen by the prediction get λ |F(e)|. We continue by exposing our main conceptual contribution. To that end letS(A,I) denote the cost of the covering solution described by prediction Aon instance I. 3Algorithm 1PRIMAL DUAL METHOD FOR ONLINE WEIGHTED SET COVER [1]. Initialize: xS ←0, ye ←0 ∀S,e for allelement ethat just arrived do while ∑ S∈F(e) xS <1 do /* Primal Update for allS ∈F(e) do xS ←xS ( 1 + 1 wS ) + 1 wS|F(e)| end for /* Dual Update ye ←ye + 1 end while end for ⇒⇒ Algorithm 2 PDLA FOR ONLINE WEIGHTED SET COVER . Input: λ, A Initialize: xS ←0, ye ←0 ∀S,e for allelement ethat just arrived do while ∑ S∈F(e) xS <1 do /* Primal Update for allS ∈F(e) and S ∈A do xS ←xS ( 1 + 1 wS ) + λ wS|F(e)|+ 1−λ wS|F(e)∩A| end for for allS ∈F(e) and S ̸∈A do xS ←xS ( 1 + 1 wS ) + λ wS|F(e)| end for /* Dual Update ye ←ye + 1 end while end for Theorem 1.Assuming Ais a feasible solution, the cost of the fractional solution output by Algorithm 2 satisﬁes cPDLA(A,I,λ) ⩽ min { O ( 1 1 −λ ) ·S(A,I),O ( log (d λ )) ·OPT(I) } Proof sketch. The proof is split in two parts. The ﬁrst part is to bound the cost of the algorithm by the term O ( 1 1−λ ) ·S(A,I). As mentioned in the introduction we use a charging argument to do so. After each execution of the while loop we can decompose the primal increase into two parts. ∆Pc which denotes the increase due to sets in F(e) ∩Aand ∆Pu which denotes the increase due to sets in F(e) \\A, thus for the overall primal increase ∆P we have ∆P = ∆Pc + ∆Pu. We continue by upper bounding ∆Pu as a function of λand ∆Pc, that is ∆Pu ⩽ O(1+λ 1−λ)∆Pc, and deducing that ∆P ⩽ O( 1 1−λ)∆Pc. Now the consistency proof terminates by noting that since ∆Pc is generated by sets in the prediction, we can charge this increase to S(A,I). The robustness bound, which is independent of the prediction, is retrieved by mimicking the proof of the original online algorithm of Alon et al. [1]. See appendix A for more details. 3 The Ski rental problem Primal minimize B ·x + ∑ j∈[N] fj subject to: x + fj ≥1 ∀j ∈[N] x, fj ≥0 ∀j ∈[N] Dual maximize ∑ j∈[N] yj subject to: ∑ j∈[N] yj ≤B 1 ≥yj ≥0 ∀j ∈[N] Figure 2: Primal dual formulation of the ski rental problem. As another application of PDLA we design a learning augmented algorithm for one of the simplest and well studied online problems, the ski rental problem. In this problem, every new day, one has to decide whether to rent skis for this day, which costs 1 dollar or to buy skis for the rest of the vacation at a cost of Bdollars. In its ofﬂine version the total number of vacation days, N, is known in advance and the problem becomes trivial. From the primal- dual formulation of the problem (Figure 2) it is clear that if B < N, the optimal strategy is to buy the skis at day one while if B ⩾ N the optimal strategy is to always rent. In the online setting the difﬁculty relies in the fact that we do not know N in advance. A deterministic 2-competitive online algorithm has been known for a long time [13] and a randomized e e−1 ≈1.58-competitive algorithm was also designed later [14]. Both competitive ratios are known to be optimal for deterministic and randomized algorithms respectively. This problem was already studied in various learning augmented settings [10, 16, 25, 28]. Our approach recovers, using the 4primal-dual method, the results of [ 25]. As in [ 25] our prediction Awill be the total number of vacation days Npred. PDLA for ski rental. To simplify the description, we denote an instance of the problem as I= (N,B) and deﬁne the function e(z) = (1 + 1/B)z·B. Note that if B →∞, then e(z) ap- proaches ez hence the choice of notation. In an integral solution, the variable xis 1 to indicate that the skis are bought and 0 otherwise. In the same spirit fj indicates whether we rent on day jor not. Buchbinder et al. [5] showed how to easily turn a fractional monotone solution (i.e. it is not permitted to decrease a variable) to an online randomized algorithm of expected cost equal to the cost of the fractional solution. Hence we focus only on building online a fractional solution. Algorithm 3 is due to [5] and uses the Primal-Dual method to solve the problem. Each new day ja new constraint x+ fj ⩾ 1 is revealed. To satisfy this constraint, the algorithm updates the primal and dual variables while trying to maintain (1) the ratio ∆P/∆D as small as possible and (2) the primal and dual solutions feasible. As in the online weighted set cover problem, the key idea for extending Algorithm 3 to the learning augmented Algorithm 4 is to use the prediction Npred in order to adjust the rate at which each variable is increased. Thus, when Npred >B we increase the buying variable more aggressively than the pure online algorithm. Here, the cost of following blindly the prediction Npred is S(Npred,I) = B·1 {Npred >B }+ N ·1 {Npred ⩽ B}. Algorithm 3 PRIMAL DUAL FOR SKI- RENTAL [5]. Initialize: x←0, fj ←0, ∀j c←e(1), c′←1 for each new day js.t. x+ fj <1 do /* Primal Update fj ←1 −x x←(1 + 1 B)x+ 1 (c−1)·B /* Dual Update yj ←c′ end for ⇒⇒ Algorithm 4PDLA FOR SKI-RENTAL . Input: λ, Npred Initialize: x←0, fj ←0, ∀j if Npred ⩾ Bthen /* Prediction suggests buying c←e(λ), c′←1 else /* Prediction suggests renting c←e(1/λ), c′←λ end if for each new day js.t. x+ fj <1 do /* Primal Update fj ←1 −x x←(1 + 1 B)x+ 1 (c−1)·B /* Dual Update yj ←c′ end for In the following we assume that either λBor B/λis an integer (depending on whether cequals e(λ) or e(1/λ) respectively in Algorithm 4). Our results do not change qualitatively by rounding up to the closest integer. See appendix B for details. Theorem 2(PDLA for ski rental). For any λ∈(0,1], the cost of PDLA for ski rental is bounded as follows cPDLA(Npred,I,λ) ⩽ min { λ 1 −e(−λ) ·S(Npred,I), 1 1 −e(−λ) ·OPT(I) } Proof sketch. The robustness bound is proved essentially using the same proof as for the original analysis of Algorithm 3 in [5]. For the consistency bound we ﬁrst note that after an update the primal increase is 1 + 1 c−1 , now depending on the value ofcwe distinguish between two cases. If Npred ⩾ B then Algorithm 4 is always aggressive in buying. In this case it is easy to show that at most λB updates are made before we get x ⩾ 1. Once x ⩾ 1, no more updates are needed. Since each aggressive update costs at most 1 + 1 e(λ)−1 = e(λ) e(λ)−1 = 1 1−e(−λ) we get that the total cost paid by Algorithm 4 is at most λB 1−e(−λ) = S(Npred,I) · λ 1−e(−λ) . Similarly, in the second case Npred <B and the algorithm increases the buying variable less aggressively. In this case each update costs at most 1 + 1 e(1/λ)−1 = 1 1−e(−1/λ) and at most N of these updates are made therefore Algorithm 4 5pays at most N 1−e(−1/λ) = S(Npred,I) · 1 1−e(−1/λ) . To conclude the consistency proof, note that 1 1−e(−1/λ) ⩽ λ 1−e(−λ) (see Lemma 19 inequality (2)). In addition to recovering the positive results of [25], we additionally show in appendix D that this consistency-robustness trade-off is optimal. Lemma 3.Any λ 1−e−λ-consistent learning augmented algorithm for ski rental has robustnessR(λ) ⩾ 1 1−e−λ To emphasize how PDLA permits us to tackle more general problems, we apply the same ideas to a generalization of the ski-rental problem, namely, the Bahncard problem [9]. This problem models a situation where a tourist travels every day multiple trips. Before any new trip, the tourist has two choices, either to buy a ticket for that particular trip at a cost of 1 or buy a discount card, at a cost of B, that allows to buy tickets at a cheaper price of β <1. The discount card remains valid during T days. Note that ski-rental is modeled by taking β = 0 and T − →∞. In the learning augmented version of the problem we are given a prediction Awhich consists in a collection of times where we are advised to acquire the discount card. We state the main result on this problem and defer the proof to Appendix B. Theorem 4(PDLA for the Bahncard problem). For any λ∈(0,1], any β ∈[0,1] and B 1−β − →∞, we have the following guarantees on any instance Iand prediction A costPDLA(A,I,λ) ⩽ min { λ 1 −β+ λβ ·eλ −β eλ −1 ·S(A,I),eλ −β eλ −1 ·OPT(I) } 4 Dynamic TCP acknowledgement Primal minimize ∑ t∈T xt + ∑ j∈M ∑ t|t≥t(j) 1 d fjt subject to: fjt + ∑t k=t(j) xk ≥1 ∀j, t≥t(j) fjt ≥0 ∀j, t≥t(j) xt ≥0 ∀t ∈T Dual maximize ∑ j∈M ∑ t|t≥t(j) yjt subject to: ∑ j|t≥t(j) ∑ t′≥t yjt ≤1 ∀t ∈T 0 ≤yjt ≤1 d ∀j, t≥t(j) Figure 3: Primal Dual formulation of the TCP acknowledgement problem In this section, we continue by applying PDLA to a classic network congestion problem of the Transmission Control Protocol (TCP). During a TCP interaction, a server receives a stream of packets and replies back to the sender ac- knowledging that each packet arrived correctly. Instead of sending an acknowledgement for each packet separately, the server can choose to delay its response and acknowledge multiple packets simultaneously via a single TCP response. Of course, in this scenario there is an additional cost incurred due to the delayed packets, which is the total latency incurred by those packets. Thus, on one hand sending too many acknowledgments (acks) overloads the network, on the other hand sending one ack for all the packets slows down the TCP interaction. Hence a good trade-off has to be achieved and the objective function which we aim to minimize will be the sum of the total number of acknowledgements plus the total latency. The problem was ﬁrst modeled by Dooly et al. [8], where they showed how to solve the ofﬂine problem optimally in quadratic time along with a deterministic 2-competitive online algorithm. Karlin et al. [15] provided the ﬁrst e e−1 -competitive randomized algorithm which was later shown to be optimal by Seiden in [27]. The problem was later solved using the primal-dual method by Buchbinder et al. [5] who also obtained an e e−1 -competitive algorithm. Figure 3 presents the primal-dual formulation of the problem. In this formulation each packet jarrives at time t(j) and is acknowledged by the ﬁrst ack sent after t(j). Here, variable xt corresponds to sending an ack at time tand fjt is set to one (in the integral solution) if packet jwas not acknowledged by timet. The time granularity is controlled by the parameterdand each additional time unit of latency comes at a cost of 1/d. As in the ski rental problem, there is no integrality gap and a fractional monotone solution can be converted to a randomized algorithm in a lossless manner (see [5] for more details). 64.1 The PDLA algorithm and its theoretical analysis Our prediction consists in a collection of times Ain which the prediction suggests sending an ack. Let α(t) be the next time t′ ⩾ twhen prediction sends an ack. With this deﬁnition each packet j, if the prediction is followed blindly, is acknowledged at time α(t(j)) incurring a latency cost of (α(t(j)) −t(j)) ·1 d. In the same spirit as for the ski rental problem we adapt the pure online Algorithm 5 into the learning augmented Algorithm 6. Algorithm 6 adjusts the rate at which we increase the primal and dual variables according to the prediction A. Thus if a packet jat time t is \"uncovered\" (∑t k=t(j) xk+ fjt <1) by our fractional solution and \"covered\" by A(α(t(j)) ⩽ t) we increase xt at a faster rate. To simplify the description of Algorithm 6 we deﬁne e(z) = (1 + 1 d)z·d. To get to the continuous time case, we will take the limitd→∞ so the reader should think intuitively as e(z) ≈ez. Algorithm 5 PRIMAL DUAL METHOD FOR TCP ACKNOWLEDGEMENT [5]. Initialize: x←0, y←0 for alltimes tdo for all packages j such that∑t k=t(j) xk <1 do c← −e(1), c′← −1/d /* Primal Update fjt ←1 −∑t k=t(j) xk xt ←xt + 1 d · (∑t k=t(j) xk + 1 c−1 ) /* Dual Update yjt ←c′ end for end for ⇒⇒ Algorithm 6PDLA FOR TCP ACKNOWLEDGE - MENT Input: λ, A Initialize: x←0, y←0 for alltimes tdo for allpackages jsuch that ∑t k=t(j) xk <1 do if t⩾ α(t(j)) then /* Prediction already acknowledged packetj c← −e(λ), c′← −1/d else /* Prediction did not acknowledge packet jyet c← −e(1/λ), c′← −λ/d end if /* Primal Update fjt ←1 −∑t k=t(j) xk xt ←xt + 1 d · (∑t k=t(j) xk + 1 c−1 ) /* Dual Update yjt ←c′ end for end for We continue by presenting Algorithm’s 6 guarantees together with a proof sketch. As before I denotes the TCP ack problem instance which is revealed in an online fashion. The full proof is deferred to appendix C. Theorem 5(PDLA for TCP-ack). For any prediction A, any instance Iof the TCP ack problem, any parameter λ ∈(0,1], and d →∞: Algorithm 6 outputs a fractional solution of cost at most cPDLA(A,I,λ) ⩽ min { λ 1−e−λ ·S(A,I), 1 1−e−λ ·OPT(I) } Proof sketch. The two bounds are proven separately. For the robustness bound, while our analysis is slightly more technical, we use the same idea as the original analysis in [ 5]. That is, upper bounding the ratio ∆P/∆Din every iteration and using weak duality. The consistency proof uses a simple charging scheme that can be seen as a generalization of our consistency proof for the ski rental problem. We essentially have two cases, big ( c = e(λ)) and small ( c = e(1/λ)) updates. In the case of a small update, a simple calculation reveals that the increase in cost of the solution is at most ∆P = 1 d ( 1 −∑t k=t(j) xk ) + 1 d (∑t k=t(j) xk + 1 e(1/λ)−1 ) = 1 d ( 1 + 1 e(1/λ)−1 ) = 1 d · ( 1 1−e(−1/λ) ) . Notice then whenever Algorithm 6 does a small update at time tdue to request j, prediction Apays a latency cost of 1/dsince it has not yet acknowledged request j. Hence the primal increase of cost which is at most 1 d · 1 1−e(−1/λ) can be charged to the latency cost 1/dpaid by Awith a multiplicative factor 1 1−e(−1/λ) ⩽ λ 1−e(−λ) (see Lemma 19, inequality (3)). The case of big updates is slightly different. Consider a time t0 at which Asends an acknowledgement and consider the big updates performed by Algorithm 6 for packets jarrived before that time (t(j) ⩽ t0). We claim that at most ⌈λd⌉such big updates can be made. Indeed, big updates are more aggressive (i.e. xt increases 7faster), and a “covering” due to∑t k=t0 xk ⩾ 1 is reached after only ⌈λd⌉updates (after this point, the packets arrived before time t0 will never force Algorithm 6 to make an update). Thus Algorithm’s 6 cost due to these big updates is at most ⌈λd⌉·(cost of a big update) = ⌈λd⌉·(1 d · λ 1−e(−λ) ) which can be charged to the cost of 1 incurred by Afor sending an ack at time t0. 4.2 Experiments We present experimental results that conﬁrm the theoretical analysis of Algorithm 6 for the TCP acknowledgement problem. The code is publicly available at https://github.com/etienne4/ PDLA. We experiment on various types of distribution for packet arrival inputs. Historically, the distribution of TCP packets was often assumed to follow some Poisson distribution ([ 20, 30]). However, it was later shown than this assumption was not always representative of the reality. In particular real-world distributions often exhibit a heavy tail (i.e. there is still a signiﬁcant probability of seeing a huge amount of packets arriving at some time). To better integrate this in models, heavy tailed distributions such as the Pareto distribution are often suggested (see for instance [11, 22]). This motivates our choice of distributions for random packet arrival instances. We will experiment on Poisson distribution, Pareto distribution and a custom distribution that we introduce and seems to generate the most challenging instances for our algorithms. Input distributions. In all our instances, we set the subdivision parameter dto 100 which means that every second is split into 100 time units. Then we deﬁne an array of length 1000 where the i-th entry deﬁnes how many requests arrive at the i-th time step. Each entry in the array is drawn independently from the others from a distribution D. In the case of a Poisson distribution, we set D= P(1) (the Poisson distribution of mean 1). For the Pareto distribution, we choose Dto be the Lomax distribution (which is a special case of Pareto distribution) with shape parameter set to 2 ([29]). Finally, we deﬁne the iterated Poisson distribution as follows. Fix an integer n> 0 and µ> 0. Draw X1 ∼P(µ). Then for ifrom 2 to ndraw Xi ∼P (Xi−1). The ﬁnal value returned is Xn. This distribution, while still having an expectation of µ, appears to generate more spikes than the classical Poisson distribution. The interest of this distribution in our case is that it generates more challenging instances than the other two (i.e. the competitive ratios of online algorithms are closer to the worst-case bounds). In our experiments, we choose µ= 1 and n= 10. Plots of typical instances under these laws can be seen in appendix C. Note that for all these distributions, the expected value for each entry is 1. Noisy prediction. The prediction Ais produced as follows. We perturb the real instances with noise, then compute an optimal solution on this perturbed instance and use this as a prediction. More precisely, we introduce a replacement rate p ∈[0,1]. Then we go through the instance generated according to some distribution Dand for each each entry at index 1 ⩽ i⩽ 1000, with probability p we set this entry to 0 (i.e. we delete this entry) and with probability pwe add to this entry a random variable Y ∼D. Both operations, adding and deleting, are performed independently of each other. We then test our algorithm with 4 different values of robustness parameter λ∈{1,0.8,0.6,0.4}. Results. The plots in Figure 4 present the average competitive ratios of Algorithm 6 over 10 experiments for each distribution and each value of λ. As expected, with a perfect prediction, setting a lower λwill yield a much better solution while setting λ= 1 simply means that we run the pure online algorithm of Buchbinder et al. [5] (that achieves the best possible competitive ratio for the pure online problem). On the most challenging instances generated by the iterated Poisson distribution (Figure 4c), even with a replacement rate of 1 where the prediction is simply an instance totally uncorrelated to the real instance, our algorithm maintains good guarantees for small values of λ. We note that in all the experiments the competitive ratios achieved by Algorithm 6 are better than the robustness guarantees of Theorem 5, which are {1.58,1.68,2.21,3.03}for λ ∈{1,0.8,0.6,0.4} respectively. In addition to that, all the competitive ratios degrade smoothly as the error increases which conﬁrms our earlier discussion about smoothness. 80.0 0.2 0.4 0.6 0.8 1.0 Replacement rate 1.1 1.2 1.3Competitive ratio Lambda = 1 Lambda = 0.8 Lambda = 0.6 Lambda = 0.4 (a) Poisson distribution 0.0 0.2 0.4 0.6 0.8 1.0 Replacement rate 1.1 1.2 1.3Competitive ratio Lambda = 1 Lambda = 0.8 Lambda = 0.6 Lambda = 0.4 (b) Pareto distribution 0.0 0.2 0.4 0.6 0.8 1.0 Replacement rate 1.1 1.2 1.3 1.4 1.5 1.6Competitive ratio Lambda = 1 Lambda = 0.8 Lambda = 0.6 Lambda = 0.4 (c) Iterated Poisson distribution Figure 4: Competitive ratios under various distributions and replacement rates from 0 to 1 5 Future Directions In this paper we present the PDLA technique, a learning augmented version of the classic Primal-Dual technique, and apply it to design algorithms for some classic online problems when a prediction is provided. Since the Primal-Dual technique is used to solve many more covering problems, like for instance weighted caching or load balancing [4], an interesting research direction would be to apply PDLA to tackle those problems and (hopefully) get tight consistency-robustness trade-off guarantees (as the one achieved by Algorithm 4 and proved in Lemma 3). In addition to that, we suspect that this work might provide insights not only for covering but also for some packing problems which are solved using the Primal-Dual technique in the classic online model (e.g. revenue maximization in ad-auctions [5]). Finally, another interesting direction would be to incorporate predictions into the Primal-Dual technique when used to solve covering problems where the objective function is non linear (e.g. convex). Broader Impact The ﬁeld of learning augmented algorithms lies in the intersection of machine learning and online algorithms, trying to combine the best of the two worlds. Learning augmented algorithms are particularly suited for critical applications where maintaining worst-case guarantees is mandatory but at the same time predictions about the future are possible. Thus, our work represents a stepping stone towards (easily) integrating ML predictions in such applications, increasing this way the possible beneﬁts of ML to society. PDLA offers a recipe on how to incorporate predictions to tackle classical covering online problems, that is to ﬁrst solve the online problem using the Primal-Dual technique and then use the prediction to change the rate at which primal and dual variables increase or decrease. We believe that since the idea behind this technique is simple and does not require too much domain- speciﬁc knowledge, it might be applicable to different problems and can also be implemented in practice. Acknowledgments and Disclosure of Funding This research is supported by the Swiss National Science Foundation project 200021-184656 “Ran- domness in Problem Instances and Randomized Algorithms”. Andreas Maggiori was supported by the Swiss National Science Fund (SNSF) grant no 200020_182517/1 “Spatial Coupling of Graphical Models in Communications, Signal Processing, Computer Science and Statistical Physics”. References [1] Noga Alon, Baruch Awerbuch, and Yossi Azar. The online set cover problem. In Proceedings of the Thirty-Fifth Annual ACM Symposium on Theory of Computing, STOC ’03, page 100–105, New York, NY , USA, 2003. Association for Computing Machinery. ISBN 1581136749. doi: 10.1145/780542.780558. URL https://doi.org/10.1145/780542.780558. [2] Antonios Antoniadis, Christian Coester, Marek Elias, Adam Polak, and Bertrand Simon. Online metric algorithms with untrusted predictions, 2020. 9[3] N. Bansal, N. Buchbinder, and J. Naor. A primal-dual randomized algorithm for weighted paging. In 48th Annual IEEE Symposium on Foundations of Computer Science (FOCS’07), pages 507–517, 2007. [4] Niv Buchbinder and Joseph (Sefﬁ) Naor. The design of competitive online algorithms via a pri- mal: Dual approach. Found. Trends Theor. Comput. Sci., 3(2–3):93–263, February 2009. ISSN 1551-305X. doi: 10.1561/0400000024. URL https://doi.org/10.1561/0400000024. [5] Niv Buchbinder, Kamal Jain, and Joseph (Sefﬁ) Naor. Online primal-dual algorithms for maximizing ad-auctions revenue. In Lars Arge, Michael Hoffmann, and Emo Welzl, editors, Algorithms – ESA 2007, pages 253–264, Berlin, Heidelberg, 2007. Springer Berlin Heidelberg. [6] Eunjoon Cho, Seth A. Myers, and Jure Leskovec. Friendship and mobility: User movement in location-based social networks. In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD ’11, page 1082–1090, New York, NY , USA, 2011. Association for Computing Machinery. ISBN 9781450308137. doi: 10.1145/2020408.2020579. URL https://doi.org/10.1145/2020408.2020579. [7] Yihe Dong, Piotr Indyk, Ilya Razenshteyn, and Tal Wagner. Learning space partitions for nearest neighbor search. In Eighth International Conference on Learning Repre- sentations (ICLR) , April 2020. URL https://www.microsoft.com/en-us/research/ publication/learning-space-partitions-for-nearest-neighbor-search/ . [8] Daniel R Dooly, Sally A Goldman, and Stephen D Scott. Tcp dynamic acknowledgment delay (extended abstract) theory and practice. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 389–398, 1998. [9] Rudolf Fleischer. On the bahncard problem. Theoretical Computer Science, 268(1):161 – 174, 2001. ISSN 0304-3975. doi: https://doi.org/10.1016/S0304-3975(00)00266-8. URL http: //www.sciencedirect.com/science/article/pii/S0304397500002668. On-line Al- gorithms ’98. [10] Sreenivas Gollapudi and Debmalya Panigrahi. Online algorithms for rent-or-buy with expert advice. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 ofProceedings of Machine Learning Research, pages 2319–2327, Long Beach, California, USA, 09–15 Jun 2019. PMLR. URL http://proceedings.mlr.press/v97/gollapudi19a.html. [11] Weibo Gong, Yong Liu, Vishal Misra, and Don Towsley. On the tails of web ﬁle size distributions. In in: Proceedings of 39th Allerton Conference on Communication, Control, and Computing, 2001. [12] Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation algorithms. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 , 2019. URL https://openreview.net/forum?id= r1lohoCqY7. [13] A. R. Karlin, M. S. Manasse, L. Rudolph, and D. D. Sleator. Competitive snoopy caching. In 27th Annual Symposium on Foundations of Computer Science (sfcs 1986), pages 244–254, 1986. [14] Anna R. Karlin, Mark S. Manasse, Lyle A. McGeoch, and Susan Owicki. Competitive ran- domized algorithms for non-uniform problems. In Proceedings of the First Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’90, page 301–309, USA, 1990. Society for Indus- trial and Applied Mathematics. ISBN 0898712513. [15] Anna R. Karlin, Claire Kenyon, and Dana Randall. Dynamic tcp acknowledgement and other stories about e/(e-1). In Proceedings of the Thirty-Third Annual ACM Symposium on Theory of Computing , STOC ’01, page 502–509, New York, NY , USA, 2001. Association for Computing Machinery. ISBN 1581133499. doi: 10.1145/380752.380845. URL https: //doi.org/10.1145/380752.380845. 10[16] Rohan Kodialam. Optimal algorithms for ski rental with soft machine-learned predictions. CoRR, abs/1903.00092, 2019. URL http://arxiv.org/abs/1903.00092. [17] Silvio Lattanzi, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Online schedul- ing via learned weights. In Proceedings of the 2020 ACM-SIAM Symposium on Discrete Algorithms, SODA 2020, Salt Lake City, UT, USA, January 5-8, 2020, pages 1859–1877, 2020. doi: 10.1137/1.9781611975994.114. URL https://doi.org/10.1137/1.9781611975994. 114. [18] Russell Lee, Mohammad H. Hajiesmaili, and Jian Li. Learning-assisted competitive algorithms for peak-aware energy scheduling. CoRR, abs/1911.07972, 2019. URL http://arxiv.org/ abs/1911.07972. [19] Thodoris Lykouris and Sergei Vassilvitskii. Competitive caching with machine learned ad- vice. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018 , pages 3302–3311, 2018. URL http://proceedings.mlr.press/v80/lykouris18a.html. [20] M. Marathe and W. Hawe. Predicted capacity of ethernet in a university environment. In Proceedings of Southcon 1982, pages 1–10, 1982. [21] Andres Muñoz Medina and Sergei Vassilvitskii. Revenue optimization with approxi- mate bid predictions. In Advances in Neural Information Processing Systems 30: An- nual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA , pages 1858–1866, 2017. URL http://papers.nips.cc/paper/ 6782-revenue-optimization-with-approximate-bid-predictions . [22] Michael Mitzenmacher. Dynamic models for ﬁle sizes and double pareto distributions. In- ternet Math., 1(3):305–333, 2003. URL https://projecteuclid.org:443/euclid.im/ 1109190964. [23] Michael Mitzenmacher. A model for learned bloom ﬁlters and optimizing by sand- wiching. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31 , pages 464–473. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/ 7328-a-model-for-learned-bloom-filters-and-optimizing-by-sandwiching. pdf. [24] Michael Mitzenmacher. Scheduling with Predictions and the Price of Misprediction. arXiv e-prints, art. arXiv:1902.00732, February 2019. [25] Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ML predictions. In Advances in Neural Information Processing Systems 31: Annual Con- ference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montréal, Canada, pages 9684–9693, 2018. URL http://papers.nips.cc/paper/ 8174-improving-online-algorithms-via-ml-predictions . [26] Dhruv Rohatgi. Near-optimal bounds for online caching with machine learned advice. In Proceedings of the Thirty-First Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’20, page 1834–1845, USA, 2020. Society for Industrial and Applied Mathematics. [27] Steven S. Seiden. A guessing game and randomized online algorithms. In Proceedings of the Thirty-Second Annual ACM Symposium on Theory of Computing, STOC ’00, page 592–601, New York, NY , USA, 2000. Association for Computing Machinery. ISBN 1581131844. doi: 10.1145/335305.335385. URL https://doi.org/10.1145/335305.335385. [28] Shufan Wang, Jian Li, and Shiqiang Wang. Online Algorithms for Multi-shop Ski Rental with Machine Learned Predictions. arXiv e-prints, art. arXiv:2002.05808, February 2020. [29] Wikipedia contributors. Lomax distribution, 2004. URL https://en.wikipedia.org/ wiki/Lomax_distribution. [Online; accessed 18-May-2020]. [30] Michael Wilson. A historical view of network trafﬁc models. http://www.cse.wustl.edu/ ~jain/cse567-06/traffic_models2.htm, 2006. 11[31] Yinfeng Xu and Weijun Xu. Competitive algorithms for online leasing problem in probabilis- tic environments. In Advances in Neural Networks - ISNN 2004, International Symposium on Neural Networks, Dalian, China, August 19-21, 2004, Proceedings, Part II , pages 725– 730, 2004. doi: 10.1007/978-3-540-28648-6\\_116. URL https://doi.org/10.1007/ 978-3-540-28648-6_116 . 12A Missing proofs for Set Cover We ﬁrst present the slightly modiﬁed algorithm where we do not need the prediction to form a feasible solution. As mentioned in the main paper, when an element eis uncovered by the prediction, i.e. |F(e) ∩A| = 0, we just run the purely online algorithm (λ= 1). Algorithm 7PDLA FOR ONLINE WEIGHTED SET COVER . Input: λ, A Initialize: xS ←0, ye ←0 ∀S,e for allelement ethat just arrived do while ∑ S∈F(e) xS <1 do for allS ∈F(e) do if |F(e) ∩A| ⩾ 1 then /* Primal Update (more aggressive if 1 {S ∈A} = 1) xS ←xS ( 1 + 1 wS ) + λ wS·|F(e)|+ (1−λ)·1 {S∈A} wS·|F(e)∩A| else /* e is not covered by the prediction xS ←xS · ( 1 + 1 wS ) + 1 wS·|F(e)| end if end for /* Dual Update ye ←ye + 1 end while end for We start by proving that the dual constraints are only violated by a multiplicative factor ofO ( log (d λ )) . Thus, scaling down the dual solution of Algorithm 7 by O ( log (d λ )) creates a feasible dual solution which will permit us to use weak duality. Lemma 6. Let ybe the dual solution built by Algorithm 7. Then y Θ(log(d/λ)) is a feasible solution to the dual problem. Proof. The proof essentially follows the same path as in [4]. The only constraints that can be violated are of the form ∑ e∈Sye ⩽ wS for some S ∈F. Consider one such constraint. At every update of the primal variable xS the sum ∑ e∈Sye increases by 1, since the dual variable corresponding to the newly arrived element increases by 1 . We prove by induction on the number of such updates that at any point in time xS ⩾ λ d (( 1 + 1 wS )∑ e∈Sye −1 ) . Indeed, when no update concerning Sis done we have that xS = 0 and ∑ e∈Sye = 0. Suppose this is true after kupdates of the variable xS, i.e.∑ e∈Sye = k. Now, assume that a newly arrived element e∗∈Sprovokes a primal update from xold S to xnew S and increases its dual value by one, i.e. ynew e∗ = yold e∗ + 1. Then we always have: xnew S ⩾xold S · ( 1 + 1 wS ) + min { 1 |F(e)|·wS , λ |F(e)|·wS + (1 −λ) ·1 {S ∈A} |F(e) ∩A|· wS } ⩾ ⩾xold S · ( 1 + 1 wS ) + λ d·wS Thus, by the induction hypothesis xnew S ⩾λ d (( 1 + 1 wS )∑ e∈S\\{e∗}ye+yold e∗ −1 ) · ( 1 + 1 wS ) + λ d·wS =λ d (( 1 + 1 wS )∑ e∈S\\{e∗}ye+ynew e∗ −1 ) = λ d (( 1 + 1 wS )∑ e∈Sye −1 ) 13Moreover, since wS ⩾ 1, we have that (1 + 1/ws)ws ⩾ 2, thus: xS ⩾ λ d   ( 1 + 1 wS )wS· ∑ e∈Sye wS −1  ⩾ λ d ( 2 ∑ e∈Sye wS −1 ) We continue by upper bounding the value of xS. Note that once xS ⩾ 1, no more primal updates can happen, therefore whenever an update is made we have xS <1 just before the update. Thus: xnew S ⩽ xold S · ( 1 + 1 wS ) + max { λ wS ·|F(e)|+ (1 −λ) ·1 {S ∈A} wS ·|F(e) ∩A| , 1 wS ·|F(e)| } ⩽ xold S ·2 + 1 ⩽ 3 Combining the lower and upper bound on xS we get that: ∑ e∈S ye ⩽ log (3d λ + 1 ) ·wS = O(log (d/λ)) ·wS which concludes the proof. Lemma 7(Robustness). The competitive ratio is always bounded by O ( log (d λ )) Proof. We denote as before by xold S and xnew S the primal variables before and after the update respec- tively. Each time the while loop is executed we have that∑ S∈F(e) xold S <1 and the increase in the dual is ∆D= 1. Denote by δxS = xnew S −xold S the increase of a variable for a speciﬁc set S. If an element is covered by the prediction then it holds that: ∆P = ∑ S∈F(e) wS ·δxS = ∑ S∈F(e)∩A wS ·δxS + ∑ S∈F(e)\\F(e)∩A wS ·δxS = = ∑ S∈F(e) ( xold S + λ |F(e)| ) + ∑ S∈F(e)∩A (1 −λ) |F(e) ∩A| = ∑ S∈F(e) xold S + λ+ 1 −λ⩽ 2 By repeating the same calculation we get that if an element is uncovered by the prediction then: ∆P = ∑ S∈F(e) wS ·δxS = ∑ S∈F(e) ( xold S + 1 |F(e)| ) = ∑ S∈F(e) xold S + 1 ⩽ 2 Overall we have that: 1. At any iteration ∆P ∆D ⩽ 2. 2. The ﬁnal primal solution is feasible. 3. By Lemma 6, denoting ythe ﬁnal dual solution, y Θ(log(d/λ) is feasible. Thus, by weak duality we get that the competitive ratio of Algorithm 7 is upper bounded by 2 ·O(log (d/λ)) = O(log (d/λ))). In the following we do not assume that our prediction Aforms a feasible solution. Therefore we will denote by 1. S(A,I) the cost of the (possibly partial) covering if prediction Ais followed blindly. 2. Cnc the cost of optimally covering elements which are not covered by the prediction. 3. cPDLA(A,I,λ) the cost of the covering solution calculated by Algorithm 7. Lemma 8(Consistency). cPDLA(A,I,λ) ⩽ O ( 1 1−λ ) ·S(A,I) + O(log (d)) ·Cnc 14Proof. We split the analysis in two parts. First, we look at the case when an element which is uncovered by the prediction arrives. In this case Algorithm 7 emulates the pure online algorithm (λ= 1). More precisely, by the same calculations as before, we can show that ync the solution of the dual problem restricted to the uncovered elements satisfy the property that ync O(log d) is feasible. Therefore for those elements by Lemma 7 the cost of Algorithm 7 is upper bounded byO(log d) ·Cnc. We turn our attention to the more interesting case where the prediction covers an element. In this case, after the execution of the while loop we decompose the primal increase into two parts. ∆Pc which denotes the increase due to setsSchosen by A(1 {S ∈A} = 1) and ∆Pu which denotes the increase due to sets Snot chosen by the prediction (1 {S ∈A} = 0), thus we have ∆P = ∆Pc + ∆Pu. Let c= {S ∈F(e) : 1 {S ∈A} = 1}and u= {S ∈F(e) : 1 {S ∈A} = 0}. We then have: ∆Pc = ∑ S∈c xS + λ·|c| |c|+ |u|+ 1 −λ⩾ λ d + 1 −λ ∆Pu = ∑ S∈u xS + λ·|u| |c|+ |u|⩽ 1 + λ since , |c| |c|+|u| ⩾ 1 d and |u| |c|+|u| ⩽ 1. Combining the two bounds we get that ∆Pu ⩽ 1+λ λ d+1−λ ·∆Pc and consequently: ∆P ⩽ ( 1 + 1 + λ λ d + 1 −λ ) ∆Pc = O ( 1 1 −λ ) ∆Pc Since the cost increase ∆Pc is caused by sets which are selected by the prediction, we can charge this cost to the corresponding increase of S(A,I) loosing only a multiplicative O(1) factor. By combining the two cases we conclude the proof. B Missing proofs for ski rental and the Bahncard problem We detail here the missing proofs from section 3. We ﬁrst prove our results regarding the ski rental problem and then focus on the Bahncard problem. B.1 The ski rental problem We provide here a full proof of Theorem 2. In our setting, the prediction Ais the predicted number of skiing days Npred and S(A,I) = S(Npred,I) = B·1 {Npred >B }+ N ·1 {Npred ⩽ B}is the cost of following blindly the prediction. We ﬁrst prove an easy lemma about the feasibility of the dual solution. Lemma 9. Let ybe the dual solution built by Algorithm 4. Then yis a feasible solution (assuming B λ is integral if the prediction suggests to rent). Proof. To see this, note that the only constraint that might by violated is the constraint ∑ j∈[N] yj ⩽ B. Denote by S the value of the sum ∑ j∈[N] yj. Note that once x⩾ 1, the value of S will never change anymore. The value of Sincreases by 1 for every big update and by λfor every small update. In the case Npred >B , the algorithm always does big updates (the prediction suggest to buy). We claim that at most ⌈λB⌉big updates can be made before x⩾ 1. We denote x(k) the value of xafter k updates. We then prove by induction that x(k) ⩾ e(k/B)−1 e(λ)−1 (recall that e(z) = (1 + 1/B)z·B ≈ez). 15Clearly, if k= 0, we have x(0) ⩾ 0. Now assume this is the case for kupdates we then have x(k+ 1) = ( 1 + 1 B ) ·x(k) + 1 (e(λ) −1) ·B ⩾ ( 1 + 1 B ) ·e(k/B) −1 e(λ) −1 + 1 (e(λ) −1) ·B = (1 + 1/B) ·(e(k/B) −1) + 1/B e(λ) −1 = e((k+ 1)/B) −1 e(λ) −1 which ends the induction. Hence at most ⌈λB⌉⩽ Bbig updates can be made before x⩾ 1. This implies that S ⩽ Bat the end of the algorithm. In the case where Npred ⩽ B, we prove in exactly the same way that at most ⌈B λ ⌉ updates are performed before x⩾ 1. Hence we have that S ⩽ λ· ⌈B λ ⌉ . By assumption, we have that B/λis an integer hence S ⩽ 1 and yis again feasible. We can ﬁnish the main proof. Proof of Theorem 2. We prove ﬁrst the robustness bound. By the Lemma 9, we know that the dual solution is feasible. Hence what remains to prove is to upper bound the ratio ∆P ∆D and use weak duality. In the case of a big update we have ∆P ∆D = ∆P = 1 + 1 e(λ) −1 = 1 1 −e(−λ) In the case of a small update we have ∆P ∆D = ∆P λ = 1 λ · 1 1 −e(−1/λ) ⩽ 1 1 −e(−λ) where the last inequality comes from Lemma 19 inequality (2). By weak duality, we have the robustness bound. To prove consistency, we have two cases. IfNpred ⩽ B, then Algorithm 4 does at most N updates, each of cost at most 1 1−e(−1/λ) while the prediction Apays a cost of N. Noting again that, by Lemma 19, 1 1−e(−1/λ) ⩽ λ 1−e(−λ) ends the proof of consistency in this case. The other case is different. As in the proof of Lemma 9, we still have that x(k) ⩾ e(k/B)−1 e(λ)−1 hence at most ⌈λB⌉⩽ Bupdates are done by Algorithm, each of cost at most 1 1−e(−λ) hence a total cost of at most ⌈λB⌉ 1 −e(−λ) Since we assume in this case that λB is integral and that the prediction Apays a cost of B, the competitive ratio is indeed λ 1−e(−λ) B.2 The Bahncard problem History of the problem.The Banhcard problem, which was initially introduced in [9], models a situation where a tourist travels every day multiple trips. Before any new trip, the tourist has two choices, either to buy a ticket for that particular trip at a cost of 1 or buy a discount card, at a cost of B, and use this discount card to get a ticket for a price of β <1. The discount card is then valid for rest of that day and for the next T −1 days. This generalizes the ski rental problem in several ways, ﬁrst the discount expires after a ﬁxed amount of time, second buying only offers a discount and not a free trip. Note that if β = 0 and T − →∞we recover the ski-rental problem. Karlin et al. [15] designed an optimal randomized online algorithm of competitive ratio e e−1+β when B − →∞. 16PDLA for the Bahncard problem.We design, using PDLA, a learning augmented algorithm for the Bahncard problem. The ﬁnal goal is to prove Theorem 4. An interesting feature of our algorithm is that, as for the TCP ack problem, it does not need to be given the full prediction in advance. If Bahncards are bought by the prediction Aat a set of times {t1,t2,...,t k}, the algorithm does not need to know before time ti that the Bahncard iis bought. For instance we could think of the prediction of an employee of the station giving short-term advice to a traveller every time he shows up at the station. We now give the primal dual formulation of the Bahncard problem along with its corresponding learning augmented algorithm. We mention that, to the best of our knowledge, no online algorithm using the primal-dual method was designed before. Hence the primal-dual formulation (Figure 5) of the problem is new. In an integral solution, we would havext = 1 if the solution buys a Bahncard at time tand xt = 0 otherwise. Then fj represents the fractional amount of trip jdone at time t(j) that is bought at full price and dj the amount of the trip bought at discounted price. The ﬁrst natural constraint is the one that says that each trip should be paid entirely either in discounted or full price, i.e. dj + fj ⩾ 1. We then have the constraint ∑t(j) t=t(j)−T xt ⩾ dj that says that to be able to buy a ticket at discounted price, at least one Bahncard must have been bought in the last T time steps. Figure 5: Primal Dual formulation of the Bahncard problem. Primal Dual minimize B·∑ t∈T xt + ∑ j∈M βdj + fj maximize ∑ j∈M cj subject to: dj + fj ⩾ 1 ∀j subject to: cj ⩽ 1 ∀j∑t(j) t=t(j)−T xt ⩾ dj ∀j cj −bj ⩽ β ∀j xt ⩾ 0 ∀t∈T ∑ j:t(j)−T⩽t⩽t(j) bj ⩽ B ∀t∈T dj,fj ⩾ 0 ∀j cj,bj ⩾ 0 ∀j Following the same idea as for the ski rental problem, we will guide the updates in the primal-dual algorithm with the advice provided. We deﬁne a function e(z) = ( 1 + 1−β B )z·(B/(1−β)) . Again for B 1−β →∞, the reader should think intuitively of e(z) as ez. The parameter zwill then take values either λor 1/λdepending on if we want to do a big or small update in the primal. As for ski rental, when we do a small update, we will need to scale down the dual update by a factor of λto maintain feasibility of the dual solution. The rule to decide if an update should be big or small is the following: if the prediction Abought a Bahncard less than T time steps in the past (i.e. if the predicted solution has currently a valid Bahncard) the update should be big. Otherwise the update should be cautious. In algorithm 8, we denote by lA(t) the latest time before time t at which the prediction Abought a Bahncard. We use the convention that lA(t) = −∞if no Bahncard was bought before time t. Of course in this problem it is possible that trips show up while the fractional solution already has a full Bahncard available (i.e.∑t(j) t=t(j)−T xt ⩾ 1). In this case there is no point in buying more fractions of a Bahncard and the algorithm will do what we call a minimal update. 17Algorithm 8LA O NLINE PRIMAL -DUAL FOR THE BAHNCARD PROBLEM Input: λ, A Initialize: x,d,f ←0, c,b ←0 for alltrip jdo if ∑t(j) t=t(j)−T xt ⩾ 1 then dj ←1 cj ←β end if if ∑t(j) t=t(j)−T xt <1 then if t(j) ⩽ lA(t(j)) + T then dj ←∑t(j) t=t(j)−T xt fj ←1 −dj xt(j) ←xt(j) + 1−β B · (∑t(j) t=t(j)−T xt + 1 e(λ)−1 ) bj ←1 −β cj ←bj + β end if if t(j) >lA(t(j)) + T then dj ←∑t(j) t=t(j)−T xt fj ←1 −dj xt(j) ←xt(j) + 1−β B · (∑t(j) t=t(j)−T xt + 1 e(1/λ)−1 ) bj ←λ(1 −β) cj ←bj + β end if end if end for We ﬁrst prove that the dual built by the algorithm is almost feasible. Lemma 10. Let (c,b) be the dual solution built by Algorithm 8, then (c,b) 1+(1−β)/B is feasible. Proof. Note that the constraints cj ⩽ 1 and cj −bj ⩽ β are clearly maintained by the algorithm. And scaling down both cand bby some factor bigger than 1 will not alter their feasibility. Hence we focus only on the constraints of the form ∑ j:t(j)−T⩽t⩽t(j) bj ⩽ Bfor a ﬁxed time t. Note that during a minimal update, the value of bj is not changed hence only small or big updates can alter the value of the sum ∑ j:t(j)−T⩽t⩽t(j) bj. Similarly as for proofs in ski rental, denote by bthe number of big updates that are counted in this sum and by sthe number of small updates in this sum. We ﬁrst notice that once we have that ∑t+T t′=txt′ ⩾ 1, no updates that alter the constraint∑ j:t(j)−T⩽t⩽t(j) bj can happen. To see this, note that upon arrival of a trip j between time t and t+ T, we have ∑t(j) t=t(j)−T xt ⩾ ∑t(j) t′=txt′ = ∑t+T t′=txt′. Denote by Sthe value of the sum ∑t+T t′=txt′. Note that for a big update, we have that the value of the sum Sis increased to at least S· ( 1 + 1−β B ) + 1−β B · 1 e(λ)−1 . Similarly for a small update the new value of the sum is at least S· ( 1 + 1−β B ) + 1−β B · 1 e(1/λ)−1 . Hence we can apply directly Lemma 20 with d= B 1−β to conclude that once b+ λs⩾ B 1−β, we have that S ⩾ 1. Since for a big update, the sum ∑ j:t(j)−T⩽t⩽t(j) bj increases by 1 −βand by λ(1 −β) for a small update we can see that the ﬁrst time the constraint ∑ j:t(j)−T⩽t⩽t(j) bj ⩽ B is violated, we have S ⩾ 1. Now since each update in the sum ∑ j:t(j)−T⩽t⩽t(j) bj is of value at most 1 −β we can conclude that at the end of the algorithm, we have ∑ j:t(j)−T⩽t⩽t(j) bj ⩽ B+ 1 −β hence the conclusion. 18We then prove robustness of Algorithm 8 by the following lemma. Lemma 11(Robustness). For any λ∈(0,1] and any β ∈[0,1], PDLA for the Bahncard problem is (e(λ)−β)·(1+(1−β)/B) e(λ)−1 -robust. Proof. Algorithm 8 makes 3 possible types of updates. For a minimal update, we have∆P = ∆D= β. For a small update we have ∆P = (1 −β) ·   t(j)∑ t=t(j)−T xt + 1 e(1/λ) −1  + β· t(j)∑ t=t(j)−T xt + 1 − t(j)∑ t=t(j)−T xt = 1 + 1 −β e(1/λ) −1 = e(1/λ) −β e(1/λ) −1 and ∆D= λ(1 −β) + β = β(1 −λ) + λ hence the ratio is ∆P ∆D = 1 β(1 −λ) + λ ·e(1/λ) −β e(1/λ) −1 Similarly in the case of a big update we have ∆P = e(λ) −β e(λ) −1 and ∆D= 1 which gives a ratio of ∆P ∆D = e(λ) −β e(λ) −1 We can conclude by Lemma 19 (inequality (7)) that the ratio of primal cost increase vs dual cost increase is always bounded by ∆P ∆D ⩽ e(λ) −β e(λ) −1 Using Lemma 10 along with weak duality is enough to conclude that the cost of the fractional solution built by the algorithm is bounded as follows costPDLA(A,I,λ) ⩽ (e(λ) −β) ·(1 + (1−β)/B) e(λ) −1 ·OPT which ends the proof. For consistency, we analyze the algorithm’s cost in two parts. When the heuristic algorithmAbuys its ith Bahncard at some time ti, deﬁne the interval Ii = [ti,ti+ T] which represents the set of times during which this speciﬁc Bahncard is valid. This creates a family of intervals I1,...I k if Abuys kBahncards. Note that we can assume that all these intervals are disjoint since if the prediction A suggests to buy a new Bahncard before the previous one expires, it is always better to postpone this buy to the end of the validity of the current Bahncard. Lemma 12. Denote by (∆P)Ii the increase in the primal cost of Algorithm 8 during interval Ii and by cost(A)Ii what prediction Apays during this same interval Ii (including the buy of the Bahncard at the beginning of the interval Ii). Then, for all iwe have (∆P)Ii cost(A)Ii ⩽ ⌈ λ· B 1−β ⌉ B+ β· ⌈ λ· B 1−β ⌉·e(λ) −β e(λ) −1 Proof. Assume that mtrips are requested during this interval Ii. Then we ﬁrst have that cost(A)Ii = B+ βm(Abuys a Bahncard then pays a discounted price for every trip in the interval Ii). 19As for Algorithm 8, for each trip j, we are possibly in the ﬁrst two cases: either ∑t(j) t=t(j)−T xt ⩾ 1 in which case the increase in the primal is ∆P = βor in the second case in which case the increase in the primal is ∆P = (1 −β) ·   t(j)∑ t=t(j)−T xt + 1 e(λ) −1  + β· t(j)∑ t=t(j)−T xt + 1 − t(j)∑ t=t(j)−T xt = 1 + 1 −β e(λ) −1 We claim that the updates of the second case can happen at most ⌈ λ· B 1−β ⌉ times during interval Ii. To see this, denote by S(l) the value of ∑ t′⩾ti xt′ after lbig updates in interval Ii. Note that once ∑ t′⩾ti xt′ ⩾ 1, big updates cannot happen anymore. Hence all we need to prove is that S (⌈ λ· B 1−β ⌉) ⩾ 1. We prove by induction that S(k) ⩾ e(k·(1 −β)/B) −1 e(λ) −1 This is indeed true for k= 0 as S(0) is the value of ∑ t′⩾ti xt′ before any big update was made in Ii hence S(0) ⩾ 0. Now assume this is the case for some kand compute S(k+ 1) ⩾ ( 1 + 1 −β B ) ·S(k) + 1 −β B · 1 e(λ) −1 ⩾ ( 1 + 1−β B ) ·(e(k·(1 −β)/B) −1) + 1−β B e(λ) −1 ⩾ e((k+ 1) ·(1 −β)/B) −1 e(λ) −1 which concludes the induction. Hence on interval Ii, the total increase in the cost of the solution can be bounded as follows (∆P)Ii ⩽ min {⌈ λ· B 1 −β ⌉ ,m } · ( 1 + 1 −β e(λ) −1 ) + max { 0, ( m− ⌈ λ· B 1 −β ⌉)} ·β One can see that the worst case possible for the ratio (∆P)Ii cost(A)Ii is obtained for m= ⌈ λ· B 1−β ⌉ and is bounded by (∆P)Ii cost(A)Ii ⩽ ⌈ λ· B 1−β ⌉ · ( 1 + 1−β e(λ)−1 ) B+ β· ⌈ λ· B 1−β ⌉ = ⌈ λ· B 1−β ⌉ B+ β· ⌈ λ· B 1−β ⌉·e(λ) −β e(λ) −1 We then consider times tthat do not belong to any interval Ii. More precisely, we upper bound the value (∆P)j that is the increase in cost of the primal solution due to trip jsuch that t(j) does not belong to any interval Ii. Note that in this case the prediction always pays a cost of 1. Lemma 13. For any trip jsuch that t(j) /∈⋃ iIi, we have that (∆P)j ⩽ e(1/λ) −β e(1/λ) −1 Proof. Note that Algorithm 8 pays either the cost of a small update which is e(1/λ)−β e(1/λ)−1 or the cost of a minimal update which is β. 20For simplicity and better readability, we will formulate the ﬁnal theorem of this section only for B 1−β →∞. Theorem (Theorem 4 restated). For any λ ∈(0,1], any β ∈[0,1] and B 1−β − →∞, we have the following guarantees on any instance I costPDLA(A,I,λ) ⩽ min { λ 1 −β+ λβ ·eλ −β eλ −1 ·S(A,I),eλ −β eλ −1 ·OPT } Proof. By taking the limit in Lemma 11, we see that the cost of the solution output by Algorithm 8 is at most eλ−β eλ−1 ·OPT which proves the second bound in the theorem. For the ﬁrst bound, note that we can write the ﬁnal cost of the solution as costPDLA(A,I,λ) = ∆P = ∑ i (∆P)Ii + ∑ j:t(j)/∈⋃ iIi (∆P)j By taking the limit in Lemma 12 we get that ∑ i (∆P)Ii ⩽ λ 1 −β+ βλ ·eλ −β eλ −1 · ∑ i cost(A)Ii and by taking the limit in Lemma 13, we get that ∑ j:t(j)/∈⋃ iIi (∆P)j ⩽ e1/λ −β e1/λ −1 · ∑ j:t(j)/∈⋃ iIi cost(A)j By using Lemma 19 (inequality (6)), we see that max { λ 1 −β+ βλ ·eλ −β eλ −1 ,e1/λ −β e1/λ −1 } = λ 1 −β+ βλ ·eλ −β eλ −1 which ends the proof. We ﬁnish this section by proving that a fractional solution can be rounded online into a randomized integral solution. The expected cost of the rounded instance will be equal to the cost of the fractional solution. If the rounding is very similar to the existing rounding of Buchbinder et al. [5] for ski rental or TCP acknowledgement, we still include it here for completeness as the Bahncard problem was never solved in a primal-dual way. The argument is summarized in the following lemma. Lemma 14. Given a fractional solution (x,d,f ) to the Bahncard problem, it can be rounded online into an integral solution of expected cost equal to the fractional cost of (x,d,f ). Proof. Choose some real number puniformly at random in the interval [0,1]. Then arrange the variables xt on the real line (i.e. iteratively as follows, each time ttakes an interval It of length xt right after the interval taken by xt−1). Then buy a Bahncard at every time tsuch that the interval corresponding to time tcontains the real number p+ kfor some integer k. We check ﬁrst that the expected buying cost is B· ∑ t E (1p+k∈It) = B· ∑ t xt Next, to compute the total expected price of the tickets, notice that if a ticket was bought in the previous T time steps, we can pay a discounted price, otherwise we need to pay the full price of 1. For a trip j, the probability that a ticket was bought in the previous T time steps is at least∑t(j) t=t(j)−T xt. Hence with probability at least ∑t(j) t=t(j)−T xt ⩾ dj we pay a price of β and with probability 1 −dj ⩽ fj we pay a price of 1 which ends the proof. 21C Missing proofs for TCP C.1 Plots of instances We brieﬂy show in Figures 6, 7, and 8 how typical instances under various distributions look like. 0 200 400 600 800 1000 time 0 1 2 3 4 5 6number of requests Figure 6: Typical instance under Poisson distribution 0 200 400 600 800 1000 time 0 10 20 30 40number of requests Figure 7: Typical instance under Pareto distribution 0 200 400 600 800 1000 time 0 5 10 15 20 25 30number of requests Figure 8: Typical instance under iterated Poisson distribution 22C.2 Theoretical analysis Recall that we deﬁne in this section e(z) = (1 + 1/d)z·d which will be roughly equal to ez for big d. The big updates are then the updates where zis set to λand during a small update, zis set to 1/λ. We ﬁrst analyze the consistency of this algorithm. To this end denote by nAthe number of acknowl- edgements sent by Aand by latency (A) the latency paid by the prediction A. Lemma 15. For any λ∈(0,1], d> 0, cPDLA(A,I,λ) ⩽ nA·1 d · ⌈λd⌉ 1 −e(−λ) + latency(A) · 1 1 −e(−1/λ) Proof. We will use a charging argument to analyze the performance of Algorithm 6. Note that for a small update, the increase in cost of the fractional solution is ∆P = 1 d  1 − t∑ k=t(j) xk  + 1 d ·   t∑ k=t(j) xk + 1 e(1/λ) −1  = 1 d · 1 1 −e(−1/λ) However, for every small update that is made, it must be thatApays a latency of at least 1 d. Hence the total cost of small updates made by Algorithm 6 is at most latency(A) · 1 1−e(−1/λ) . Secondly we bound the total cost of big updates of our algorithm. Let t0 be a time at which Asends an acknowledgment. Let Y be the set of big updates made because of jobs jthat are acknowledged at time t0 by A(these big updates are hence made at some time t⩾ t0). We claim that |Y|⩽ ⌈λd⌉. To prove this denote by S(l) the value of ∑+∞ k=t0 xk after lsuch big updates (there might be small updates inﬂuencing this value but only to make it bigger). Notice that once ∑+∞ k=t0 xk ⩾ 1 there is no remaining update in Y. We prove by induction that S(l) ⩾ (1 + 1/d)l −1 (1 + 1/d)λd −1 This is clear for l = 0 as S(0) ⩾ 0. Now assume this is the case for some value land apply a big update at time tfor job jto get S(l+ 1) = S(l) + 1 d ·   t∑ k=t(j) xk + 1 e(λ) −1   ⩾ S(l) ·(1 + 1/d) + 1 d(e(λ) −1) = (1 + 1/d)l+1 −1 −1/d (1 + 1/d)λd −1 + 1/d (e(λ) −1) = (1 + 1/d)l+1 −1 −1/d (1 + 1/d)λd −1 + 1/d (1 + 1/d)λd −1 = (1 + 1/d)l+1 −1 (1 + 1/d)λd −1 Where the second inequality comes from noting that since we are considering an update due to a request j acknowledged at time t0 by the predicted solution, it must be that t(j) ⩽ t0 and∑t k=t(j) xk ⩾ ∑t k=t0 xk. Hence we get that S(⌈λd⌉) ⩾ 1 which implies that |Y|⩽ ⌈λd⌉. By a similar calculation as for the small update case, we have that the cost of a big update is ∆P = 1 d · 1 1 −e(−λ) 23Hence the total cost of these updates in Y is charged to the acknowledgement that Apays at time t0 to ﬁnish the proof. Taking the limit d→+∞we get the following corollary: Corollary 16. For any λ∈(0,1] and taking d→+∞, we have that cPDLA(A,I,λ) ⩽ nA· λ 1 −e−λ + latency(A) · 1 1 −e−1/λ We then prove robustness of the algorithm with the following lemmas. Lemma 17. Let ybe the dual solution produced by Algorithm 6. Then y 1+1/d is feasible. Proof. Notice that the constraints of the second type (i.e. 0 ⩽ yjt ⩽ 1/d) are always satisﬁed since 0 < λ⩽ 1. We now check that the second constraints are almost satisﬁed (within some factor (1 + 1/d)). Fix a time t∈T and consider the corresponding constraint: ∑ j|t⩾t(j) ∑ t′⩾t yjt ⩽ 1 Note that for a small update for some job jsuch that t(j) ⩽ tthe sum above increases by λ/dwhile it increases by 1/dfor a big update. Notice that once we have that ∑ t′⩾txt′ ⩾ 1, no more such update will be performed. Denote by Sthe value of this sum. Notice that for a big update, the sum Sbecomes ( 1 + 1 d ) ·S+ 1 d((1+1/d)λd−1) . Similarly, for a small updates it becomes ( 1 + 1 d ) ·S+ 1 d((1+1/d)d/λ−1) . Hence, if we denote by sthe number of small updates in this sum and by bthe number of big updates, by Lemma 20 we have that if λs+ b⩾ dthen ∑ t′⩾txt′ ⩾ 1. This directly implies that the value of ∑ j|t⩾t(j) ∑ t′⩾tyjt is at most 1 + 1/dat the end of the algorithm (each update in the dual is of value at most 1/d). Therefore scaling down all yjt by a multiplicative factor of 1 + 1/dyields a feasible solution to the dual. Lemma 18. For d→+∞, Algorithm 6 outputs a solution of cost at most 1 1−e−λ ·OPT Proof. We ﬁrst compare the increase ∆P in the primal value to the increase ∆Din the dual value at every update. We claim that for every update we have ∆P ∆D ⩽ 1 1 −e(−λ) In the case of a big update we directly have ∆P = 1 d ( 1 + 1 e(λ)−1 ) = 1 d · 1 1−e(−λ) and ∆D = 1 d. In the case of a small update we have ∆D= λ d and ∆P = 1 d ( 1 + 1 e(1/λ)−1 ) = 1 d · 1 1−e(−1/λ) and we conclude applying Lemma 19 (inequality (3)) that we always have ∆P ∆D ⩽ 1 1 −e(−λ) By lemma 17, y 1+1/d is a feasible solution. Hence taking d→+∞together with the previous remark and weak duality we get the result. Combining Lemma 16 and Lemma 18 yields Theorem 5. 24D Optimality bound Lemma 3.Any λ 1−e−λ-consistent learning augmented algorithm for ski rental has robustnessR(λ) ⩾ 1 1−e−λ Proof. For simplicity, we will consider the ski-rental problem in the continuous case which corre- sponds to the behaviour of the discrete version when B − →∞. In this problem, the cost of buying is 1 and a randomized algorithm has to deﬁne a (buying) probability distribution {pt}t⩾0. Moreover, consider the case where the true number of vacation daystend ∈[0,1] ∪(2,∞). In such a case we can assume w.l.o.g. that pt = 0,∀t> 1. Indeed moving buying probability mass from any pt,t> 1 to p1 does not increase the cost of the randomized algorithm. Assume now that the prediction suggests us that the end of vacations is at ˆtend > 2, thus the optimal ofﬂine solution, if the prediction is correct, is to buy the skis in the beginning for a total cost of 1. Since the algorithm has to deﬁne a probability distribution in [0,1], {pt}needs to satisfy the equality constraint ∫1 0 ptdt= 1. Moreover, note that when the prediction is correct, i.e. tend >2, the LA algorithm suffers an expected cost of∫1 0 (t+ 1)ptdtwhile the optimum ofﬂine has a cost of 1. Thus the consistency requirement forces the distribution to satisfy the inequality ∫1 0 (t+ 1)ptdt⩽ λ 1−e−λ. Now assume that the best possible LA algorithm is c-robust. If tend ⩽ 1 then the LA algorithm’s cost is ∫tend 0 (t+ 1)ptdt+ tend ∫1 tend ptdt while the optimum ofﬂine cost is tend. Thus, due to c-robustness we have that for every t′∈[0,1],∫t′ 0 (t+ 1)ptdt+ t′∫1 t′ptdt⩽ ct′. We calculate the best possible robustness cwith the following LP: Figure 9: Primal Robustness for ski-rental problem. Primal minimize c subject to: ∫1 0 ptdt= 1∫1 0 (t+ 1)ptdt⩽ λ 1−e−λ ∫t′ 0 (t+ 1)ptdt+ t′∫1 t′ptdt⩽ ct′ ∀t′∈[0,1] pt ⩾ 0 ∀t′∈[0,1] To lower bound the best possible robustnesscwe will present a feasible solution to the dual of 9. The dual variables λd and λc correspond respectively to the ﬁrst and second primal constraints in Figure 9. The dual variables λt, ∀t∈[0,1] correspond to the robustness constraints described in the third line of the primal. The corresponding dual is: Figure 10: Dual Robustness for ski-rental problem. Dual maximize λd −λc · λ 1−e−λ subject to: ∫1 0 tλtdt⩽ 1 λd −(t′+ 1)λc ⩽ ∫t′ 0 tλtdt+ (t′+ 1) ∫1 t′λtdt ∀t′∈[0,1] λc,λt ⩾ 0 ∀t∈[0,1] Let K = 1 1−λe−λ−e−λ. Then, λt = K·e−t ·1 {t⩽ λ}, λd = Kand λc = K·e−λ. We ﬁrst prove that this dual solution is feasible. For the ﬁrst constraint notice that ∫ 1 0 tλtdt= K· ∫ λ 0 te−tdt= K· ( 1 −(λ+ 1)e−λ) = 1 For the second type of constraint ﬁrst in the case t′>λ we get ∫ t′ 0 tλtdt+ (t′+ 1) ∫ 1 t′ λtdt= ∫ λ 0 tλtdt= 1 25and we note that λd −(t′+ 1)λc ⩽ λd −(λ+ 1)λc = K· ( 1 −(λ+ 1)e−λ) = 1 hence these constraints are satisﬁed. In the second case t′⩽ λ, we have that ∫ t′ 0 tλtdt+ (t′+ 1) ∫ 1 t′ λtdt= K· (∫ t′ 0 te−tdt+ (t′+ 1) ∫ λ t′ e−tdt ) = K· ( 1 −(t′+ 1)e−t′ + (t′+ 1)(e−t′ −e−λ) ) = K· ( 1 −(t′+ 1)e−λ) = λd −(t′+ 1)λc which proves that these constraints are also satisﬁed. Hence this dual solution is feasible. Finally note that the cost of this dual solution is λd −λc · λ 1 −e−λ = K· ( 1 − λ 1 −e−λ ·e−λ ) = K·1 −e−λ −λe−λ 1 −e−λ = 1 1 −e−λ By weak duality, we conclude that the best robustness cannot be better than 1 1−e−λ E Technical lemmas A few inequalities that will be useful: Lemma 19. For any d> 0, any 0 <λ ⩽ 1, and any β ∈[0,1], we have: λ 1 −e−λ ⩾ 1 1 −e−1/λ (2) λ 1 −(1 + 1/d)−λd ⩾ 1 1 −(1 + 1/d)−d/λ (3) 1 eλ −1 ⩾ 1−λ λ ·e1/λ + 1 e1/λ −1 (4) 1 (1 + 1/d)λd −1 ⩾ 1−λ λ ·(1 + 1/d)d/λ + 1 (1 + 1/d)d/λ −1 (5) λ 1 −β+ βλ ·eλ −β eλ −1 ⩾ e1/λ −β e1/λ −1 (6) (λ+ β−βλ) ·eλ −β eλ −1 ⩾ e1/λ −β e1/λ −1 (7) Proof. Since the formal proof of (2) and (4) seems to require heavy calculations and that they are easy to check on computer we will only give a proof by a plot (see Figures 11a and 11b). For 11b, note that (4) ⇐⇒ 1 eλ−1 −1−λ λ −1 λ · e−1/λ 1−e−1/λ ⩾ 0. 260.0 0.2 0.4 0.6 0.8 1.0 lambda 0.00 0.02 0.04 0.06 0.08 0.10 0.12 (a) Plot of λ 1−e−λ − 1 1−e−1/λ 0.0 0.2 0.4 0.6 0.8 1.0 lambda 0.0 0.1 0.2 0.3 0.4 0.5 (b) Plot of 1 eλ−1 −1−λ λ −1 λ · e−1/λ 1−e−1/λ Figure 11: Plots for (2) and (4) We now prove that inequality (2) implies inequality (3). For this end notice that we can write (1 + 1/d)d = ex for some x∈(0,1) since (1 + 1/d)d ∈(1,e) for all d> 0. We prove that for any x∈(0,1] λ ( 1 −e−x/λ) 1 −e−xλ ⩾ λ ( 1 −e−1/λ) 1 −e−λ which will imply our claim since by inequality (2) the right hand side is bigger than 1. First note this is equivalent to prove that gλ(x) = (1 −e−λ) ·(1 −e−x/λ) −(1 −e−1/λ) ·(1 −e−xλ) ⩾ 0 Taking the derivative ofgλ(x) we obtain g′ λ(x) = 1 −e−λ λ ·e−x/λ −λ(1 −e−1/λ) ·e−xλ hence we can write g′ λ(x) ⩾ 0 ⇐⇒ex(λ−1/λ) ⩾ λ2 ·1 −e−1/λ 1 −e−λ Notice that the left hand side in this inequality is decreasing because λ ∈(0,1]. Also notice that gλ(0) = gλ(1) = 0 . These two facts together imply that gλ is ﬁrst increasing for x ∈(0,c] then decreasing for x∈(c,1] for some unknown c. In particular, we indeed have that gλ(x) ⩾ 0 which ends the proof of inequality (3). Similarly, we prove that inequality (4) implies inequality (5). Again we write (1 + 1/d)d = ex for some x∈(0,1). We ﬁrst rewrite inequality (5). (5) ⇐⇒ 1 eλx −1 ⩾ 1−λ λ ·ex/λ + 1 ex/λ −1 ⇐⇒ 1 eλx −1 ⩾ 1−λ λ ·(ex/λ −1) + 1 λ ex/λ −1 ⇐⇒λ(ex/λ −1) ⩾ (1 −λ)(ex/λ −1)(eλx −1) + (eλx −1) ⇐⇒λ(ex/λ −1) −(1 −λ)(ex/λ −1)(eλx −1) −(eλx −1) ⩾ 0 Deﬁne the following function hλ(x) = λ(ex/λ−1) −(1 −λ)(ex/λ−1)(eλx−1) −(eλx−1). One can ﬁrst compute: 27h′ λ(x) = ex/λ −(1 −λ) · ( λeλx(ex/λ −1) + 1 λex/λ(eλx −1) ) −λeλx = ex/λ −λeλx −(1 −λ) · ( (λ+ 1/λ)ex(λ+1/λ) −λeλx −1 λex/λ ) = ex/λ · ( 1 + 1 −λ λ ) + eλx ·(−λ+ λ(1 −λ)) −ex(λ+1/λ) ·(1 −λ) · ( λ+ 1 λ ) = ex/λ λ −λ2eλx −ex(λ+1/λ) λ ·(1 −λ) ·(λ2 + 1) Hence we can rewrite h′ λ(x) ⩾ 0 ⇐⇒ ex/λ λ −λ2eλx −ex(λ+1/λ) λ ·(1 −λ) ·(λ2 + 1) ⩾ 0 ⇐⇒ex/λ −λ3eλx −ex(λ+1/λ) ·(1 −λ) ·(λ2 + 1) ⩾ 0 ⇐⇒1 −λ3ex(λ−1/λ) −exλ ·(1 −λ) ·(λ2 + 1) ⩾ 0 Let us deﬁne iλ(x) = 1 −λ3ex(λ−1/λ) −exλ ·(1 −λ) ·(λ2 + 1) and we derive i′ λ(x) = −λ3 ·(λ−1/λ) ·ex(λ−1/λ) −λeλx ·(1 −λ) ·(λ2 + 1) We can now notice that i′ λ(x) ⩾ 0 ⇐⇒ −λ3 ·(λ−1/λ) ·ex(λ−1/λ) −λeλx ·(1 −λ) ·(λ2 + 1) ⩾ 0 ⇐⇒ −λ3 ·(λ−1/λ) ·e−x/λ −λ(1 −λ) ·(λ2 + 1) ⩾ 0 ⇐⇒λ3 ·(1/λ−λ) ·e−x/λ −λ(1 −λ) ·(λ2 + 1) ⩾ 0 Since the left hand side is decreasing as xincreases we only need to check one extreme value which is i′ λ(0). We write i′ λ(0) ⩽ 0 ⇐⇒λ3 ·(1/λ−λ) −λ·(1 −λ) ·(λ2 + 1) ⩽ 0 ⇐⇒λ2 −λ4 −(λ3 + λ−λ4 −λ2) ⩽ 0 ⇐⇒ −λ3 + 2λ2 −λ⩽ 0 ⇐⇒ −λ·(λ−1)2 ⩽ 0 hence we always have i′ λ(0) ⩽ 0. Therefore we get that i′ λ(x) ⩽ 0 for all xand λ. Note that iλ(0) = 1 −λ3 −(1 −λ)(λ2 + 1) = 1 −λ3 −λ2 −1 + λ3 + λ= λ−λ2 ⩾ 0. Therefore we get that hλ is ﬁrst positive on some interval [0,c] and then negative for x∈[c,∞). Therefore hλ is ﬁrst increasing then decreasing. Notice that hλ(0) = 0 and hλ(1) ⩾ 0 by inequality (4). Hence inequality (5) is true for all x ∈[0,1] which concludes the proof. Finally, the proof of (6) and (7) are quicker and similar. Note that (6) ⇐⇒λ·eλ −β eλ −1 ⩾ (1 −β+ βλ) ·e1/λ −β e1/λ −1 which is equivalent to a polynomial (in β) of degree 2 being positive. The leading coefﬁcient of this polynomial P is negative and we notice that P(1) = 0 and that P(0) ⩾ 0 by (2). All these facts together imply that P(β) ⩾ 0 for all β ∈[0,1]. The proof of (7) is similar. Lemma 20. Let 0 <λ ⩽ 1, d> 0 and deﬁne the following functions (x∈R): f(x) = ( 1 + 1 d ) ·x+ 1 d((1 + 1/d)λd −1) 28g(x) = ( 1 + 1 d ) ·x+ 1 d ( (1 + 1/d)d/λ −1 ) Given S ⩾ 0 and a word w∈{a,b}∗we deﬁne a sequence Sw recursively as follows: Sw.y = { Sif w.y = ε f(Sw) if y= a g(Sw) if y= b Then for any w∈{a,b}∗such that |w|a + λ|w|b ⩾ dwe have that Sw ⩾ 1. Proof. Let w′ = b...ba...a = b|w|ba|w|a be the word made of |w|b consecutive bs followed by |w|a consecutive as. Then we claim that Sw′ ⩽ Sw. This directly follows from the fact that for any real number x, f(g(x)) ⩽ g(f(x)). Noticing this, we can swap positions between an afollowed by a band reducing the ﬁnal value. We keep doing this until all the bs in wend up in front position. With standard computations one can check that Sb|w|b = S·(1 + 1/d)|w|b + (1 + 1/d)|w|b −1 (1 + 1/d)d/λ −1 For ease of notation deﬁne S′= Sb|w|b. Using the assumption that |w|a + λ|w|b ⩾ dand that S ⩾ 0 we get that S′⩾ (1 + 1/d)(d−|w|a)/λ −1 (1 + 1/d)d/λ −1 Again using standard calculations we get that Sw′ ⩾ S′·(1 + 1/d)|w|a + (1 + 1/d)|w|a −1 (1 + 1/d)λd −1 which implies Sw′ ⩾ (1 + 1/d)(d−|w|a)/λ −1 (1 + 1/d)d/λ −1 ·(1 + 1/d)|w|a + (1 + 1/d)|w|a −1 (1 + 1/d)λd −1 Deﬁne h(x) = (1+1/d)(d−x)/λ−1 (1+1/d)d/λ−1 ·(1 + 1/d)x + (1+1/d)x−1 (1+1/d)λd−1 . We ﬁnish the proof by proving that for any 0 <λ ⩽ 1, any d> 0 and any x⩾ 0, we have that h(x) ⩾ 1. Note that h(0) = 1 and that h′(x) = ln(1 + 1/d) · ( (1 + 1/d)x (1 + 1/d)λd −1 −1 −λ λ ·(1 + 1/d)(d−(1−λ)x)/λ (1 + 1/d)d/λ −1 − (1 + 1/d)x (1 + 1/d)d/λ −1 ) To study the sign of h′(x) we can drop the ln(1 + 1/d) and write h′(x) ⩾ 0 ⇐⇒ (1 + 1/d)x (1 + 1/d)λd −1 −1 −λ λ ·(1 + 1/d)(d−(1−λ)x)/λ (1 + 1/d)d/λ −1 − (1 + 1/d)x (1 + 1/d)d/λ −1 ⩾ 0 ⇐⇒ 1 (1 + 1/d)λd −1 −1 −λ λ ·(1 + 1/d)(d−x)/λ (1 + 1/d)d/λ −1 − 1 (1 + 1/d)d/λ −1 ⩾ 0 Clearly the last term is increasing as xincreases hence we can limit ourselves to prove thath′(0) ⩾ 0 which we can rewrite 29h′(0) ⩾ 0 ⇐⇒ 1 (1 + 1/d)λd −1 −1 −λ λ · (1 + 1/d)d/λ (1 + 1/d)d/λ −1 − 1 (1 + 1/d)d/λ −1 ⩾ 0 ⇐⇒ 1 (1 + 1/d)λd −1 ⩾ 1−λ λ ·(1 + 1/d)d/λ + 1 (1 + 1/d)d/λ −1 Which holds by equation (5) of Lemma 19. 30",
      "references": [
        "The online set cover problem",
        "Online metric algorithms with untrusted predictions",
        "A primal-dual randomized algorithm for weighted paging",
        "The design of competitive online algorithms via a pri- mal: Dual approach",
        "Online primal-dual algorithms for maximizing ad-auctions revenue",
        "Friendship and mobility: User movement in location-based social networks",
        "Learning space partitions for nearest neighbor search",
        "Tcp dynamic acknowledgment delay (extended abstract) theory and practice",
        "On the bahncard problem",
        "Online algorithms for rent-or-buy with expert advice",
        "On the tails of web ﬁle size distributions",
        "Learning-based frequency estimation algorithms",
        "Competitive snoopy caching",
        "Competitive randomized algorithms for non-uniform problems",
        "Dynamic tcp acknowledgement and other stories about e/(e-1)",
        "Optimal algorithms for ski rental with soft machine-learned predictions",
        "Online scheduling via learned weights",
        "Learning-assisted competitive algorithms for peak-aware energy scheduling",
        "Competitive caching with machine learned advice",
        "Predicted capacity of ethernet in a university environment",
        "Revenue optimization with approxi- mate bid predictions",
        "Dynamic models for ﬁle sizes and double pareto distributions",
        "A model for learned bloom ﬁlters and optimizing by sand- wiching",
        "Scheduling with Predictions and the Price of Misprediction",
        "Improving online algorithms via ML predictions",
        "Near-optimal bounds for online caching with machine learned advice",
        "A guessing game and randomized online algorithms",
        "Online Algorithms for Multi-shop Ski Rental with Machine Learned Predictions",
        "Lomax distribution",
        "A historical view of network trafﬁc models",
        "Competitive algorithms for online leasing problem in probabilis- tic environments"
      ],
      "meta_data": {
        "arxiv_id": "2010.11632v1",
        "authors": [
          "Étienne Bamas",
          "Andreas Maggiori",
          "Ola Svensson"
        ],
        "published_date": "2020-10-22T11:58:47Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper extends the classic primal‐dual method for online algorithms by incorporating machine-learned predictions into the decision-making process. It develops a general PDLA (Primal-Dual Learning Augmented) framework that yields learning augmented algorithms for a range of online covering problems, including weighted set cover, ski rental, the Bahncard problem, and TCP acknowledgment. The work demonstrates that these algorithms achieve a favorable consistency-robustness trade-off, attaining near-optimal performance when predictions are accurate while ensuring robust guarantees under worst-case conditions.",
        "methodology": "The approach is based on a modification of the classical primal-dual method by adjusting the rate at which primal variables are updated, using a robustness parameter λ to control trust in the provided prediction. For each problem, the algorithm builds an online fractional solution to the primal and its associated dual, and then rounds the fractional solution into an integral one via an online rounding scheme. The paper provides detailed analyses and proof sketches for each application, showing that the modified algorithm retains good competitive ratios by bounding the increases in cost relative to the dual updates.",
        "experimental_setup": "The experimental evaluation focuses on the TCP acknowledgment problem. The experiments use simulated packet arrival instances generated from several distributions – including Poisson, Pareto (Lomax), and an iterated Poisson distribution – to mimic real-world network traffic with heavy-tailed behavior. Additionally, the study incorporates noisy predictions by perturbing the input with a replacement rate. The performance of the PDLA algorithm is then measured in terms of competitive ratios over multiple runs, comparing different settings of the robustness parameter λ and verifying the smooth degradation of performance as prediction errors increase.",
        "limitations": "The approach assumes that a prediction is available and, in some cases, that the prediction forms a feasible solution (or can be augmented to handle infeasibility). The theoretical guarantees rely on specific assumptions related to the update rates controlled by the parameter λ, which means that performance is sensitive to the choice of λ. Furthermore, the extension to other classes of online problems may require additional adaptations, particularly if the underlying LP formulations have large integrality gaps or non-linear objectives, potentially complicating the rounding scheme and overall analysis.",
        "future_research_directions": "Potential avenues include applying the PDLA framework to a wider range of covering problems such as weighted caching, load balancing, and other network optimization settings. Another direction is to extend the approach to problems formulated as packing or those with convex objective functions. Moreover, further research could explore incorporating predictions in other algorithmic paradigms and refining the trade-off guarantees between consistency and robustness, especially in scenarios where predictions are highly inaccurate or dynamically changing.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Novel Object Synthesis via Adaptive Text-Image Harmony",
      "full_text": "Novel Object Synthesis via Adaptive Text-Image Harmony Zeren Xiong1, Zedong Zhang1, Zikun Chen1, Shuo Chen2, Xiang Li3, Gan Sun4, Jian Yang1, Jun Li1∗ 1School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, 210094, China 2RIKEN, 3College of Computer Science, Nankai University, Tianjing, 300350, China 4College of Automation Science and Engineering, South China University of Technology, Guangzhou, 510640, China {zandyz,csjyang,junli}@njust.edu.cn, {xzr3312,zikunchencs,sungan1412}@gmail.com xiang.li.implus@nankai.edu.cn, shuo.chen.ya@riken.jp porcupine king penguin butternut squash bald eagle  Figure 1: We propose a straightforward yet powerful approach to generate combinational objects from a given object text-image pair for novel object synthesis. Our algorithm produces these combined object images using the central image and its surrounding text inputs, such as glass jar (image) and porcupine (text) in the left picture, and horse (image) and bald eagle (text) in the right picture. Abstract In this paper, we study an object synthesis task that combines an object text with an object image to create a new object image. However, most diffusion models struggle with this task, i.e., often generating an object that predominantly reflects either the text or the image due to an imbalance between their inputs. To address this issue, we propose a simple yet effective method called Adaptive Text-Image Harmony (ATIH) to generate novel and surprising objects. First, we introduce a scale factor and an injection step to balance text and image features in cross- attention and to preserve image information in self-attention during the text-image inversion diffusion process, respectively. Second, to better integrate object text and image, we design a balanced loss function with a noise parameter, ensuring both ∗Corresponding author 38th Conference on Neural Information Processing Systems (NeurIPS 2024). arXiv:2410.20823v1  [cs.CV]  28 Oct 2024SDXL Turbo (Img2Img)  Ours Text PnPInv (inject 1step) Ours Toucan ImageText Terrier Iron White Shark Text ImageText Image Image Figure 2: Imbalances between text and image in diffusion models. Using SDXL-Turbo [56] (left) and PnPinv [27] (right), the top pictures show a tendency for generated objects to align with textual content (green circles), while the bottom pictures tend to align with visual aspects (orange circles). In contrast, our approach achieves a more harmonious integration of both object text and image. optimal editability and fidelity of the object image. Third, to adaptively adjust these parameters, we present a novel similarity score function that not only maximizes the similarities between the generated object image and the input text/image but also balances these similarities to harmonize text and image integration. Extensive experiments demonstrate the effectiveness of our approach, showcasing remarkable object creations such as colobus-glass jar in Fig. 1. Project Page. 1 Introduction Image synthesis from text or/and image using diffusion models such as Stable Diffusion [ 51], SDXL [56], and DALL·E3 [43] has gained considerable attention due to their impressive generative capabilities and practical applications, including editing [6; 75] and inversion [24; 61]. Many of these methods focus on object-centric diffusion, utilizing textual descriptions to manipulate objects within images through operations like composition [58], addition [36; 15], removal [63], replacement [7], movement [29], and adjustments in size, shape, action, and pose [17]. In contrast, we study an object synthesis task that creates a new object image by combining an object text with an object image. For instance, combining kingfisher (image) and terrier (text) results in a new and harmonious terrier-like kingfisher object, as shown in the right-side of Fig. 2. To implement object text-image fusion, most diffusion models, such as SDXL-Turbo [56], often use cross-attention [24] to integrate the input text and image. However, the cross-attention frequently results in imbalanced outcomes, as evidenced by the following observations. On the left side of Fig. 2, when inputting an axolotl (image) and a toucan (text), SDXL-Turbo only generates an image of a toucan, showing a bias towards the toucan text (green circles). Conversely, when inputting a rooster (image) and an iron (text), it produces an image of a rooster, which closely resembles the original rooster image (orange circles). These observations reveal that the text (or image) feature often suppresses the influence of the image (or text) feature during the diffusion process, leading to a failed fusion. To mitigate the image degeneration, Plug-and-Play [61] can inject the guidance image features into self-attention. Unfortunately, even with the application of the best inversion editing method, PnPinv [27], which incorporates the plug-and-play inversion into diffusion-based editing methods for improved performance, we still observe similar imbalances, as shown on the right-side of Fig. 2. This arises an important problem: how can we balance object text and image integration? To address this problem, we propose an Adaptive Text-Image Harmony (ATIH) method for novel object synthesis, as shown in Fig. 3. First, during the inversion diffusion process, we introduce a scale factor α to balance text and image features in cross-attention, and an injection step i to preserve image information in self-attention for adaptive adjustment. Second, the inverted noise maps adhere to the statistical properties of uncorrelated Gaussian white noise, which increases editability [ 46]. However, they are preferable for approximating the feed-forward noise maps, thereby enhancing fidelity. To better integrate object text and image, we treat sampling noise as a parameter in designing 2a balanced loss function, which strikes a balance between reconstruction and Gaussian white noise approximation, ensuring both optimal editability and fidelity of the object image. Third, we present a novel similarity loss that considers bothi and α. This loss function not only maximizes the similarities between the generated object image and the input text/image but also balances these two similarities to harmonize text and image integration. Furthermore, we employ the Golden Section Search [47] algorithm to quickly find the optimal parameters α and i. Therefore, our ATIH method is capable of generating novel object combinations. For instance, an iron-like roosteris produced by merging the image rooster with the text iron, resulting in a rooster image with an iron texture, as shown in Fig. 2. Overall, our contributions can be summarized as follows: (1) To the best of our knowledge, we are the first to propose an adaptive text-image harmony method for generating novel object synthesis. The key idea is to achieve a balanced blend of object text and image by adaptively adjusting a scale factor and an injection step in the inversion diffusion process, ensuring their effective harmony. (2) We introduce a novel similarity score function that incorporates the scale factor and injection step. This aims to balance and maximize the similarities between the generated image and the input text/image, achieving a harmonious integration of text and image. (3) Experimental results on PIE-bench [26] and ImageNet [53] demonstrate the effectiveness of our method. Our approach shows superior performance in creative object combination compared to state-of-the-art image-editing and creative mixing methods. Examples of these creative objects, such as sea lion-glass jar, African chameleon-bird, and mud turtle-car are shown in Figs. 1, 6, and 8. 2 Related Work Text-to-Image Generation The rapid development of generative models based on diffusion processes has advanced the state-of-the-art for tasks [12; 21; 33] like text-to-image synthesis [22; 31], image editing [64; 2], and style transfer [ 65; 23; 35]. Large-scale models such as Stable Diffusion [ 51], Imagen [55], and DALL-E [49] have demonstrated remarkable capabilities. Sdxlturbo [56] introduced a distillation method that further enhances efficiency by reducing the steps needed for high-quality image generation. Our method utilizes Sdxlturbo for adaptive and innovative object fusion, preserving the original image’s layout and details while requiring only the textual description of the target object. Text Guided Image Editing. Diffusion models have garnered significant attention for their success in text-to-image generation and text-driven image editing using natural language descriptions. Early studies [1; 54; 70; 40], such as SDEdit [40], balanced authenticity and fidelity by adding noise, while Prompt2Prompt [24] and Plug-and-Play (PNP) [61] enhanced editing through attention mechanisms. Further research, including MasaCtrl [5], Instructpix2pix [4], and InfEdit [69], explored non-rigid editing, specialized image editing models, and rapid editing via consistency sampling. Advances in image inversion and reconstruction [20] have focused on diffusion-based denoising process inversion, categorized into deterministic and non-deterministic sampling [28]. Deterministic methods, such as Null-text inversion using DDIM sampling [41], precisely recover original images but require lengthy optimization; non-deterministic methods, such as DDPM inversion [25] and CycleDiffusion [67], achieve precision by storing variance noise. PnPinv [26] simplifies the process by accurately replacing latent features during denoising, achieving perfect reconstruction but with weaker editability.We propose a framework for creative object synthesis using object textual descriptions for effective fusion and a regularization technique to enhance PnPinv editability. Object Composition. Compositional Text-to-Image synthesis and multi-image subject blending methods [37; 19; 58; 70; 59] aim to create novel images by integrating various concepts, including object interactions, colors, shapes, and attributes. Numerous methodologies [ 8; 71; 24; 52; 55] have been developed focusing on object combinations, context integration, segmentation, and text descriptions. However, these methods often merely assemble components without effectively melding inter-object relationships, resulting in compositions that, while accurate, lack deeper integration and interaction. This limitation is particularly evident in image editing, where multiple objects in a single image fail to achieve cohesive synthesis. Our method addresses this by harmoniously fusing two objects to create novel entities, thereby enhancing creativity and imagination. Semantic Mixing. The breadth of creativity spans diverse fields, from scientific theories to culinary recipes, driving advancements in AI as highlighted by scholars [3][39] and recent researchers [62] [32]. This creativity has led to significant innovations in AI, particularly through generative models. Creative Adversarial Networks [16] push traditional art boundaries, producing norm-defying works 3Peacock 𝒛𝑻 Txt/Img Encoder 𝑻𝐬𝐢𝐦(𝜶,𝒊) 𝒛𝒕 Adaptive Text-Image Harmony Img Encoder Balance Factor 𝜶 InSelf-Attn Cross-Attn Self-Attn Inject ෡𝑴𝒕>𝒊 𝒔  Step 𝒊 Noise 𝝐𝒕 Decoder  Decoder Encoder Encoder Scale 𝜶𝑽𝒕 𝒄 ො𝒛𝒕 𝑳 ො𝒛𝒕,ො𝒛𝒕 ′,𝝐𝒕 𝑰𝐬𝐢𝐦(𝜶,𝒊) Rabbit (text) (Image) ScCross-Attn ො𝒛𝒕 ′ Peacock (text) Figure 3: Framework of our object synthesis incorporating a scale factor α, an injection step i and noise ϵt in the diffusion process. We design a balance loss for optimizing the noiseϵt to balance object editability and fidelity. Using the optimal noise ϵt, we introduce an adaptive harmony mechanism to adjust α and i, balancing text (Peacock) and image (Rabbit) similarities. while maintaining artistic connections. Efforts to adapt AI for novel engineering designs [11] further exemplify this technological creativity. MagicMix [ 34] introduced semantic mixing task,unlike traditional style transfer methods [73; 60; 10] which blending two concepts into a photo-realistic object while retaining the original image’s layout and geometry, but often resulting in biased images and less harmonious fusion. ConceptLab [ 50] uses diffusion models to generate unique concepts, like new types of pets, but requires time-consuming optimization and struggles to semantically blend real images. Our method operates at the attention layer of diffusion models for harmonious semantic fusion and proposes an adaptive fast search to quickly produce balanced, fused images, ensuring novel and cohesive integration of semantic concepts. 3 Methodology Let OI and OT be an object image and an object text, respectively, used as inputs for diffusion models. Our goal is to create a novel object image O by combining OI with OT during the diffusion process. To achieve this goal, we develop an adaptive text-image harmony (ATIH) method in our object synthesis framework, as shown in Fig. 3. In subsection 3.1, we introduce a text-image diffusion model with a scale factor α, an injection step i and noise ϵt. In subsection 3.2, we present to optimize the noise ϵt to balance object editability and fidelity. In subsection 3.3, we propose a simple yet effective ATIH method to adaptively adjustα and i for harmonizing text and image. 3.1 Text-Image Diffusion Model (TIDM) Here, we construct a Text-Image Diffusion Model (TIDM) by utilizing the pre-trained SDXL Turbo [56]. The key components include dual denoising branches: inversion for inverting the input object image, and fusion for fusing the object text and image. Following the latent diffusion model [51], the input latent codes are defined as z0 = E(OI) for object image OI and τ = E(OT ) for object text OT , using a pre-trained image/text encoder E(·). τN = E(ON ) denotes as a null-text embedding. The latent denoising process is described as follows: Inversion Denoising.The inversion denoising process predicts the latent code at the previous noise level, bzt−1, based on the current noisy data bzt. This process is defined as: bzt−1 = νtbzt + βtϵθ(bzt, t, τ) +γtϵt, (1) where νt, βt and γt are sampler parameters, ϵt is sampled noise, and ϵθ(bzt, t, τ) is a pre-trained U-Net model [56] with self-attention and cross-attention layers. The self-attention is implemented as: Self-Attn \u0010 bQs t , bKs t , bV s t \u0011 = cMs t · bV s t , cMs t = Softmax \u0010 bQs t ( bKs t )T / √ d \u0011 , (2) where bQs t , bKs t and bV s t are the query, key and value features derived from the representation bzt, and d is the dimension of projected keys and queries. The cross-attention is to control the synthesis process 4through the input null-text embedding τN , implemented as follows: Cross-Attn \u0010 bQc t, bKc t , bV c t \u0011 = cMc t · bV c t , where cMc t = Softmax \u0010 bQc t( bKc t )T / √ d \u0011 , bQc t is the query feature derived from the output of the self-attention layer, bKc t and bV c t are the key and value features derived from τN . Fusion Denoising.Similar to the inversion denoising branch, we redefine the self-attention and cross- attention for easily adjusting the balance between the image latent code zt and the text embedding τ. The fusion denoising process is redefined as: zt−1 = νtzt + βtϵθ(zt, t, τ, α, i) +γtϵt, (3) where νt, βt, γt and ϵt are defined as Eq. (1), and ϵθ(zt, t, τ, α, i) is also the pre-trained U-Net model [56] with injected self-attention and scale cross-attention layers. The injected self-attention with an adjustable injection step i(0 ≤ i ≤ T) is implemented as: InSelf-Attn (Ms t , Vs t ) =Ms t · V s t , M s t =    cMs t , if t > i Softmax \u0010 Qs t (Ks t )T / √ d \u0011 , otherwise , (4) where Qs t , Ks t and V s t are the query, key and value features derived from the representationzt. Unlike the approach of injecting bKs t and bV s t from Eq. (2) into Ks t and V s t in MasaCtrl [5], we focus on adjusting the injection step i by injecting cMs t from Eq. (2) into Ms t . The scale cross-attention with an adjustable factor α ∈ [0, 2] is to control the synthesis process through the input text embedding τ, implemented as follows: ScCross-Attn (Qc t, Kc t , Vc t ) =Mc t · α · V c t , M c t = Softmax \u0010 Qc t(Kc t )T / √ d \u0011 , (5) where Qc t is the query feature derived from the output of the self-attention layer, Kc t and V c t are the key and value features derived from the text embedding τ. Unlike the non-adjustable scale attention map approach in Prompt-to-Prompt [24], we introduce a factor, α, to adjust the value feature. This allows for better balancing of the text and image features, even though they share the same form. Using this fusion denoising process, the generation of a new object image is denoted as O. Following the ReNoise inversion technique [20], based on the denoising Eq.(1) and the approximation ϵθ(bzt, t, τ) ≈ ϵθ(bzt−1, t, τ) [14], the noise addition process is reformulated as: bz ′ t = \u0010 bz ′ t−1 − βtϵθ(bz ′ t, t, τ) − γtϵt \u0011 /νt. (6) 3.2 Balance fidelity and editability by optimizing the noise ϵt in inversion process In this subsection, our goal is to achieve better fidelity and editability of the object image during the inversion process. We observe that increasing the Gaussian white noise of the denoising latent code bzt−1 can enhance editability [46], while reducing the difference between the denoising latent code bzt−1 and the standard path noise code bz ′ t−1 in Eq. (6) can improve fidelity [25; 67]. However, these two objectives are contradictory. To address this, we treat the sampling noise ϵt in Eq.(1) as a learnable parameter. We define a reconstructed ℓ2 loss between bzt−1 and bz ′ t−1, Lr(ϵt) = ∥bz ′ t−1 − (νtbzt + βtϵθ(bzt, t, τ) +γtϵt)∥, and a KL divergence loss between ϵt and a Gaussian distribution, Ln(ϵt) = KL(q(ϵt)||p(N(0, I))), to simultaneously handle fidelity and editability. Based on Eqs.(1) and (6), we design a balance loss function as follows: L(ϵt) =|Lr(ϵt) − λLn(ϵt)|, (7) where λ represents the weight to balance Lr and Ln, and in this paper, we set to λ = Lr Ln = 125. Since the parameter ϵt is sampled from a standard Gaussian distribution during the noise addition process, Ln is used solely to balance Lr and its gradient is not computed for optimization. 3.3 Text-image harmony by adaptively adjusting injection step i and scale factor α Using the optimal noise ϵt, a fused object image O(α, i) can be generated by the TIDM with an initial scale factor α0 = 1and injection step i0 = ⌊T/2⌋ from the input object image OI and object text 5𝑘𝑘𝑚𝑚𝑚𝑚𝑚𝑚 = ⁄𝐼𝐼α1 𝑇𝑇α1 𝑘𝑘𝑚𝑚𝑚𝑚𝑚𝑚 = ⁄𝐼𝐼α2 𝑇𝑇α2 𝑘𝑘𝑚𝑚𝑚𝑚𝑚𝑚𝑘𝑘𝑚𝑚𝑚𝑚𝑚𝑚 ≥ 𝑘𝑘 ≥ 𝛼𝛼1 𝛼𝛼2 Figure 4: Isim and Tsim with α ∈ [0, 1.4]. Adjust injection Adjust factor Bald Eagle  𝑰𝐬𝐢𝐦 𝐦𝐢𝐧 𝑰𝐬𝐢𝐦 𝐦𝐚𝐱 Figure 5: The adjusted process of our ATIH with three initial points andε = Isim(α) +k · Tsim(α) − F(α). OT . Here, we adaptively adjust α ∈ [0, 2] and i (0 ≤ i ≤ T) by introducing an Adaptive Text-Image Harmony (ATIH) method. We denote the similarity between the image OI and the fused image O(α, i) as Isim(α, i) =d(OI, O(α, i)), and the similarity between the text OT and the fused image O(α, i) as Tsim(α, i) = d(OT , O(α, i)), where d(·, ·) represents the similarity distance between text/image and image. In this paper, we compute the similarities Isim(α, i) and Tsim(α, i) using the DINO features [44] and the CLIP features [48], respectively, based on a cosine distance d. Our key idea is to balance and maximize both Isim(α, i) and Tsim(α, i) for optimal text-image fusion. Adjust injection step i to balance fidelity and editability. Before achieving the idea, we first enable the object image to be smoothly editable by adjusting the injection step i in the injected self-attention. We denote Isim(i) =Isim(α0, i) for for convenience. In the inversion process, it is generally observed that more injections lead to less editability. When all injections are applied (i = T), an ideal fidelity is achieved. We observe that when Isim(i) < Imin sim , the fused image deviates significantly from the input image, resulting in a loss of fidelity. Conversely, whenIsim(i) > Imax sim , the fused image is too similar to the input image, resulting in no editability. To balance fidelity and editability,Isim(i) must satisfy Imin sim ≤ Isim(i) ≤ Imax sim , in Fig. 5. Therefore, initializing i = ⌊T/2⌋, i is adjusted as follows: i =    i − 1, I sim(i) < Imin sim i, I min sim ≤ Isim(i) ≤ Imax sim i + 1, I sim(i) > Imax sim , (8) where Imin sim and simmax are set to 0.45 and 0.85 in this paper, respectively, based on observations from Fig. 17. After using Eq. (8), this adaptive approach can obtain an injection step i∗ to smooth the fusion process while maintaining a balance between fidelity and editability. Fixing the injection step i = i∗, next we use abbreviations, Isim(α) =Isim(α, i∗) and Tsim(α) =Tsim(α, i∗). Adaptively adjust the scale factor α for harmonizing text and image. To implement our key idea, we design an exquisite score function with α as: max α F(α) :=Isim(α) +k · Tsim(α)| {z } maximize similarities (ellipse) −β |Isim(α) − k · Tsim(α)|| {z } balance similarities (hyperbola) , (9) where β is a weighting factor, and the parameter k is introduced to mitigate inconsistencies in scale between high Isim(α) and low Tsim(α) due to differences in text and image modalities, ensuring their scale balance. As shown in Fig. 4, Isim(α) decreases and Tsim(α) increases as α increases, and vice versa. Based on these observations, we set k = 2.3 and β = 1in this paper. In Eq. (9), the left-hand side represents the sum of the text and image similarities, forming an ellipse, while the right-hand side represents the absolute value of the difference between the text and image similarities, forming a hyperbola. A larger sum value indicates that the generated image integrates more information from the input text and image. Conversely, a smaller absolute value signifies a better balance between the text and image similarities. Additionally, given that Isim(α) ∈ [0, 1] and Tsim(α) ∈ [0, 1], their sum is greater than or equal to the absolute value of their difference, leading to F(α) ≥ 0. Therefore, our objective is to maximize F(α) to simultaneously enhance and balance both Isim(α)and and Tsim(α). Maximizing F(α) is easily implemented by the Golden Section Search [47] algorithm, and we get the optimal α∗. Fig. 5 depicts a schematic diagram to adjust both ii and α. Overall, our novel object synthesis, detailed in Algorithm 1, is presented in Appendix E. 6Bighorn MasactrlInfedit Our Original image Bowling ball Triceratops Peacock African chameleon European fire  salamande Instruct pix2pix Text: i:2  𝜶𝜶:0.875  i:2  𝜶𝜶:0.811  i:3  𝜶𝜶:0.967  i:3  𝜶𝜶:0.580  i:3  𝜶𝜶:1.053  i:3  𝜶𝜶:0.392   Figure 6: Comparisons with different image editing methods. We observe that InfEdit [ 69] MasaCtrl [5] and InstructPix2Pix [ 4] struggle to fuse object images and texts, while our method successfully implements new object synthesis, such as bowling ball-fawn in the second row. 4 Experiments 4.1 Experimental Settings Datasets. We constructed a text-image fusion (TIF) dataset consisting of 1,800 text-image pairs, derived from 60 texts and 30 images in Appendix B. Images, selected from various classes in PIE- bench [26], include 20 animal and 10 non-animal categories. Texts were chosen from the 1,000 classes in ImageNet [53], with ChatGPT [42] filtering out 40 distinct animals and 20 non-animals. Details. We implemented our method on SDXLturbo [56] only taking ten seconds. For image editing, we set the source prompt ps as an empty string ”Null” and the target prompt Pt as the target object class name. During sampling, we used the Ancestral-Euler sampler [28] with four denoising steps. All input images were uniformly scaled to 512 × 512 pixels to ensure consistent resolution in all the experiments. Our experiments were conducted using two NVIDIA GeForce RTX 4090 GPUs. Metrics. To comprehensively evaluate the performance of our method, we employed four key metrics: aesthetic score (AES) [ 57], CLIP text-image similarity (CLIP-T) [ 48], Dinov2 image similarity (Dino-I) [44], and human preference score (HPS) [68]. Following the Eq. (9), Fscore and balance similarities (Bsim) with k = 2.3 are used to measure the text-image fusion effect. 4.2 Main Results We conducted a comprehensive comparison of our ATIH model with three image-editing models (i.e., MasaCtrl [5], InfEdit [69], and InstructPix2pix [4]), two mixing models (i.e., MagicMix [34] and ConceptLab [50]), and ControlNet [72]. Notably, MagicMix and ConceptLab share a similar objective with ours to fuse object text/image, while ConceptLab only accepts two text prompts as its inputs. Due to no available code for MagicMix, we utilized its unofficial implementation [13]. 7Image  scale 1.0 1.5 4.0 4.5 5.0 7.56.5 7.0 2.0 1.5 2.0 2.5 Text scale Original Image Injection Step 1 2 3 Factor 𝛼𝛼0.34 0.44 0.54 Object text Cock Instructpix2pix Ours Figure 7: Comparisons with InstructPix2Pix [4] using image/text strength variations. Conceptlab Zucchini Cock Triceratops Peacock Grocery bagKing penguin Original  imageOurMagicmix Text: i:3  𝜶𝜶:0.390  i:3 𝜶𝜶:0.823 i:2  𝜶𝜶:0.464  i:2  𝜶𝜶:0.472 bear rabbit owl pepper corgi horse i:2  𝜶𝜶:0.966i:3  𝜶𝜶:0.548 Figure 8: Comparisons with different creative mixing methods. We observe that our results surpass those of MagicMix [34]. For ConceptLab [ 50], we exclusively examine its fusion results without making good or bad comparisons, as it is a distinct approach to creative generation. Comparisons with image-editing methods. For a fair comparison, we uniformly set the editing text prompt in all methods as a photo of an {image category} creatively fused with a {text category} to achieve the fusion of two objects. Fig. 6 visualizes some combinational objects, with additional results available in Appendix G. Our observations are as follows: Firstly, MasaCtrl and InfEdit generally preserve the original image’s details better during editing, as seen in examples like sheep-triceratops. In contrast, InstructPix2Pix tends to alter the image more significantly, making it closer to the edited text description. Secondly, different methods exhibit varying degrees of distortion when fusing two objects during the image editing process. For instance, in the case of African chameleon-bird, our method performs better by minimizing distortions and maintaining the harmony and high quality of the image. Thirdly, our method shows significant advantages in enhancing the editability of images. For the European fire salamander-glass jar example, other methods often result in only color changes and slight deformations, failing to effectively merge the two objects. In contrast, our method harmoniously integrates the colors and shapes of both the glass jar and the European fire salamander, significantly improving the editing effect and operability. Specially, Fig. 7 shows the results of 8Ours Zebra ControlNet-d(description)Original Image Pineapple ControlNet-d(object text) ControlNet-e(description) ControlNet-e(object text) Object text Figure 9: Comparisons with ControlNet-depth and ControlNet-edge [72] using a description that “A photo of an {object image} creatively fused with an {object text }”. InstructPix2Pix with manually adjusted image strengths (1.0, 1.5, 2.0) and text strengths (ranging from 1.5 to 7.5). At optimal settings of image strength 1.5 and text strength 5.0, InstructPix2Pix produced its best fusion, though some results were unnatural, like replacing the rabbit’s ears with a rooster’s head. In contrast, our method created novel and natural combinations of the rabbit and rooster by automatically achieving superior visual synthesis without manual adjustments. Comparisons with the mixing methods. Fig. 8 illustrates the results of text-image object synthesis. We observe that both MagicMix and ConceptLab tend to overly bias towards one class, such as zucchini-owl and corgi-cock. Their generated images often lean more towards one category. In contrast, our method achieves a more harmonious balance between the features of the two categories. Moreover, the fusion images produced by MagicMix frequently exhibit insufficiently smooth feature blending. For instance, in the fusion of a rabbit and an emperor penguin, the rabbit’s facial features nearly disappear. Conversely, our method seamlessly merges the facial features of both the penguin and the rabbit in the head region, preserving the main characteristics of each. Comparisons with ControlNet. We rigorously compared our method with ControlNet to assess their performance in text-image fusion tasks, as shown in Fig. 9. Our results highlight notable differences: ControlNet preserves structure well from depth or edge maps but struggles with semantic integration, especially with complex prompts, often failing to achieve seamless blending. In contrast, our method leverages full RGB features, including color and texture, alongside structural data. Table 1: Quantitative comparisons on our TIF dataset. Models DINO-I↑[44] CLIP-T↑[48] AES↑[57] HPS↑[68] Fscore↑ Bsim↓ Our ATIH 0.756 0.296 6.124 0.383 1.362 0.075 MagicMix [34] 0.587 0.328 5.786 0.373 1.174 0.167 InfEdit [69] 0.817 0.255 6.080 0.367 1.173 0.230 MasaCtrl [5] 0.815 0.234 5.684 0.343 1.077 0.277 InstructPix2Pix [4]0.384 0.394 5.881 0.375 0.768 0.522 Table 2: H-statistics (↑) (P-value (↓)) between our ATIH and other methods under different metrics. Methods DINO-I [44] CLIP-T [48] AES [57] HPS [68] Fscore Bsim MagicMix [34]665.20 (1.10e−146) 248.15 (6.58e−56) 433.00 (3.61e−96) 232.1 (1.45e−08) 633.89 (7.13e−140) 792.72 (2.06e−174) InfEdit [69]402.36 (1.68e−89) 477.31 (8.22e−106) 3.70 (5.45e−02) 114.02 (1.29e−26) 504.53 (9.81e−112) 917.99 (1.20e−201) MasaCtrl [5]404.87(4.81e−90) 943.37(3.67e−207) 277.80 (2.27e−62) 654.62 (2.21e−144) 991.48 (1.28e−217) 1183.59 (2.25e−259) InstructPix2Pix [4]1565.18 (0.000000)1891.69 (0.000000)268.57 (2.32e−60) 39.63 (3.06e−10) 1421.64 (4.18e−311) 1997.67(0.000000) Quantitative Results. Table 1 displays the quantitative results, illustrating that our method achieves state-of-the-art performance in AES, HPS, Fscore and Bsim, surpassing other methods. These results indicate that our approach excels in enhancing the visual appeal and artistic quality of images, while also aligning more closely with human preferences and understanding in terms of object fusion. Moreover, when dealing with text-image inconsistencies at scalek=2.3, our method achieves superior text-image similarity and balance, demonstrating superior fusion capability. Despite achieving the best DINO-I and CLIP-T scores under inconsistencies, InfEdit and InstructPix2Pix perform worse than our method in terms of AES, HPS,Fscore and Bsim, and their visual results remain sub-optimal. These inconsistencies ultimately lead to the failure of integrating object text and image. In contrast, our approach achieves a better text-image balance similarities. Furthermore, Table 2 presents the H-statistics [30] and P-values [66] assessing the statistical significance of performance differences between our ATIH and other methods across various metrics. Compared to Instructpix2pix, for 9instance, our method shows significant differences, with H-statistics of 268.57 for AES and 39.63 for HPS, indicating potential improvements in both aesthetic quality and human preference scoring. User Study. We conducted two user studies to assess intuitive human perception of results presented in Table 3, Table 4, and Appendix F. Each participant evaluated 6 image-editing sets and 6 fusion sets. In total, these studies garnered 570 votes from 95 participants. Our method received the highest ratings in both studies, capturing 74.03% and 79.47% of the total votes, respectively. Among the image-editing methods, InfEdit [69] garnered 14.7% of votes for its superior editing performance, while InstructPix2Pix [4] and MasaCtrl [5] received only 8% and 2.8%, respectively. In the fusion category, ConceptLab [50] received 12.28% of votes, while MagicMix [34] received 8%. Table 3: User study with image editing methods. ModelsOur ATIHMasaCtrl[5]InstructPix2Pix[4]InfEdit[69] V ote↑ 422 16 48 84 Table 4: User study with mixing methods. ModelsOur ATIHMagicMix[34]ConceptLab[50] V ote↑ 453 47 70 4.3 Parameter Analysis and Ablation Study Parameter analysis. Our primary parameters include λ in (7), Imin sim and Imax sim in (8), and k in (9). λ = Lr Ln balances the editability and fidelity of our model. We determined the specific value through personal observation combined with the changes in AES, CLIP-T, and Dino-I values at differentλ settings. Ultimately, we set λ to 125. To address the inconsistency between image similarity and text similarity scales, we approximated the scale k. Initially, we measured the variations in image similarity and text similarity with changes in α, and identified the balanced similarity regions in the fusion results. As shown in Fig. 4, the optimal range for k was found to be between [0.21, 0.27]. Based on these observations and experimental experience, we ultimately set k to 0.23. As shown in Fig. 17 of Appendix C, we observe that when the similarity between the image and the original exceeds 0.85, the images become too similar, making edits with different class texts less effective and necessitating a decrease in i. Conversely, when the similarity is below 0.45, the images overly favor the text, making them excessively editable, requiring an increase in injection steps. Therefore, we set Imin sim to 0.45 and Imax sim to 0.85. More discussions are provided in Appendix C. w/ adaptive injectPnPinv w/ balance w/ adaptive select + “broccoli” + “bald eagle” Figure 10: Ablation study of the balance loss, adaptive injection ii and adaptive selection α from the third column to the fifth column. Ablation Study. In Figs. 10 and 18 in Appendix D, we visualize the results with and without the balance loss in Eq. (7), the adaptive in- jection ii in Eq. (8), and the adaptive selection α in Eq. (9) within our object synthesis framework. Pn- Pinv, used for direct inver- sion and prompt editing, re- sulted in some distortion and blurriness. Compared to PnPinv, the balance loss significantly enhances image fidelity, improving details, textures, and editability. The adaptive injection enables a smooth transition from Corgi to Fire Engine in Fig. 18. Without this injection, the transformation is too abrupt, lacking a seamless fusion process. Finally, the adaptive selection achieves a balanced image that harmoniously integrates the original and target features. Note that for limitations, please refer to Appendix A. 5 Conclusion In this paper, we explored a novel object synthesis framework that fuses object texts with object images to create unique and surprising objects. We introduced a simple yet effective difference loss to optimize sampling noise, balancing image fidelity and editability. Additionally, we proposed an adaptive text-image harmony module to seamlessly integrate text and image elements. Extensive experiments demonstrate that our framework excels at generating a wide array of impressive object combinations. This capability is particularly advantageous for crafting innovative and captivating animated characters in the entertainment and film industry. 10References [1] Johannes Ackermann and Minjun Li. High-resolution image editing via multi-stage blended diffusion. In Proceedings of the NeurIPS Workshop on Machine Learning for Creativity and Design, 2022. [2] David Bau, Jiaming Song, Xinlei Chen, David Belanger, Jonathan Ho, Andrea Vedaldi, and Bolei Zhou. Editing implicit assumptions in text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [3] Margaret A. Boden. The creative mind-Myths and mechanisms. Taylor & Francis e-Library, 2004. [4] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 18392–18402, 2023. [5] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 22560–22570, October 2023. [6] Goirik Chakrabarty, Aditya Chandrasekar, Ramya Hebbalaguppe, and Prathosh AP. Lomoe: Localized multi-object editing via multi-diffusion. arXiv preprint arXiv:2403.00437, 2024. [7] Haoxing Chen, Zhuoer Xu, Zhangxuan Gu, Jun Lan, Xing Zheng, Yaohui Li, Changhua Meng, Huijia Zhu, and Weiqiang Wang. Diffute: Universal text editing diffusion model. In Proceedings of the Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS), 2023. [8] L. Chen, Y . Wang, and X. Zhao. Multi-concept customization of text-to-image diffusion. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [9] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz, Xuhui Jia, Ming-Wei Chang, and William W. Cohen. Subject-driven text-to-image generation via apprenticeship learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [10] Y . Chen, H. Wu, and G. Li. Neural preset for color style transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [11] Celia Cintas, Payel Das, Brian Quanz, Girmaw Abebe Tadesse, Skyler Speakman, and Pin-Yu Chen. Towards creativity characterization of generative models via group-based subset scanning. arXiv preprint arXiv:2203.00523, 2022. [12] Yuqin Dai, Wanlu Zhu, Ronghui Li, Zeping Ren, Xiangzheng Zhou, Xiu Li, Jun Li, and Jian Yang. Harmonious group choreography with trajectory-controllable diffusion. arXiv preprint arXiv:2403.06189, 2024. [13] Partho Das. magicmix. https://github.com/daspartho/MagicMix, January 2022. [14] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS), pages 8780–8794, 2021. [15] Xiaoyue Dong and Shumin Han. Prompt tuning inversion for text-driven image editing using diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 7430–7440, 2023. [16] Ahmed Elgammal, Bingchen Liu, Mohamed Elhoseiny, and Marian Mazzone. Can: Creative adversarial networks: Generating “art” by learning about styles and deviating from style norms. In Proceedings of the International Conference on Computational Creativity (ICCC), pages 96–103, 2017. [17] Dave Epstein, Allan Jabri, Ben Poole, Alexei Efros, and Aleksander Holynski. Diffusion self-guidance for controllable image generation. In Advances in Neural Information Processing Systems 36 (NeurIPS), 2023. [18] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In Proceedings of the International Conference on Learning Representations (ICLR), 2023. [19] Vidit Goel, E. Peruzzo, Yifan Jiang, Dejia Xu, N. Sebe, Trevor Darrell, Zhangyang Wang, and Humphrey Shi. Pair-diffusion: Object-level image editing with structure-and-appearance paired diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 11[20] Chaoqun Gong, Yuqin Dai, Ronghui Li, Achun Bao, Jun Li, Jian Yang, Yachao Zhang, and Xiu Li. Renoise: Real image inversion through iterative noising. arXiv preprint arXiv:2401.00711, 2024. [21] Chaoqun Gong, Yuqin Dai, Ronghui Li, Achun Bao, Jun Li, Jian Yang, Yachao Zhang, and Xiu Li. Text2avatar: Text to 3d human avatar generation with codebook-driven body controllable attribute.arXiv preprint arXiv:2401.00711, 2024. [22] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, and Bo Zhang. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10696–10706, 2022. [23] Mark Hamazaspyan and Shant Navasardyan. Diffusion-enhanced patchmatch: A framework for arbitrary style transfer with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 797–805, 2023. [24] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt- to-prompt image editing with cross attention control. In Proceedings of the International Conference on Learning Representations (ICLR), 2023. [25] Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. An edit friendly ddpm noise space: Inversion and manipulations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [26] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Direct inversion: Boosting diffusion-based editing with 3 lines of code. In Proceedings of the International Conference on Learning Representations (ICLR), 2024. [27] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Pnp inversion: Boosting diffusion-based editing with 3 lines of code. In Proceedings of the International Conference on Learning Representations (ICLR), 2024. [28] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 26565–26577, 2022. [29] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6007–6017, 2023. [30] William H. Kruskal and W. Allen Wallis. Use of ranks in one-criterion variance analysis. Journal of the American Statistical Association, 47(260):583–621, 1952. [31] Chongxuan Li, Jun Zhu, Bo Zhang, Zhenghao Peng, and Yong Jiang. Shifted diffusion for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10707–10716, 2022. [32] Jun Li, Zedong Zhang, and Jian Yang. Tp2o: Creative text pair-to-object generation using balance swap-sampling. In Proceedings of the European Conference on Computer Vision (ECCV), 2024. [33] Ronghui Li, Yuqin Dai, Yachao Zhang, Jun Li, Jian Yang, Jie Guo, and Xiu Li. Exploring multi-modal control in music-driven dance generation. arXiv preprint arXiv:2401.01382, 2024. [34] Jun Hao Liew, Hanshu Yan, Daquan Zhou, and Jiashi Feng. Magicmix: Semantic mixing with diffusion models. arXiv preprint arXiv:2210.16056, 2022. [35] Chenxi Liu, Gan Sun, Wenqi Liang, Jiahua Dong, Can Qin, and Yang Cong. Museummaker: Continual style customization without catastrophic forgetting. arXiv preprint arXiv:2404.16612, 2024. [36] Shilin Lu, Yanzhu Liu, and Adams Wai-Kin Kong. Tf-icon: Diffusion-based training-free cross-domain image composition. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2294–2305, 2023. [37] Wuyang Luo, Su Yang, Xinjian Zhang, and Weishan Zhang. Siedob: Semantic image editing by disen- tangling object and background. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1868–1878, 2023. [38] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subject-diffusion: Open domain personalized text-to-image generation without test-time fine-tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 12[39] Mary Lou Maher. Evaluating creativity in humans, computers, and collectively intelligent systems. In Proceedings of the 1st DESIRE Network Conference on Creativity and Innovation in Design, pages 22–28, 2010. [40] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations (ICLR), 2022. [41] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), pages 6038–6047, 2023. [42] OpenAI. Chatgpt: Optimizing language models for dialogue. 2023. Accessed: 2023-05-07. [43] OpenAI. Dall·e 3: Ai system for generating images from text. https://www.openai.com/dall-e-3, 2024. Accessed: 2024-05-16. [44] Maxime Oquab, Timoth´ee Darcet, Th´eo Moutakanni, Huy V o, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv ´e Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research (TMLR), 2023. [45] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. Kosmos-g: Generating images in context with multimodal large language models. In Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 2023. [46] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In Proceedings of the ACM SIGGRAPH, pages 1–11, 2023. [47] William H. Press, William T. Vetterling, Saul A. Teukolsky, and Brian P. Flannery. Numerical recipes. Citeseer, 1988. [48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021. [49] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Proceedings of the International Conference on Machine Learning (ICML), pages 8821–8831, 2021. [50] Elad Richardson, Kfir Goldberg, Yuval Alaluf, and Daniel Cohen-Or. Conceptlab: Creative concept generation using vlm-guided diffusion prior constraints. ACM Transactions on Graphics (TOG), 43(2), 2024. [51] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10684–10695, 2022. [52] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream- booth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 2022. [53] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115:211–252, 2015. [54] Chitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee, Jonathan Ho, Tim Salimans, David J. Fleet, and Mohammad Norouzi. Palette: Image-to-image diffusion models. In ACM SIGGRAPH 2022 Conference Proceedings, 2021. [55] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to- image diffusion models with deep language understanding.Proceedings of Advances in Neural Information Processing Systems, 35:36479–36494, 2022. [56] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023. 13[57] Christoph Schuhmann. aesthetic-predictor. https://github.com/LAION-AI/aesthetic-predictor, January 2022. [58] X. Song, Y . Zhang, and L. Wang. Objectstitch: Object compositing with diffusion model. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [59] Gan Sun, Wenqi Liang, Jiahua Dong, Jun Li, Zhengming Ding, and Yang Cong. Create your world: Lifelong text-to-image diffusion. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [60] H. Tang, L. Yu, and J. Song. Master: Meta style transformer for controllable zero-shot and few-shot artistic style transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [61] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text- driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1921–1930, June 2023. [62] Renke Wang, Guimin Que, Shuo Chen, Xiang Li, Jun Li, and Jian Yang. Creative birds: Self-supervised single-view 3d style transfer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8775–8784, 2023. [63] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David J. Fleet, Radu Soricut, Jason Baldridge, Mohammad Norouzi, Peter Anderson, and William Chan. Imagen editor and editbench: Advancing and evaluating text-guided image inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 18359–18369, 2023. [64] Xin Wang, Yang Song, Shuyang Gu, Jianmin Bao, Dong Chen, Han Hu, and Baining Guo. Zero-shot spatial layout conditioning for text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10717–10726, 2022. [65] Zhizhong Wang, Lei Zhao, and Wei Xing. Stylediffusion: Controllable disentangled style transfer via diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 7677–7689, 2023. [66] Ronald L. Wasserstein and Nicole A. Lazar. The asa’s statement on p-values: context, process, and purpose. The American Statistician, 70(2):129–133, 2016. [67] Chen Henry Wu and Fernando De la Torre. Unifying diffusion models’ latent space, with applications to cyclediffusion and guidance. arXiv preprint arXiv:2210.05559, 2022. [68] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. [69] Sihan Xu, Yidong Huang, Jiayi Pan, Ziqiao Ma, and Joyce Chai. Inversion-free image editing with natural language. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [70] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 18381–18391, 2022. [71] L. Zhang, R. Chen, and M. Zhao. Sine: Single image editing with text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [72] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 3813–3824, 2023. [73] Y . Zhang, L. Wang, and H. Li. Inversion-based style transfer with diffusion models. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [74] Yufan Zhou, Ruiyi Zhang, Jiuxiang Gu, and Tong Sun. Customization assistant for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [75] Ye Zhu, Yu Wu, Zhiwei Deng, Olga Russakovsky, and Yan Yan. Boundary guided learning-free semantic control with diffusion models. In Advances in Neural Information Processing Systems 36 (NeurIPS), 2023. 14A Limitation +“Komondor” +“African chameleon” original images our results Figure 11: Failure results of our ATIH model. Our method relies on the semantic correlation between the original and transformed content within the diffusion feature space. When the semantic match between two categories is weak, our method tends to produce mere texture changes rather than deeper semantic transforma- tions. This limitation suggests that our approach may struggle with transformations between categories with weak semantic associations. Future work could focus on enhancing semantic matching between different cat- egories to improve the generalizability and applicability of our method. There are still some failure cases in our model, as shown in Fig. 11. These failures can be categorized into two types. The first row illustrates that when the content of the image is significantly different from the text prompt, the changes become implicit. The second row demon- strates that in certain cases, our adaptive function results in changes that only affect the texture of the original image. In our future work, we will investigate these sit- uations further and analyze the specific items that do not yield satisfactory results. B Text and Image Categories. We selected 60 texts, as detailed in Table 5, and categorized them into 7 distinct groups. The 30 selected images are shown in Fig.12, with each image corresponding to similarly categorized texts, as outlined in Table 6. Our model is capable of fusing content between any two categories, showcasing its strong generalization ability. Table 5: List of Text Items by Object Category. Category Items Mammals kit fox, Siberian husky, Australian terrier, badger, Egyptian cat, cougar, gazelle, porcupine, sea lion, bison, komondor, otter, siamang, skunk, giant panda, zebra, hog, hippopotamus, bighorn, colobus, tiger cat, impala, coyote, mongoose Birds king penguin, indigo bunting, bald eagle, cock, ostrich, peacock Reptiles and Amphibians Komodo dragon, African chameleon, African crocodile, European fire salamander, tree frog, mud turtle Fish and Marine Life anemone fish, white shark, brain coral Plants broccoli, acorn Fruits strawberry, orange, pineapple, zucchini, butternut squash Objects triceratops, beach wagon, beer glass, bowling ball, brass, airship, digital clock, espresso maker, fire engine, gas pump, grocery bag, harp, parking meter, pill bottle Table 6: Original Object Image Categories. Category Items Mammals Sea lion, Dog (Corgi), Horse, Squirrel, Sheep, Mouse, Panda, Koala, Rabbit, Fox, Giraffe, Cat, Wolf, Bear Birds Owl, Duck, Bird Insects Ladybug Plants Tree, Flower vase Fruits and Vegeta- bles Red pepper, Apple Objects Cup of coffee, Jar, Church, Birthday cake Human Man in a suit Artwork Lion illustration, Deer illustration, Twitter logo  Figure 12: Original Object Image Set. 15C Parameter Analysis. max constructabilitymax editability balanced “husky” (reconstruction) “deer”“indigo bunting” original image Figure 13: Image variations under different λ values. The first row displays the reconstructed images. The middle and bottom rows show the results of editing with different prompts, demonstrating variations in maximum editability, a balanced approach, and maximum constructability Table 7: Quantitative comparison results with different λ. λ AES↑ CLIP-T↑ Dino-I↑ 0 6.116 0.413 0.927 125 6.153 0.417 0.902 260 6.012 0.419 0.760 Analysis of λ. Here, we provide a detailed explanation of the determination of λ. As shown in Fig. 13, we use the ratio λ = Lr Ln to balance editability and fidelity. We iteratively adjust this ratio in the range of [0, 400] with intervals of 10, measuring the Dino-I score between the reconstructed and original images, as well as the CLIP-T and AES scores for images directly edited with the inverse latent values at different ratios. These experi- ments were conducted on the class fusion dataset, using fusion text for direct image editing. Figs. 14, 15, and 16 indicate that as the ratio increases, image editability improves, peaking at a ratio of around 260, but with a decrease in quality. At a ratio of 125, both image fidelity and the AES score achieve an optimal balance. Therefore, we set λ to 125. Figure 14: Dino-I changing with λ Figure 15: CLIP-T score chang- ing with λ Figure 16: AES changing with λ Analysis of k. The experimental analysis of parameter k was conducted using sdxlturbo as the base model. The range for i was set to [0, 4], and for each value of i, α was iterated from 0 to 2.2 in steps of 0.02 to observe changes in the fused image. The averaged experimental results produced a smooth curve, as shown in Fig.4. Based on these observations, the optimal range for kk was determined to be between [2.1,2.7][2.1, 2.7]. In our experiments, we set the value of kk to 2.3. Analysis of Imin sim and Imax sim . As shown in Fig. 17, we visualized several specific node images generated during the variation of different α factor values. When the image similarity with the original image exceeds 0.85, the images become overly similar. For example, in the dog-zebra fusion experiment, the dog’s texture remains largely unchanged, and no zebra features are visible. Conversely, when the image similarity falls below 0.45, the images overly conform to the text description. In this case, the entire head of the image turns into a zebra, 16representing an over-transformation phenomenon. Based on these observations, we set the minimum similarity threshold Imin sim to 0.45 and the maximum similarity threshold Imax sim to 0.85. This range helps us achieve a good balance between retaining original image information and integrating text features. Origin I_sim:0.8523 I_sim:0.6727 I_sim:0.4462 T_sim:0.2395 T_sim:0.3951T_sim:0.2871 Text: Zebra Figure 17: Illustrates the visual results of images at different similarity levels. D Ablation Study. We present another set of ablation study results in Fig. 18, where the two rows represent the cases without (w/o) and with (w) attention projection. The input image is a Corgi, and the text is Fire engine. The output images display the different transformations as α varies. The top row shows the abrupt change in appearance without attention projection, resulting in a sudden transition from a Corgi to a fire engine. In contrast, with attention projection (bottom row), the change is smoother, achieving the desired blending result in the middle. 0.20 0.22 0.24 0.26 0.20 0.50 0.80 1.10 w/o injectionw/ injection(ours) Fire enging Fire enging Figure 18: Results changing in Iteration w/ and w/o attention injection. E Algorithm. Overall, our novel object synthesis comprises three key components: optimizing the noise ϵt through a balance of fidelity and editability loss, adaptively adjusting the injection step i, and dynamically modifying the factor α. These processes are detailed in Algorithm 1. Additionally, we utilize the Golden Section Search method to identify an optimal or sufficiently good value for α that maximizes the score function F(α) in Eq. (9). This approach operates independent of the function’s derivative, enabling rapid iteration towards achieving optimal harmony. The key steps of the Golden Section Search algorithm are outlined as follows: α1 = b − b − a ϕ , α 2 = a + b − a ϕ , where ϕ (approximately 1.618) is the golden ratio, and a and b are the current search bounds for α. During each iteration, we compare F(α1) and F(α2), and adjust the search range accordingly: if F(α1) > F(α2) then b = α2 else a = α1. This process continues until the length of the search interval|b −a| is less than a predefined tolerance, indicating convergence to a local maximum. F User Study. In this section, we delve into our two user studies in greater detail. The image results are illustrated in Figs. 6 and 8, while the outcomes of the user studies for both tasks are presented in Figs. 19 and 20. In total, we collected 17Algorithm 1 Novel Object Synthesis 1: Input: An initial image latent z0, a target prompt OT , the number of inversion steps T, inject step i, sampled noise ϵt, scale factor α, F(α) is Eq.(9) 2: Output: Object Synthesis O 3: {zT , ··· , bz ′ t−1, ··· , z0} ←scheduler inverse(z0) 4: for t = 1to T do 5: bzt−1 ← step(bzt) 6: ϵall[t] ← Balance-fidelity-editability(bzt−1, bz ′ t−1, bzt, ϵt) 7: end for 8: iinit ← T/2 9: ifinal ← Adjust-Inject(zT , ϵall, OT , iinit) 10: αgood ← Golden-Section-Search(F, αmin, αmax) 11: O ← DM(zT , ϵall, OT , ifinal, αgood) 12: return O 13: function BALANCE -FIDELITY -EDITABILITY (bzt−1, bzt−1, bz ′ t−1, ϵt) 14: while Lr/Ln > λdo 15: ϵt ← ϵt − ∇ϵt Lr(bzt−1, bz ′ t−1, ϵt, bzt) 16: end while 17: return ϵt 18: end function 19: function GOLDEN -SECTION -SEARCH (F, a, b) 20: ϕ ← 1+ √ 5 2 ▷ Golden ratio 21: c ← b − b−a ϕ 22: d ← a + b−a ϕ 23: while |b − a| > ϵdo 24: if f(c) < f(d) then 25: b ← d 26: else 27: a ← c 28: end if 29: c ← b − b−a ϕ 30: d ← a + b−a ϕ 31: end while 32: return b+a 2 33: end function 34: function ADJUST -INJECT (zT , ϵall, i, OT ) 35: ite ← 0 36: while iter <T 2 do 37: Isim ← modelIsim (zT , ϵall, i, OT ) 38: if Isim < Imin sim then 39: i ← i + 1 40: else if Imin sim ≤ Isim ≤ Imax sim then 41: i ← i 42: break 43: else 44: i ← i − 1 45: end if 46: iter ← iter + 1 47: end while 48: return i 49: end function 18570 votes from 95 participants across both studies. The specific responses for each question are detailed in Tables 8 and 9. Notably, for the fourth question in the user study corresponding to our editing method, the example of peacock and cat fusion is shown in Fig.6, the number of votes for InfEdit [69] slightly exceeded ours. However, upon examining the image results, it becomes evident that their approach leans towards a disjointed fusion, where one half of an object is spliced with the corresponding half of another object, rather than directly generating a new object as our method does. Figure 19: An example of a user study compar- ing various image-editing methods. Figure 20: An example of a user study compar- ing various mixing methods. Table 8: User study with image editing methods. image-prompt options(Models) A(Our ATIH) B(MasaCtrl) B(InstructPix2Pix) D(InfEdit) glass jar-salamander 77.89 % 1.05% 16.84% 4.21% giraffe-bowling ball 89.74 % 2.11% 2.11% 6.32% wolf-bighorn 84.21 % 1.05% 10.53% 4.21% cat-peacock 40 % 3.16% 5.26% 51.58% sheep-triceraptors 78.95 % 3.16% 11.58% 6.32% bird-African chameleon 73.68 % 6.32% 4.21% 15.79% Table 9: User study with mixing methods. (prompt) image-prompt options(Models) Our ATIH B(MagicMix) C(ConceptLab) Dog-white shark 81.05% 2.11% 16.84% Rabbit-king penguin 83.16% 11.58% 5.26% horse-microwave oven 71.58% 9.47% 18.95% camel-candelabra 86.32% 6.32% 7.37% airship-espresso maker 71.58% 11.58% 16.84% jeep-anemone fish 83.16% 8.42% 8.42% G More results. In this section, we present additional results from our model. Fig. 21 showcases further generation results using our ATIH model. We experimented with four different images, each edited with four distinct text prompts. Fig. 22 provides further examples showcasing the effectiveness of our method in complex text-driven fusion tasks. Specifically, our approach excels in extreme cases by accurately extracting prominent features, such as color and basic object forms, from detailed textual descriptions. For instance, Fig. 22 shows a well-defined edge structure for the fawn image and the text ’Green triceratops with rough, scaly skin and massive frilled head.’ Additionally, Fig. 23 illustrates our model’s versatility with multiple prompts, emphasizing its capability for continuous editing. 19broccoli salamander Komodo  dragon  skunk original image Figure 21: More visual Results. Brown triceratops with rugged,  textured horns stout legsand Green triceratops with rough, scaly  skin and massive frilled head Olive triceratops with mottled,  pebbly hide and sturdy tail Emerald cock with shimmering green  feathers and sharp beak Vibrant cock with iridescent feathers  and prominent scarlet comb Original Image Figure 22: More visual results using complex prompt fusion. H More Comparisons In this section, we present additional results from our model and compare its performance against other methods. In Fig. 24, we compare our results with those from the state-of-the-art T2I model DALL· E·3 assisted by Copilot. Our model shows superior performance when handling complex descriptive prompts for image editing. We observe that the competing model struggles to achieve results comparable to ours, particularly in maintaining the original structure and layout of images, despite adequate prompts. In Figs. 25 and 26, we present additional comparison results with mixing methods. We observed that both MagicMix and ConceptLab tend to overly favor one category, as seen in examples likeTriceratops-Teddy Bear Toy and Anemone fish-Car. Their generated images often lean more towards a single category. Recently, subject-driven text-to-image generation focuses on creating highly customized images tailored to a target subject [18; 52; 9; 74]. These methods often address the task, such as multiple concept composition, style transfer and action editing [38; 8; 45]. In contrast, our approach aims to generate novel and surprising object images by combining object text with object images. Kosmos-G [45] utilize a single image input and a creative 20+ hippopotamus + zebra original image + hog + African chameleon Figure 23: Fused results using three prompts. prompt to merge with specified text objects. The prompt is structured as “¡i¿ creatively fuse with object text,” guiding the synthesis to innovatively blend image and text elements. Our findings indicate that Kosmos-G can sometimes struggle to maintain a balanced integration of original image features and text-driven attributes. In Fig. 27, the images generated by Kosmos-G often exhibit a disparity in feature integration. “Transform the image of a majestic lion with a  golden mane into an image of a fierce eagle with  vivid red and orange feathers. Change the lion‘s  facial features to resemble an eagle, including  the beak and eyes, while maintaining the  dynamic, stylized design.” “Transform the image of a horse into a shark- like horse. Change the horse's body to have  smooth, gray skin and fins while keeping the  overall shape similar. Adjust the head to  resemble a shark's with sharp teeth and a dorsal  fin. Maintain the outdoor setting with a grassy  background.” “Transform the  image of flowers into  strawberry-like flowers. Change the petals to  resemble the texture and color of strawberries,  maintaining the overall shape of the flowers.  Adjust the colors to include vibrant reds and  greens, while keeping the same vase and  arrangement.” “Transform the image of a squirrel into a  \"pineapple squirrel.\" Change its fur texture to  resemble pineapple skin and add pineapple-like  tufts on its ears. Adjust the background to match  the outdoor setting with a tree and sky.” original image ours bing(DALLE·3) complex prompt + “cock” + “pineapple” + “strawberry” + “shark” Figure 24: Comparisons with complex prompt editing. 21Conceptlab Candelabra White shark Espresso maker Anemone fish Micro-wave oven Generated imageOurMagicmix Jeep Text: Camel Horse Dog Airship Figure 25: Comparison results of mixing methods using text-generated images. Conceptlab Cougar Porcupine Triceratops Mud turtle Strawberry Original  imageOurMagicmix Text: i:2  𝜶𝜶:0.81 1   i:2  𝜶𝜶:0.46 4   i:2  𝜶𝜶:0.52 car naruto toy monkey dog teddy bear toy i:2  𝜶𝜶:1.22 4   i:3  𝜶𝜶:1.05 1   Figure 26: Further comparisons with mixing methods. 22Ours Kosmos-GOriginal Image StrawberryBadger Object text Figure 27: Comparisons with Subject-driven method. 23",
      "references": [
        "High-resolution image editing via multi-stage blended diffusion",
        "Editing implicit assumptions in text-to-image diffusion models",
        "The creative mind-Myths and mechanisms",
        "Instructpix2pix: Learning to follow image editing instructions",
        "Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing",
        "Lomoe: Localized multi-object editing via multi-diffusion",
        "Diffute: Universal text editing diffusion model",
        "Multi-concept customization of text-to-image diffusion",
        "Subject-driven text-to-image generation via apprenticeship learning",
        "Neural preset for color style transfer",
        "Towards creativity characterization of generative models via group-based subset scanning",
        "Harmonious group choreography with trajectory-controllable diffusion",
        "magicmix",
        "Diffusion models beat gans on image synthesis",
        "Prompt tuning inversion for text-driven image editing using diffusion models",
        "Can: Creative adversarial networks: Generating “art” by learning about styles and deviating from style norms",
        "Diffusion self-guidance for controllable image generation",
        "An image is worth one word: Personalizing text-to-image generation using textual inversion",
        "Pair-diffusion: Object-level image editing with structure-and-appearance paired diffusion models",
        "Renoise: Real image inversion through iterative noising",
        "Text2avatar: Text to 3d human avatar generation with codebook-driven body controllable attribute",
        "Vector quantized diffusion model for text-to-image synthesis",
        "Diffusion-enhanced patchmatch: A framework for arbitrary style transfer with diffusion models",
        "Prompt-to-prompt image editing with cross attention control",
        "An edit friendly ddpm noise space: Inversion and manipulations",
        "Direct inversion: Boosting diffusion-based editing with 3 lines of code",
        "Pnp inversion: Boosting diffusion-based editing with 3 lines of code",
        "Elucidating the design space of diffusion-based generative models",
        "Imagic: Text-based real image editing with diffusion models",
        "Use of ranks in one-criterion variance analysis",
        "Shifted diffusion for text-to-image generation",
        "Tp2o: Creative text pair-to-object generation using balance swap-sampling",
        "Exploring multi-modal control in music-driven dance generation",
        "Magicmix: Semantic mixing with diffusion models",
        "Museummaker: Continual style customization without catastrophic forgetting",
        "Tf-icon: Diffusion-based training-free cross-domain image composition",
        "Siedob: Semantic image editing by disen- tangling object and background",
        "Subject-diffusion: Open domain personalized text-to-image generation without test-time fine-tuning",
        "Evaluating creativity in humans, computers, and collectively intelligent systems",
        "Sdedit: Guided image synthesis and editing with stochastic differential equations",
        "Null-text inversion for editing real images using guided diffusion models",
        "Chatgpt: Optimizing language models for dialogue",
        "Dall·e 3: Ai system for generating images from text",
        "Dinov2: Learning robust visual features without supervision",
        "Kosmos-g: Generating images in context with multimodal large language models",
        "Zero-shot image-to-image translation",
        "Numerical recipes",
        "Learning transferable visual models from natural language supervision",
        "Zero-shot text-to-image generation",
        "Conceptlab: Creative concept generation using vlm-guided diffusion prior constraints",
        "High-resolution image synthesis with latent diffusion models",
        "Dream- booth: Fine tuning text-to-image diffusion models for subject-driven generation",
        "Imagenet large scale visual recognition challenge",
        "Palette: Image-to-image diffusion models",
        "Photorealistic text-to- image diffusion models with deep language understanding",
        "Adversarial diffusion distillation",
        "aesthetic-predictor",
        "Objectstitch: Object compositing with diffusion model",
        "Create your world: Lifelong text-to-image diffusion",
        "Master: Meta style transformer for controllable zero-shot and few-shot artistic style transfer",
        "Plug-and-play diffusion features for text- driven image-to-image translation",
        "Creative birds: Self-supervised single-view 3d style transfer",
        "Imagen editor and editbench: Advancing and evaluating text-guided image inpainting",
        "Zero-shot spatial layout conditioning for text-to-image diffusion models",
        "Stylediffusion: Controllable disentangled style transfer via diffusion models",
        "The asa’s statement on p-values: context, process, and purpose",
        "Unifying diffusion models’ latent space, with applications to cyclediffusion and guidance",
        "Human preference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis",
        "Inversion-free image editing with natural language",
        "Paint by example: Exemplar-based image editing with diffusion models",
        "Sine: Single image editing with text-to-image diffusion models",
        "Adding conditional control to text-to-image diffusion models",
        "Inversion-based style transfer with diffusion models",
        "Customization assistant for text-to-image generation",
        "Boundary guided learning-free semantic control with diffusion models"
      ],
      "meta_data": {
        "arxiv_id": "2410.20823v1",
        "authors": [
          "Zeren Xiong",
          "Zedong Zhang",
          "Zikun Chen",
          "Shuo Chen",
          "Xiang Li",
          "Gan Sun",
          "Jian Yang",
          "Jun Li"
        ],
        "published_date": "2024-10-28T08:14:40Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Adaptive Text-Image Harmony (ATIH), a novel diffusion-based method for synthesizing new objects by fusing an object image with an object text. The primary contributions include addressing the imbalance between text and image inputs in standard diffusion models and proposing an adaptive mechanism that balances editability and fidelity through learnable parameters and a novel similarity score function.",
        "methodology": "The proposed framework modifies a pre-trained diffusion model (SDXL Turbo) by incorporating a scale factor (α) into the cross-attention layers and an injection step (i) into the self-attention layers, thereby controlling the integration of text and image features. It employs a balance loss to optimize the sampling noise in order to maintain both fidelity and editability. Additionally, a Golden Section Search algorithm is used to adaptively select the optimal α that maximizes the similarity score between the generated image and the input modalities.",
        "experimental_setup": "The experimental evaluation uses a custom text-image fusion (TIF) dataset comprising 1,800 text-image pairs derived from PIE-bench and ImageNet classes. The experiments are performed on high-end GPUs (e.g., NVIDIA RTX 4090) using metrics such as aesthetic score (AES), CLIP text-image similarity, Dino-I similarity, human preference score (HPS), and additional fusion balance metrics (Fscore and Bsim). Comparative studies against state-of-the-art methods (MagicMix, InfEdit, MasaCtrl, InstructPix2Pix, ControlNet, and ConceptLab), along with ablation studies and user studies, validate the performance improvements achieved by ATIH.",
        "limitations": "The method relies heavily on the semantic correlation between the image and text inputs. In cases where the semantic association is weak, the model may only apply superficial texture changes rather than deeper semantic transformation. Additionally, the approach might struggle with transformations between categories that do not share strong semantic ties, leading to failure cases in achieving harmonious fusion.",
        "future_research_directions": "Future work could focus on enhancing semantic matching between disparate object categories to improve robustness. Extending the method to handle more complex, multi-object compositions and challenging editing scenarios may further broaden its applicability. Investigating more advanced optimization techniques for parameter selection and exploring integration with other modalities could also be promising research directions.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Soft Augmentation for Image Classification",
      "full_text": "Soft Augmentation for Image Classification Yang Liu, Shen Yan, Laura Leal-Taixé, James Hays, Deva Ramanan Argo AI youngleoel@gmail.com, shenyan@google.com, leal.taixe@tum.de, hays@gatech.edu, deva@cs.cmu.edu Abstract Modern neural networks are over-parameterized and thus rely on strong regularization such as data augmenta- tion and weight decay to reduce overfitting and improve generalization. The dominant form of data augmentation applies invariant transforms, where the learning target of a sample is invariant to the transform applied to that sam- ple. We draw inspiration from human visual classifica- tion studies and propose generalizing augmentation with invariant transforms to soft augmentation where the learn- ing target softens non-linearly as a function of the de- gree of the transformapplied to the sample: e.g., more ag- gressive image crop augmentations produce less confident learning targets. We demonstrate that soft targets allow for more aggressive data augmentation, offer more robust performance boosts, work with other augmentation poli- cies, and interestingly, produce better calibrated models (since they are trained to be less confident on aggressively cropped/occluded examples). Combined with existing ag- gressive augmentation strategies, soft targets 1) double the top-1 accuracy boost across Cifar-10, Cifar-100, ImageNet- 1K, and ImageNet-V2, 2) improve model occlusion perfor- mance by up to 4×, and 3) half the expected calibration error (ECE). Finally, we show that soft augmentation gen- eralizes to self-supervised classification tasks. Code avail- able at https://github.com/youngleox/soft_ augmentation 1. Introduction Deep neural networks have enjoyed great success in the past decade in domains such as visual understanding [42], natural language processing [5], and protein structure pre- diction [41]. However, modern deep learning models are often over-parameterized and prone to overfitting. In addi- tion to designing models with better inductive biases, strong regularization techniques such as weight decay and data augmentation are often necessary for neural networks to achieve ideal performance. Data augmentation is often a computationally cheap and effective way to regularize mod- els and mitigate overfitting. The dominant form of data aug- mentation modifies training samples with invariant trans- forms – transformations of the data where it is assumed that the identity of the sample is invariant to the transforms. Indeed, the notion of visual invariance is supported by evidence found from biological visual systems [54]. The robustness of human visual recognition has long been docu- mented and inspired many learning methods including data augmentation and architectural improvement [19, 47]. This paper focuses on the counterpart of human visual robust- ness, namely how our vision fails. Instead of maintaining perfect invariance, human visual confidence degrades non- linearly as a function of the degree of transforms such as occlusion, likely as a result of information loss [44]. We propose modeling the transform-induced information loss for learned image classifiers and summarize the contribu- tions as follows: • We propose Soft Augmentation as a generalization of data augmentation with invariant transforms. With Soft Aug- mentation, the learning target of a transformed training sample softens. We empirically compare several soften- ing strategies and prescribe a robust non-linear softening formula. • With a frozen softening strategy, we show that replac- ing standard crop augmentation with soft crop augmenta- tion allows for more aggressive augmentation, and dou- bles the top-1 accuracy boost of RandAugment [8] across Cifar-10, Cifar-100, ImageNet-1K, and ImageNet-V2. • Soft Augmentation improves model occlusion robustness by achieving up to more than 4× Top-1 accuracy boost on heavily occluded images. • Combined with TrivialAugment [37], Soft Augmentation further reduces top-1 error and improves model calibra- tion by reducing expected calibration error by more than half, outperforming 5-ensemble methods [25]. • In addition to supervised image classification models, Soft Augmentation also boosts the performance of self- supervised models, demonstrating its generalizability. arXiv:2211.04625v2  [cs.CV]  23 Jan 2024-32 -24 -16 -8 0 8 16 24 32 tx -32 -24 -16 -8 0 8 16 24 32 ty Top-1 Error: 20.80 Standard Hard Crop -32 -24 -16 -8 0 8 16 24 32 tx -32 -24 -16 -8 0 8 16 24 32 ty 22.99(+2.19) Aggressive Hard Crop -32 -24 -16 -8 0 8 16 24 32 tx -32 -24 -16 -8 0 8 16 24 32 ty 18.31(−2.49) Soft Augmentation 0 1 0 1 0 1 Target Confidence (p) original image 77% visible  38% visible  22% visible Figure 1. Traditional augmentation encourages invariance by requiring augmented samples to produce the same target label; we visualize the translational offset range (tx, ty) of Standard Hard Crop augmentations for 32 × 32 images from Cifar-100 on the left, reporting the top-1 error of a baseline ResNet-18. Naively increasing the augmentation range without reducing target confidence increases error (middle), but softening the target label by reducing the target confidence for extreme augmentations reduces the error ( right), allowing for training with even more aggressive augmentations that may even produce blank images. Our work also shows that soft augmentations produce models that are more robust to occlusions (since they encounter larger occlusions during training) and models that are better calibrated (since they are trained to be less-confident on such occluded examples). 2. Related Work 2.1. Neural Networks for Vision Since the seminal work from Krizhevskyet al. [24], neu- ral networks have been the dominant class of high per- forming visual classifiers. Convolutional Neural Networks (CNNs) are a popular family of high performing neural models which borrows a simple idea of spatially local com- putations from biological vision [12, 18, 26]. With the help of architectural improvements [15], auxiliary loss [42], and improved computational power [13], deeper, larger, and more efficient neural nets have been developed in the past decade. 2.2. Data Augmentation Data augmentation has been an essential regularizer for high performing neural networks in many domains includ- ing visual recognition. While many other regularization techniques such as weight decay [32] and batch normal- ization [4] are shown to be optional, we are aware of no competitive vision models that omit data augmentation. Accompanying the influential AlexNet model, Krizhevsky et al . [24] proposed horizontal flipping and random cropping transforms which became the back- bone of image data augmentation. Since the repertoire of invariant transformations has grown significantly in the past decade [42], choosing which subset to use and then finding the optimal hyperparameters for each transform has become computationally burdensome. This sparked a line of research [7, 28] which investigates optimal policies for data augmentation such as RandAugment [8] and TrivialAugment [37]. 2.3. Learning from Soft Targets While minimizing the cross entropy loss between model logits and hard one-hot targets remains the go-to recipe for supervised classification training, learning with soft targets has emerged in many lines of research. Label Smooth- ing [36, 43] is a straightforward method which applies a fixed smoothing (softening) factor α to the hard one-hot classification target. The motivation is that label smoothing prevents the model from becoming over-confident. Müller et al. [36] shows that label smoothing is related to knowl- edge distillation [17], where a student model learns the soft distribution of a (typically) larger teacher model. A related line of research [49,53] focuses on regularizing how a model interpolates between samples by linearly mix- ing two or more samples and linearly softening the result- ing learning targets. Mixing can be in the form of per-pixel blending [53] or patch-level recombination [49]. 2.4. Robustness of Human Vision Human visual classification is known to be robust against perturbations such as occlusion. In computer vision re- search, the robustness of human vision is often regarded as the gold standard for designing computer vision mod- els [34, 54]. These findings indeed inspire development of robust vision models, such as compositional, recurrent, and occlusion aware models [22,46,47]. In addition to specialty models, much of the idea of using invariant transforms toaugment training samples come from the intuition and ob- servation that human vision are robust against these trans- forms such as object translation, scaling, occlusion, photo- metric distortions, etc. Recent studies such as Tang et al. [44] indeed confirm the robustness of human visual recognition against mild to moderate perturbations. In a 5-class visual classifica- tion task, human subjects maintain high accuracy when up to approximately half of an object is occluded. However, the more interesting observation is that human performance starts to degenerate rapidly as occlusion increases and falls to chance level when the object is fully occluded (see Figure 2 right k = 2, 3, 4 for qualitative curves). 3. Soft Augmentation In a typical supervised image classification setting, each training image xi has a ground truth learning target yi asso- ciated to it thus forming tuples: (xi, yi), (1) where xi ∈ RC×W×H denotes the image and yi ∈ [0, 1]N denotes a N-dimensional one-hot vector representing the target label (Figure 2 left, “Hard Target”). As modern neural models have the capacity to memorize even large datasets [1], data augmentation mitigates the issue by hal- lucinating data points through transformations of existing training samples. (Hard) data augmentation relies on the key underlying assumption that the augmented variant of xi should main- tain the original target label yi: (xi, yi) ⇒ (tϕ∼S(xi), yi) , Hard Augmentation (2) where tϕ∼S(xi) denotes the image transform applied to sample xi, ϕ is a random sample from the fixed transform range S. Examples of image transforms include transla- tion, rotation, crop, noise injection, etc. As shown by Tang et al. [44], transforms of xi such as occlusion are approxi- mately perceptually invariant only whenϕ is mild. Hence S often has to be carefully tuned in practice, since naively in- creasing it can lead to degraded performance (Figure 1). In the extreme case of 100% occlusion, total information loss occurs, making it detrimental for learning. Label Smoothing applies a smoothing function g to the target label yi parameterized by a handcrafted, fixed smoothing factor α. Specifically, label smoothing replaces the indicator value ‘1’ (for the ground-truth class label) with p = 1 − α, distributing the remaining α probability mass across all other class labels (Figure 2 left, “Soft Target”). One can interpret label smoothing as accounting for the average loss of information resulting from averaging over transforms from the range S. From this perspective, the smoothing factor α can be written as a function of the fixed transform range S: (xi, yi) ⇒ \u0000 tϕ∼S(xi), gα(S)(yi) \u0001 , Label Smoothing (3) Soft Augmentation, our proposed approach, can now be described succinctly as follows: replace the fixed smoothing factor α(S) with an adaptive smoothing factor α(ϕ), that depends on the degree of thespecific sampled augmentation ϕ applied to xi: (xi, yi) ⇒ \u0000 tϕ∼S(xi), gα(ϕ)(yi) \u0001 , Soft Augmentation (Target) (4) Crucially, conditioning on the information loss from a par- ticular ϕ allows one to define far larger augmentation ranges S. We will show that such a strategy consistently produces robust performance improvements with little tun- ing across a variety of datasets, models, and augmentation strategies. Extensions to Soft Augmentation may be proposed by also considering loss reweighting [40, 48], which is an al- ternative approach for softening the impact of an augmented example by down-weighting its contribution to the loss. To formalize this, let us write the training samples of a super- vised dataset as triples including a weight factor wi (that is typically initialized to all ‘1’s). One can then re-purpose our smoothing function g to modify the weight instead of (or in addition to) the target label (Figure 2 left): (xi, yi, wi) ⇒ \u0000 tϕ∼S(xi), yi, gα(ϕ)(wi) \u0001 , Soft Augmentation (Weight) (5) (xi, yi, wi) ⇒ \u0000 tϕ∼S(xi), gα(ϕ)(yi), gα(ϕ)(wi) \u0001 . Soft Augmentation (Target & Weight) (6) Finally, one may wish to soften targets by exploit- ing class-specific confusions when applying α(ϕ); the smoothed target label of a highly-occluded truck example could place more probability mass on other vehicle classes, as opposed to distributing the remaining probability equally across all other classes. Such extensions are discussed in Section 5. 4. Experiments 4.1. Soft Augmentation with Crop As a concrete example of the proposed Soft Augmenta- tion, we consider the crop transform t(tx,ty,w,h)(x). In the case of 32×32 pixel Cifar images [23], the cropped images typically have a constant sizew = h = 32, and t(x) is fully parameterized by tx and ty, which are translational offsets1 0 0 1 0 0 0.1 0.1 0.6 0.1 0.1 0.1 0.1 0.6 0.1 0.1 0.6 x 0 0 1 0 0 0.6 x Hard Target Soft Weight Soft Target Soft Target & Weight 1.0 x 1.0 x Weight One-Hot Target  Variants of Soft Augmentation Variant 0.0 0.5 1.0 Proportion Visible (v) 0.0 pmin 1 - α 1.0 Target Confidence (p) Softening Curves chance k=1 k=2 k=3 k=4 LS Figure 2. Variants of Soft Augmentation as prescribed by Equations 4 (Soft Target), 5 (Soft Weight), 6 (Soft Target & Weight) with example target confidence p = 0.6 (left). Soft Augmentation applies non-linear (k = 2, 3, 4, ...) softening to learning targets based on the specific degree of occlusion of a cropped image (Equation 7), which qualitatively captures the degradation of human visual recognition under occlusion [44]. Label Smoothing applies a fixed softening factor α to the one-hot classification target. between the cropped and the original image. As shown in Figure 1 (left), the standard hard crop augmentation for the Cifar-10/100 classification tasks drawstx, tyindependently from a uniform distribution of a modest range U(−4, 4). Under this distribution, the minimal visibility of an image is 77% and ResNet-18 models trained on the Cifar-100 task achieve mean top-1 validation error of 20.80% across three independent runs (Figure 1 left). Naively applying aggres- sive hard crop augmentation drawn from a more aggressive range U(−16, 16) increases top-1 error by 2.19% (Figure 1 middle). We make two changes to the standard crop aug- mentation. We first propose drawing tx, tyindependently from a scaled normal distribution S∗ ∼N(0, σL) (with clipping such that |tx| < L,|ty| < L), where L is the length of the longer edge of the image ( L = 32 for Cifar). The distri- bution has zero mean and σ controls the relative spread of the distribution hence the mean occlusion level. Following the 3σ rule of normal distribution, an intuitive tuning-free choice is to set σ ≈ 0.3, where ∼ 99% of cropped samples have visibility ≥ 0. Figure 3 (left, α = 0) shows that chang- ing the distribution alone without target softening provides a moderate ∼ 0.4% performance boost across crop strength σ. Directly borrowing the findings from human vision re- seach [44], one can define an adaptive softeningα(tx, ty, k) that softens the ground truth learning target. Similar to La- bel Smoothing [43], a hard target can be softened to confi- dence p ∈ [0, 1]. Instead of a fixed α, consider a family of power functions that produces target hardness p given crop parameters tx, tyand curve shape k: p = 1 −α(tx, ty, k) = 1 −(1 −pmin)(1 −v(tx,ty))k, (7) where v(tx,ty) ∈ [0, 1] is the image visibility which is a function of tx and ty. The power function family is a simple one-parameter formulation that allows us to test both linear (k = 1) and non-linear (k ̸= 1) softening: higher k provides flatter plateaus in high visibility regime (see Figure 2 right). 0.1 0.2 0.3 0.4 0.5 Crop Strength (σ) -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Top-1 Error Reduction (%) Label Smoothing baseline α=0 α=0.001 α=0.01 α=0.1 0.1 0.2 0.3 0.4 0.5 Crop Strength (σ) -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Soft Target baseline k=1 k=2 k=3 k=4 0.1 0.2 0.3 0.4 0.5 Crop Strength (σ) -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Soft Weight baseline k=1 k=2 k=3 k=4 0.1 0.2 0.3 0.4 0.5 Crop Strength (σ) -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Soft Target & Weight baseline k=1 k=2 k=3 k=4 Figure 3. Soft Augmentation reduces the top-1 validation error of ResNet-18 on Cifar-100 by up to 2.5% via combining both target and weight softening (Equation 6). Applying target softening alone (Equation 4) can boost performance by ∼ 2%. Crop parameters tx, ty are independently drawn from N(0, σL) (L = 32). Higher error reductions indicate better performance over baseline. All results are the means and standard errors across 3 independent runs.As seen in Label Smoothing, p can be interpreted as ground truth class probability of the one-hot learning target. pmin is the chance probability depending on the task prior. For example, for Cifar-100, pmin = 1 #classes = 0.01. Equation 7 has three assumptions: 1) the information loss is a function of image visibility and all information is lost only when the image is fully occluded, 2) the original label of a training image has a confidence of 100%, which suggests that there is no uncertainty in the class of the label, and 3) the information loss of all images can be approx- imated by a single confidence-visibility curve. While the first assumption is supported by observations of human vi- sual classification research [44], and empirical evidence in Sections 4.2 and 4.3 suggests that the second and the third assumptions approximately hold, the limitations to these as- sumptions will be discussed in Section 5. 4.2. How to Soften Targets As prescribed by Equations 4, 5, and 6, three versions of Soft Augmentation are compared with Label Smoothing across a range of crop strength σ. The popular ResNet-18 models [16] are trained on the 100-class classification Cifar- 100 training set. Top-1 error reductions on the validation set are reported (details in Appendix B). Consistent with prior studies, label smoothing can boost model performance by ∼ 1.3% when the smoothing factor α is properly tuned (Figure 3 left). Combining both target and weight softening (Equation 6) with k = 2 and σ ≈ 0.3 boosts model performance by 2.5% (Figure 3 right). Note that k = 2 qualitatively re- sembles the shape of the curve of human visual confidence degradation under occlusion reported by Tang et al. [44]. Interestingly, the optimal σ ≈ 0.3 fits the intuitive 3-σ rule. In the next section we freeze k = 2 and σ = 0 .3 and show robust improvements that generalize to Cifar-10 [23], ImageNet-1K [9], and ImageNet-V2 tasks [39]. 4.3. Supervised Classification 4.3.1 Comparison with Related Methods As mentioned in Section 2, many approaches similar to soft augmentation have demonstrated empirical perfor- mance gains, including additional data augmentation trans- forms [10], learning augmentation policies [8], softening learning targets [43], and modifying loss functions [29]. However, as training recipes continued to evolve over the past decade, baseline model performance has improved ac- cordingly. As seen in Table 1 (Baseline), our baseline ResNet-18 models with a 500-epoch schedule and cosine learning rate decay [33] not only outperform many recent baseline models of the same architecture, but also beat var- ious published results of Mixup and Cutout. To ensure fair comparisons, we reproduce 6 popular methods: Mixup, Table 1. Soft augmentation outperforms related methods. Optimal hyperparameters for Mixup [53], Cutout [10], and Online Label Smoothing [52] were applied. α of Focal Loss is tuned as [29] did not prescribe an optimal α for Cifar classification. It is worth not- ing that our baseline model (20.80%) not only outperforms other published baseline models by 1.5% to 4.8%, but also beat various implementations of Mixup and Cutout. Top-1 errors of ResNet-18 on Cifar-100 are reported. ResNet-18 Top-1 Error Baseline Zhanget al. [53] 25.6 DeVries and Taylor [10]22.46±0.31 Kimet al. [20] 23.59 Ours 20.80±0.11 Mixup Zhanget al. [53] 21.1 Kimet al. [20] 22.43 Ours 19.88±0.38 Cutout DeVries and Taylor [10]21.96±0.24 Ours 20.51±0.02 Label Smoothing Ours 19.47±0.18 Online Label Smoothing 20.12±0.05 Focal Loss (α= 1) 20.45±0.08 Focal Loss (α= 2) 20.38±0.08 Focal Loss (α= 5) 20.69±0.17 RandAugment 20.99±0.11 Soft Augmentation 18.31±0.17 Cutout, Label Smoothing, Online Label Smoothing, Focal Loss, and RandAugment, and report the Top-1 Error on Cifar-100 in Table 1. Additional comparisons with the self- reported results are available in Appendix Table 5. Table 1 shows that Soft Augmentation outperforms all other single methods. It is worth noting that although focal loss [29] proposed for detection tasks, it can be tuned to slightly improve classification model performance. 4.3.2 Soft Augmentation Compliments RandAugment This section investigates the robustness of Soft Augmen- tation across models and tasks, and how well it compares or complements more sophisticated augmentation policies such as RandAugment [8]. The ImageNet-1K dataset [9] has larger and variable-sized images compared to the Ci- far [23] datasets. In contrast with the fixed-sized crop augmentation for Cifar, a crop-and-resize augmentation t(tx,ty,w,h)(x) with random location tx, tyand random size w, his standard for ImageNet training recipes [7,8,42]. The resizing step is necessary to produce fixed-sized training images (e.g. 224 × 224). We follow the same σ = 0 .3 principle for drawing tx, tyand w, h(details in Appendix B). Comparing single methods, Soft Augmentation with crop only consistently outperforms the more sophisticated RandAugment with 14 transforms (Table 2). The small ResNet-18 models trained with SA on Cifar-10/100 even outperforms much larger baseline ResNet-50 [39] andTable 2. Soft Augmentation (SA) with a fixed softening curve of k = 2 doubles the top-1 error reduction of RandAugment (RA) across datasets and models. Note that the ResNet-18 models trained with SA on Cifar-10/100 even outperform larger baseline ResNet-50 and WideResNet-28 models. All results are mean ± standard error of top-1 validation error in percentage. Best results are shown in bold, runners-up are underlined, and results in parentheses indicate improvement over baseline. Statistics are computed from three runs. Dataset Model Baseline SA RA SA+RA Cifar100 EfficientNet-B0 49.70±1.55 42.13±0.45(−7.57) 46.68±1.52(−3.02) 38.72±0.71(−10.98) ResNet-18 20.80±0.11 18.31±0.17(−2.49) 20.99±0.11(+0.19) 18.10±0.20(−2.70) ResNet-50 20.18±0.30 18.06±0.24(−2.12) 18.57±0.09(−1.61) 16.72±0.06(−3.46) WideResNet-28 18.60±0.19 16.47±0.18(−2.13) 17.65±0.14(−0.95) 15.37±0.17(−3.23) PyramidNet + ShakeDrop15.77±0.17 14.03±0.05(−1.75) 14 .02±0.28(−1.76) 12.78±0.16(−2.99) Cifar10 EfficientNet-B0 17.73±0.69 12.21±0.22(−5.52) 14.54±0.47(−3.19) 11.67±0.26(−6.06) ResNet-18 4.38±0.05 3.51±0.08(−0.87) 3.89±0.06(−0.49) 3.27±0.08(−1.11) ResNet-50 4.34±0.14 3.67±0.08(−0.67) 3.91±0.14(−0.43) 3.01±0.02(−1.33) WideResNet-28 3.67±0.08 2.85±0.02(−0.82) 3.26±0.04(−0.41) 2.45±0.03(−1.20) PyramidNet + ShakeDrop 2.86±0.03 2.26±0.02(−0.60) 2.32±0.08(−0.54) 2.02±0.01(−0.84) ImageNet-1K ResNet-50 22.62±<0.01 21.66±0.02(−0.96) 22.02±0.02(−0.60) 21.27±0.05(−1.35) ResNet-101 20.91±0.04 20.63±0.03(−0.28) 20 .39±0.07(−0.52) 19.86±0.03(−1.05) ImageNet-V2 ResNet-50 34.97±0.03 33.32±0.10(−1.65) 34.16±0.21(−0.81) 32.38±0.16(−2.59) ResNet-101 32.68±0.04 31.81±0.16(−0.87) 32.08±0.19(−0.60) 31.26±0.12(−1.42) WideResNet-28 [50] models. Because RandAugment is a searched policy that is orig- inally prescribed to be applied in addition to the standard crop augmentation [8], one can easily replace the standard crop with soft crop and combine Soft Augmentation and RandAugment. As shown in Table 2, Soft Augmentation complements RandAugment by doubling its top-1 error re- duction across tasks and models. Note that for the small ResNet-18 model trained on Cifar-100, the fixed RandAugment method slightly de- grades its performance. Consistent with observations from Cubuk et al. [8], the optimal hyperparameters for RandAug- ment depend on the combination of model capacity and task complexity. Despite the loss of performance of applying RandAugment alone, adding Soft Augmentation reverses the effect and boosts performance by 2.7%. For the preceding experiments, a fixed k = 2 is used for Soft Augmentation and the official PyTorch RandomAug- ment [38] is implemented to ensure a fair comparison and to evaluate robustness. It is possible to fine-tune the hyperpa- rameters for each model and task to achieve better empirical performance. 4.3.3 Occlusion Robustness As discussed in Section 2, occlusion robustness in both hu- man vision [34,44,54] and computer vision [22,46,47] have been an important property for real world applications of vi- sion models as objects. To assess the effect of soft augmen- tation on occlusion robustness of computer vision models, ResNet-50 models are tested with occluded ImageNet vali- dation images (Figure 4 and Appendix Figure 7).224×224 validation images of ImageNet are occluded with randomly placed square patches that cover λ of the image area. λ is set to {0%, 20%, 40%, 60%, 80%} to create a range of oc- clusion levels. As shown in Figure 5, both RandAugment (RA) and Soft Augmentation (SA) improve occlusion robustness indepen- dently across occlusion levels. Combining RA with SA re- duces Top-1 error by up to 17%. At 80% occlusion level, SA+RA achieves more than 4× accuracy improvement over the baseline (18.98% vs 3.42%). 4.3.4 Confidence Calibration In addition to top-1 errors, reliability is yet another impor- tant aspect of model performance. It measures how close a model’s predicted probability (confidence) tracks the true correctness likelihood (accuracy). Expected Calibration Er- ror (ECE) is a popular metric [14, 25, 35] to measure con- fidence calibration by dividing model predictions into M confidence bins (Bm) and compute a weighted average er- ror between accuracy and confidence: ECE = MX m=1 |Bm| n |acc(Bm) − conf(Bm)|, (8) where n is the number of samples, acc(Bm) denotes the accuracy of bin m, and conf(Bm) denotes mean model confidence of bin m. Consistent with Guo et al. [14], we set M = 10 and compute ECE for Cifar-10 and Cifar-100 tasks. As shown in Table 3, many methods [25,30,35,45] have been proposed to improve confidence calibration, some- times at the cost of drastically increased computational overhead [25], or degraded raw performance [30]. We show in Table 3 (and Appendix Table 7) that it is possible to fur- ther reduce model top-1 error and expected calibration error simultaneously.Occlusion: 0% wreckBaselineSA+RA Class Prediction Probability wreck 1.00 wreck 0.98 Class Prediction Probability liner 0.00 liner 0.00 Class Prediction Probability dock 0.00 dock 0.00 Class Prediction Probability pirate 0.00 submarine 0.00 Class Prediction Probability submarine 0.00 pirate 0.00 Occlusion: 20% Class Prediction Probability wreck 0.95 wreck 0.95 Class Prediction Probability steel_arch_bridge 0.01 dock 0.01 Class Prediction Probability pier 0.01 submarine 0.01 Class Prediction Probability liner 0.01 liner 0.00 Class Prediction Probability crane 0.00 paddlewheel 0.00 Occlusion: 40% Class Prediction Probability crane 0.28 dock 0.20 Class Prediction Probability web_site 0.18 boathouse 0.10 Class Prediction Probability beacon 0.07 paddlewheel 0.08 Class Prediction Probability pier 0.06 pier 0.08 Class Prediction Probability envelope 0.04 wreck 0.07 Occlusion: 60% Class Prediction Probability web_site 0.78 dock 0.18 Class Prediction Probability crane 0.06 liner 0.16 Class Prediction Probability beacon 0.05 submarine 0.05 Class Prediction Probability seashore 0.01 crane 0.04 Class Prediction Probability envelope 0.01 container_ship 0.03 Occlusion: 80% Class Prediction Probability beacon 0.34 liner 0.07 Class Prediction Probability web_site 0.22 dock 0.03 Class Prediction Probability crane 0.07 aircraft_carrier 0.03 Class Prediction Probability seashore 0.06 container_ship 0.03 Class Prediction Probability liner 0.02 schooner 0.02 Figure 4. Examples of occluded ImageNet validation images and model predictions of ResNet-50. 224 × 224 validation images of ImageNet are occluded with randomly placed square patches that cover λ of the image area. λ is set to {0%, 20%, 40%, 60%, 80%} to create a range of occlusion levels. 0 20 40 60 80 Occlusion Level (%) 0 20 40 60 80Top-1 Accuracy (%) baseline RA SA RA+SA 0 20 40 60 80 Occlusion Level (%) 0 20 40 60 80Top-5 Accuracy (%) baseline RA SA RA+SA Figure 5. Soft Augmentation improves occlusion robustness of ResNet-50 on ImageNet. Both RandAugment (RA) and Soft Augmentation (SA) improve occlusion robustness independently. Combining RA with SA reduces Top-1 error by up to 17%. At 80% occlusion level, compared with baseline accuracy (3.42%), SA+RA achieves more than 4× accuracy (18.98%). Compared to previous single-model methods, our strong baseline WideResNet-28 models achieves lower top-1 er- ror at the cost of higher ECE. Combining Soft Augmen- tation with more recently developed augmentation policies such as TrivialAugment [37] (SA+TA) reduces top-1 error by 4.36% and reduces ECE by more than half on Cifar-100, outperforming the 4× more computationally expensive 5- ensemble model [25]. To the best of our knowledge, this is state of the art ECE performance for WideResNet-28 on Cifar without post-hoc calibration. 4.4. Soft Augmentation Boosts Self-Supervised Learning In contrast with supervised classification tasks where the learning target yi is usually a one-hot vector, many self-supervised methods such as SimSiam [6] and Barlow Twins [51] learn visual feature representations without class labels by encouraging augmentation invariant feature rep- resentations. This section investigates whether Soft Aug- Table 3. Soft Augmentation improves both accuracy and calibra- tion. We report mean and standard error of three WideResNet-28 runs per configuration (bottom two rows). On the more challeng- ing Cifar-100 benchmark, our Baseline already outperforms much of prior work in terms of Top-1 error, but has worse calibration er- ror (ECE). Applying Soft Augment + Trivial Augment (SA+TA) reduces Top-1 error by 4.36% and reduces ECE by more than half, outperforming even compute-heavy models such as the 5- Ensemble [25]. Similar trends hold for Cifar-10. Method Cifar-100 Cifar-10 Top-1 Error ECE Top-1 Error ECE Energy-based [31] 19.74 4.62 4.02 0.85 DUQ [45] – – 5.40 1.55 SNGP [30] 20.00 4.33 3.96 1.80 DDU [35] 19.02 4.10 4.03 0.85 5-Ensemble [25] 17.21 3.32 3.41 0.76 Our Baseline 18.60±0.16 4.86±0.10 3.67±0.07 2.22±0.03 SA+TA 14.24±0.11 1.76±0.15 2.23±0.06 0.61±0.10 mentation generalizes to learning settings where no one-hot style labeling is provided. In a typical setting, two random crops of the same image are fed into a pair of identical twin networks (e.g., ResNet- 18) with shared weights and architecture. The learning tar- get can be the maximization of similarity between the fea- ture representations of the two crops [6], or minimization of redundancy [51]. By default, all randomly cropped pairs have equal weights. We propose and test two alternative hypotheses for weight softening with SimSiam. To accom- modate self-supervised learning, Equation 7 is modified by replacing visibility vtx,ty with intersection over union IoU of two crops of an image: p = 1 −α(ϕ1, ϕ2, k) = 1 −(1 −pmin)(1 −IoUϕ1,ϕ2 )k, SA#1 (9) where ϕ1 = (tx1, ty1, w1, h1) and ϕ2 = (tx2, ty2, w2, h2) are crop parameters for the first and second sample in a pair.Table 4. Soft Augmentation (SA#1) improves self supervised learning with SimSiam (ResNet-18) on Cifar-100 by down- weighting sample pairs with small intersection over union (IoU), outperforming the opposite hypothesis (SA#2) of down-weighting pairs with large IoU. For each configuration, we report means and standard errors of 3 runs with best learning rates (LR) found for Cifar-100. The effect of SA#1 (with a fixed k = 4) generalizes to Cifar-10 without re-tuning. Task LR Baseline LR SA#1 LR SA#2 Cifar100 0.2 37.64±0.06 0.2 36.61±0.05 0.1 37.39±0.06 Cifar10 0.2 9.87±0.03 0.2 9.31±0.01 - - p is used to soften weights only as no one-hot classification vector is available in this learning setting. With this hypoth- esis (SA#1), “hard\" sample pairs with low IoUs are assigned low weights. Alternatively, one can assign lower weights to “easy\" sample pairs with higher IoUs (SA#2), as prescribed by Equation 10: p = 1 − α(ϕ1, ϕ2, k) = 1 − (1 − pmin)(IoUϕ1,ϕ2 )k. SA#2 (10) We first test all three hypotheses (baseline, SA#1, and SA#2) on Cifar-100 with the SimSiam-ResNet-18 models. Table 4 (top) shows that SA#1 outperform both baseline and SA#2 (details in Appendix B.4). Additional experiments show that models trained with the same SA#1 configuration also generalize to Cifar-10 (Table 4 bottom). 5. Discussion Other augmentations. While we focus on crop aug- mentations as an illustrative example, Soft Augmentation can be easily extended to a larger repertoire of transforms such as affine transforms and photometric distortions, as seen in the more sophisticated augmentation policies such as RandAugment. As the formulation of Equation 7 (and Figure 2 right) is directly inspired by the qualitative shape of human vision experiments from Tanget al. [44], optimal softening curves for other transforms may be discovered by similar human experiments. However, results with a sec- ond transform in Appendix Table 6 suggest that Equation 7 generalizes to additive noise augmentation as well. A po- tential challenge is determining the optimal softening strat- egy when a combination of several transforms are applied to an image since the cost of a naive grid search increases ex- ponentially with the number of hyperparameters. Perhaps reinforcement learning methods as seen in RandAugment can be used to speed up the search. Other tasks. While we limit the scope of Soft Augmen- tation to image classification as it is directly inspired by hu- man visual classification research, the idea can be general- ized to other types of tasks such as natural language mod- eling and object detection. Recent studies have shown that detection models benefit from soft learning targets in the fi- nal stages [3,27], Soft Augment has the potential to comple- ment these methods by modeling information loss of image transform in the models’ input stage. Class-dependant augmentations. As pointed out by Balestriero et al. [2], the effects of data augmentation are class-dependent. Thus assumption 3 of Equation 7 does not exactly hold. One can loosen it by adaptively determining the range of transform and softening curve on a per class or per sample basis. As shown in Equation 11, (xi, yi) ⇒ \u0000 tϕ∼S(xi,yi)(xi), gα(ϕ,xi,yi)(yi) \u0001 , (11) two adaptive improvements can be made: 1) the transforma- tion range S where ϕ is drawn from can be made a function of sample (xi, yi), 2) the softening factorα can also adapt to (xi, yi). Intuitively, the formulation recognizes the hetero- geneity of training samples of images at two levels. Firstly, the object of interest can occupy different proportions of an image. For instance, a high-resolution training image with a small object located at the center can allow more ag- gressive crop transforms without losing its class invariance. Secondly, texture and shape may contribute differently de- pending on the visual class. A heavily occluded tiger may be recognized solely by its distinctive stripes; in contrast, a minimally visible cloak can be mistaken as almost any clothing. 6. Conclusion In summary, we draw inspiration from human vision re- search, specifically how human visual classification perfor- mance degrades non-linearly as a function of image occlu- sion. We propose generalizing data augmentation with in- variant transforms to Soft Augmentation where the learning target (e.g. one-hot vector and/or sample weight) softens non-linearly as a function of the degree of the transform ap- plied to the sample. Using cropping transformations as an example, we em- pirically show that Soft Augmentation offers robust top-1 error reduction across Cifar-10, Cifar-100, ImageNet-1K, and ImageNet-V2. With a fixed softening curve, Soft Aug- mentation doubles the top-1 accuracy boost of the popular RandAugment method across models and datasets, and im- proves performance under occlusion by up to 4×. Combin- ing Soft Augment with the more recently developed Triv- ialAugment further improves model accuracy and calibra- tion simultaneously, outperforming even compute-heavy 5- ensemble models. Finally, self-supervised learning exper- iments demonstrate that Soft Augmentation also general- izes beyond the popular supervised one-hot classification setting.References [1] Devansh Arpit, Stanisław Jastrz˛ ebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In International conference on machine learning , pages 233– 242. PMLR, 2017. 3 [2] Randall Balestriero, Leon Bottou, and Yann LeCun. The effects of regularization and data augmentation are class de- pendent. arXiv preprint arXiv:2204.03632, 2022. 8 [3] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and Larry S Davis. Soft-nms–improving object detection with one line of code. In Proceedings of the IEEE international conference on computer vision, pages 5561–5569, 2017. 8 [4] Andy Brock, Soham De, Samuel L Smith, and Karen Si- monyan. High-performance large-scale image recognition without normalization. In International Conference on Ma- chine Learning, pages 1059–1071. PMLR, 2021. 2 [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan- tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan- guage models are few-shot learners. Advances in neural in- formation processing systems, 33:1877–1901, 2020. 1 [6] Xinlei Chen and Kaiming He. Exploring simple siamese rep- resentation learning. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 15750–15758, 2021. 7, 14 [7] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude- van, and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 113–123, 2019. 2, 5 [8] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmenta- tion with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 702–703, 2020. 1, 2, 5, 6, 12 [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. 5 [10] Terrance DeVries and Graham W Taylor. Improved regular- ization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. 5 [11] Terrance DeVries and Graham W Taylor. Improved regular- ization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017. 12 [12] Kunihiko Fukushima and Sei Miyake. Neocognitron: A self- organizing neural network model for a mechanism of visual pattern recognition. In Competition and cooperation in neu- ral nets, pages 267–285. Springer, 1982. 2 [13] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noord- huis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large mini- batch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017. 2 [14] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In International Conference on Machine Learning, pages 1321–1330. PMLR, 2017. 6 [15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 2 [16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European conference on computer vision , pages 630–645. Springer, 2016. 5, 11 [17] Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distill- ing the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015. 2 [18] David H Hubel and Torsten N Wiesel. Receptive fields of single neurones in the cat’s striate cortex. The Journal of physiology, 148(3):574, 1959. 2 [19] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. Advances in neural informa- tion processing systems, 28, 2015. 1 [20] Jang-Hyun Kim, Wonho Choo, Hosan Jeong, and Hyun Oh Song. Co-mixup: Saliency guided joint mixup with super- modular diversity. arXiv preprint arXiv:2102.03065, 2021. 5, 12 [21] Jang-Hyun Kim, Wonho Choo, and Hyun Oh Song. Puz- zle mix: Exploiting saliency and local statistics for optimal mixup. In International Conference on Machine Learning , pages 5275–5285. PMLR, 2020. 12 [22] Adam Kortylewski, Ju He, Qing Liu, and Alan L Yuille. Compositional convolutional neural networks: A deep archi- tecture with innate robustness to partial occlusion. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8940–8949, 2020. 2, 6 [23] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 3, 5 [24] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural net- works. Advances in neural information processing systems , 25, 2012. 2 [25] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estima- tion using deep ensembles. Advances in neural information processing systems, 30, 2017. 1, 6, 7 [26] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444, 2015. 2 [27] Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang, and Jian Yang. Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection. Advances in Neural Information Processing Systems, 33:21002–21012, 2020. 8 [28] Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. Advances in Neural Information Processing Systems, 32, 2019. 2 [29] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. In Pro- ceedings of the IEEE international conference on computer vision, pages 2980–2988, 2017. 5, 12[30] Jeremiah Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax Weiss, and Balaji Lakshminarayanan. Simple and principled uncertainty estimation with deterministic deep learning via distance awareness. Advances in Neural Infor- mation Processing Systems, 33:7498–7512, 2020. 6, 7 [31] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. Advances in Neural Information Processing Systems , 33:21464–21475, 2020. 7 [32] Yang Liu, Jeremy Bernstein, Markus Meister, and Yisong Yue. Learning by turning: Neural architecture aware optimi- sation. In International Conference on Machine Learning , pages 6748–6758. PMLR, 2021. 2 [33] Ilya Loshchilov and Frank Hutter. Sgdr: Stochas- tic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. 5 [34] David G Lowe. Object recognition from local scale-invariant features. In Proceedings of the seventh IEEE international conference on computer vision, volume 2, pages 1150–1157. Ieee, 1999. 2, 6 [35] Jishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort, Philip H. S. Torr, and Yarin Gal. Deep deterministic uncer- tainty: A simple baseline, 2021. 6, 7 [36] Rafael Müller, Simon Kornblith, and Geoffrey E Hinton. When does label smoothing help? Advances in neural in- formation processing systems, 32, 2019. 2 [37] Samuel G Müller and Frank Hutter. Trivialaugment: Tuning- free yet state-of-the-art data augmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, pages 774–782, 2021. 1, 2, 7 [38] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Rai- son, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An im- perative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Informa- tion Processing Systems 32, pages 8024–8035. Curran Asso- ciates, Inc., 2019. 6, 12 [39] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to im- agenet? In International Conference on Machine Learning, pages 5389–5400. PMLR, 2019. 5, 12 [40] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urta- sun. Learning to reweight examples for robust deep learn- ing. In International conference on machine learning, pages 4334–4343. PMLR, 2018. 3 [41] Andrew W Senior, Richard Evans, John Jumper, James Kirk- patrick, Laurent Sifre, Tim Green, Chongli Qin, Augustin Žídek, Alexander WR Nelson, Alex Bridgland, et al. Im- proved protein structure prediction using potentials from deep learning. Nature, 577(7792):706–710, 2020. 1 [42] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 1–9, 2015. 1, 2, 5 [43] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception archi- tecture for computer vision. In Proceedings of the IEEE con- ference on computer vision and pattern recognition , pages 2818–2826, 2016. 2, 4, 5 [44] Hanlin Tang, Martin Schrimpf, William Lotter, Charlotte Moerman, Ana Paredes, Josue Ortega Caro, Walter Hardesty, David Cox, and Gabriel Kreiman. Recurrent computations for visual pattern completion. Proceedings of the National Academy of Sciences, 115(35):8835–8840, 2018. 1, 3, 4, 5, 6, 8 [45] Joost Van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. Uncertainty estimation using a single deep de- terministic neural network. In International conference on machine learning, pages 9690–9700. PMLR, 2020. 6, 7 [46] Angtian Wang, Yihong Sun, Adam Kortylewski, and Alan L Yuille. Robust object detection under occlusion with context- aware compositionalnets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 12645–12654, 2020. 2, 6 [47] Dean Wyatte, Tim Curran, and Randall O’Reilly. The limits of feedforward vision: Recurrent processing promotes robust object recognition when objects are degraded. Journal of Cognitive Neuroscience, 24(11):2248–2261, 2012. 1, 2, 6 [48] Mingyang Yi, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu, and Zhi-Ming Ma. Reweighting augmented samples by minimizing the maximal expected loss. arXiv preprint arXiv:2103.08933, 2021. 3 [49] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu- larization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international con- ference on computer vision, pages 6023–6032, 2019. 2 [50] Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. arXiv preprint arXiv:1605.07146, 2016. 6, 11 [51] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In International Conference on Ma- chine Learning, pages 12310–12320. PMLR, 2021. 7 [52] Chang-Bin Zhang, Peng-Tao Jiang, Qibin Hou, Yunchao Wei, Qi Han, Zhen Li, and Ming-Ming Cheng. Delving deep into label smoothing. IEEE Transactions on Image Process- ing, 30:5984–5996, 2021. 5, 12 [53] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimiza- tion. arXiv preprint arXiv:1710.09412, 2017. 2, 5, 12 [54] Hongru Zhu, Peng Tang, Jeongho Park, Soojin Park, and Alan Yuille. Robustness of object recognition under ex- treme occlusion in humans and computational models.arXiv preprint arXiv:1905.04598, 2019. 1, 2, 6Appendix A. Implementation 1import torch 2 3class SoftCropAugmentation: 4def __init__(self, n_class, sigma=0.3, k=2): 5self.chance = 1/n_class 6self.sigma = sigma 7self.k = k 8 9def draw_offset(self, limit, sigma=0.3, n =100): 10# draw an integer from a (clipped) Gaussian 11for d in range(n): 12x = torch.randn((1))*sigma 13if abs(x) <= limit: 14return int(x) 15return int(0) 16 17def __call__(self, image, label): 18# typically, dim1 = dim2 = 32 for Cifar 19dim1, dim2 = image.size(1), image.size(2) 20# pad image 21image_padded = torch.zeros((3, dim1 * 3, dim2 * 3)) 22image_padded[:, dim1:2*dim1, dim2:2*dim2] = image 23# draw tx, ty 24tx = self.draw_offset(dim1, self. sigma_crop * dim1) 25ty = self.draw_offset(dim2, self. sigma_crop * dim2) 26# crop image 27left, right = tx + dim1, tx + dim1 * 2 28top, bottom = ty + dim2, ty + dim2 * 2 29new_image = image_padded[:, left: right, top: bottom] 30# compute transformed image visibility and confidence 31v = (dim1 - abs(tx)) * (dim2 - abs(ty)) / (dim1 * dim2) 32confidence = 1 - (1 - self.chance) * (1 - v) ** self.k 33return new_image, label, confidence Listing 1. Pytorch implementation of Soft Crop Augmentation for Cifar. 1import torch 2import torch.nn.functional as F 3 4def soft_target(pred, label, confidence): 5log_prob = F.log_softmax(pred, dim=1) 6n_class = pred.size(1) 7# make soft one-hot target 8one_hot = torch.ones_like(pred) * (1 - confidence) / (n_class - 1) 9one_hot.scatter_(dim=1, index=label, src= confidence) 10# compute weighted KL loss 11kl = confidence * F.kl_div(input=log_prob, 12target=one_hot, 13reduction=’none’). sum(-1) 14return kl.mean() Listing 2. Pytorch implementation of Soft Target loss function. Appendix B. Experiment Details Appendix B.1. Supervised Cifar-10/100 For Cifar-100 experiments, we train all ResNet-like models with a batch size 128 on a single Nvidia V100 16GB GPU on Amazon Web Services (AWS) and with an intial learning rate 0.1 with cosine learning rate decay over 500 epochs. EfficientNet-B0 is trained with an initial learning rate of 0.025, PyramidNet-272 is trained with 2 GPUs. We use the Conv-BatchNorm-ReLU configuration of ResNet models [16] and WideResNet-28 with a widening factor of 10 [50]. Horizontal flip is used in all experiments as it is considered a lossless transformation in the context of Ci- far images. We find decaying crop aggressiveness towards the end of training (e.g., last 20 epochs) by a large fac- tor (e.g., reducing σ by 1000×) marginally improve per- formance on Cifar-100, but slightly hurts performance on Cifar-10. Accordingly, we only apply σ decay for all Cifar- 100 experiments. A single run of ResNet-18, ResNet-50, and WideResNet-28 takes ∼ 2.5, ∼ 7, ∼ 9 GPU hours on Cifar-10/100, respectively. BL beaver 0.30 SA+TA seal 0.44 BL pear 0.43 SA+TA apple 0.79 BL television 0.12 SA+TA dinosaur 0.37 BL skyscraper 0.18 SA+TA castle 0.44 BL kangaroo 0.35 SA+TA rabbit 0.79 BL poppy 0.83 SA+TA beetle 0.27 BL poppy 0.74 SA+TA sunflower 0.89 BL clock 0.48 SA+TA ray 0.74 BL girl 0.51 SA+TA boy 0.83 BL crab 0.52 SA+TA crocodile 0.95 BL rabbit 0.56 SA+TA mouse 0.68 BL butterfly 0.28 SA+TA skunk 0.88 BL beetle 0.12 SA+TA flatfish 0.16 BL mouse 0.17 SA+TA lizard 0.98 BL forest 0.37 SA+TA pine_tree 0.56 BL skunk 0.68 SA+TA elephant 0.78 BL hamster 0.35 SA+TA raccoon 0.17 BL bee 0.84 SA+TA caterpillar 0.95 BL cattle 0.10 SA+TA kangaroo 0.31 BL cattle 0.24 SA+TA lion 0.89 Figure 6. Example images of the Cifar-100 validation set and pre- dictions of WideResNet-28. Predicted classes and confidence lev- els of models trained with Soft Augmentation + Trivial Augment (SA+TA) and baseline (BL) augmentation are reported. In many cases, SA+TA not only corrects the class prediction, but also im- proves the model confidence. For instance, BL mistakes “seal” for “beaver” (top-left, both classes belong to the same “aquatic mammal” superclass), and SA+TA makes a correct class predic- tion with higher confidence.Appendix B.2. Additional Results Table 5. Comparing SA with other methods. Recommended hyperparameters for Mixup [53], Cutout [11], and Online Label Smoothing [52]. α of Focal Loss is tuned as Lin et al. [29] did not prescribe an optimal α for Cifar classification. Top-1 errors of ResNet-18 on Cifar-100 are reported. ResNet-18 Top-1 Error Zhanget al. [53] Baseline 25.6 Mixup 21.1 Kimet al. [21] Baseline 23.67 Mixup 23.16 Manifold Mixup 20.98 Puzzle Mix 19.62 Kimet al. [20] Baseline 23.59 Mixup 22.43 Manifold Mixup 21.64 Puzzle Mix 20.62 Co-Mixup 19.87 Our Baseline 20.80±0.11 Label Smoothing 19.47±0.18 Online Label Smoothing20.12±0.05 Focal Loss (α= 1) 20.45±0.08 Focal Loss (α= 2) 20.38±0.08 Focal Loss (α= 5) 20.69±0.17 Mixup (α= 1.0) 19.88±0.38 Cutout (L= 8) 20.51±0.02 SA 18.31±0.17 RA 20.99±0.11 SA + RA 18.10±0.20 Table 6. Soft Augmentation with additive noise improves ResNet- 18 performance on Cifar-100. Given an image X and a random noise pattern Xnoise, and augmented image is given by Xaug = X + αXnoise, where α is drawn from N(0, 0.1) and pixel values of Xnoise are also independently drawn from N(0, 0.1). Apply- ing Soft Augmentation to additive noise boost performance over baseline as well as Soft Augmentation Crop + RandAugment. ResNet-18 Top-1 Error Baseline 20.80±0.11 RA 20.99±0.11 Hard Crop 20.26±0.12 SA-Crop (k=2) 18.31±0.17 Hard Noise 20.68±0.05 SA-Noise (k=1) 19.20±0.20 SA-Crop (k=2) + RA 18.10±0.20 SA-Noise (k=1) + SA-Crop (k=2) + RA17.87±0.17 Table 7. Soft Augmentation reduces expected calibration error (ECE) of ResNet-50 on ImageNet. Dataset Baseline RA SA RA+SA ImageNet-1K 5.11 4.09 3.17 2.78 ImageNet-V2 9.91 8.84 3.24 3.18 Appendix B.3. ImageNet All ImageNet-1k experiments are conducted with a batch size of 256 distributed across 4 Nvidia V100 16GB GPUs on AWS. The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 dataset (BSD 3- Clause License) is downloaded from the official website (https://www.image-net.org/). Horizontal flip is used in all experiments as an additional lossless base augmentation. The base learning rate is set to 0.1 with a 5-epoch linear warmup and cosine decay over 270 epochs. A single run of ResNet-50 training takes ∼ 4 × 4 = 16 GPU days and ImageNet experiments take a total of 600 GPU days. We use the official PyTorch [38] implementation of Ran- dAugment and ResNet-50/101 (BSD-style license) and run all experiments with the standard square input Linput = W = H = 224. Note that the original RandAugment [8] uses a larger input size of H = 224 , W = 244 , but our re-implemention improved top-1 error (22.02 vs 22.4) of ResNet-50 despite using a smaller input size. ImageNet-V2 is a validation set proposed by He et al. [39]. For training, the standard crop transform has 4 hy- perparameters: (scalemin, scalemax) define the range of the relative size of a cropped image to the original one, (ratiomin, ratiomax) determine lower and upper bound of the aspect ratio of the cropped patch before the final resize step. In practice, a scale is drawn from a uni- form distribution U(scalemin, scalemax), then the loga- rithm of the aspect ratio is drawn from a uniform dis- tribution U(log(ratiomin), log(ratiomax)). Default val- ues are scalemin = 0 .08, scalemax = 1 .0, ratiomin = 3/4, ratiomax = 4/3. Similar to our Cifar crop augmentation, we propose a simplified ImageNet crop augmentation with only 2 hy- perparameters σ, Lmin. First, we draw ∆w, ∆h from a clipped rectified normal distribution NR(0, σ(L − Lmin)) and get w = W − ∆w, h= H − ∆h Lmin is the mini- mum resolution of a cropped image and set to half of input resolution 224. tx, tyare then independently drawn from N(0, σ(W +w)), N(0, σ(H +h)). Note that we use a fixed set of intuitive values σ = 0.3, Lmin = 1/2Linput = 112 for all the experiments. For model validation, standard augmentation practice first resizes an image so that its short edge has length Linput = 256 , then a center 224 × 224 crop is applied. Note that Linput is an additional hyperparameter introduced by the test augmentation. In contrast, we simplify this by setting Linput to the final input size 224 and use this con- figuration for all ImageNet model evaluation.Occlusion: 0% Boston_bullBaselineSA+RA Class Prediction Probability Boston_bull 0.82 Boston_bull 0.65 Class Prediction Probability French_bulldog 0.09 French_bulldog 0.23 Class Prediction Probability toy_terrier 0.03 toy_terrier 0.01 Class Prediction Probability tennis_ball 0.01 Chihuahua 0.00 Class Prediction Probability Chihuahua 0.00 pug 0.00 Occlusion: 20% Class Prediction Probability Boston_bull 0.95 Boston_bull 0.86 Class Prediction Probability French_bulldog 0.05 French_bulldog 0.13 Class Prediction Probability toy_terrier 0.00 toy_terrier 0.00 Class Prediction Probability Chihuahua 0.00 Chihuahua 0.00 Class Prediction Probability tennis_ball 0.00 pug 0.00 Occlusion: 40% Class Prediction Probability French_bulldog 0.83 Boston_bull 0.96 Class Prediction Probability Boston_bull 0.17 French_bulldog 0.04 Class Prediction Probability Staffordshire_bullterrier 0.00 toy_terrier 0.00 Class Prediction Probability American_Staffordshire_terrier0.00 Italian_greyhound 0.00 Class Prediction Probability Great_Dane 0.00 Chihuahua 0.00 Occlusion: 60% Class Prediction Probability Boston_bull 0.97 Boston_bull 0.80 Class Prediction Probability French_bulldog 0.01 French_bulldog 0.02 Class Prediction Probability Chihuahua 0.01 Italian_greyhound 0.02 Class Prediction Probability whippet 0.00 toy_terrier 0.02 Class Prediction Probability toy_terrier 0.00 Chihuahua 0.01 Occlusion: 80% Class Prediction Probability French_bulldog 0.88 Boston_bull 0.13 Class Prediction Probability Boston_bull 0.06 French_bulldog 0.06 Class Prediction Probability Chihuahua 0.01 Italian_greyhound 0.03 Class Prediction Probability pug 0.01 miniature_pinscher 0.02 Class Prediction Probability Staffordshire_bullterrier 0.01 Staffordshire_bullterrier 0.01 Occlusion: 0% papillonBaselineSA+RA Class Prediction Probability papillon 1.00 papillon 0.97 Class Prediction Probability Chihuahua 0.00 Chihuahua 0.00 Class Prediction Probability Japanese_spaniel 0.00 Japanese_spaniel 0.00 Class Prediction Probability toy_terrier 0.00 toy_terrier 0.00 Class Prediction Probability Pomeranian 0.00 Pomeranian 0.00 Occlusion: 20% Class Prediction Probability papillon 0.74 papillon 0.76 Class Prediction Probability Blenheim_spaniel 0.09 Blenheim_spaniel 0.03 Class Prediction Probability clumber 0.06 Japanese_spaniel 0.03 Class Prediction Probability Japanese_spaniel 0.05 Chihuahua 0.02 Class Prediction Probability Welsh_springer_spaniel 0.02 Pomeranian 0.01 Occlusion: 40% Class Prediction Probability papillon 0.29 Blenheim_spaniel 0.15 Class Prediction Probability Blenheim_spaniel 0.19 papillon 0.13 Class Prediction Probability Welsh_springer_spaniel 0.14 Welsh_springer_spaniel 0.13 Class Prediction Probability collie 0.11 Brittany_spaniel 0.05 Class Prediction Probability Brittany_spaniel 0.05 hare 0.02 Occlusion: 60% Class Prediction Probability envelope 0.09 bustard 0.04 Class Prediction Probability Indian_cobra 0.03 hare 0.02 Class Prediction Probability hognose_snake 0.02 golf_ball 0.02 Class Prediction Probability web_site 0.02 partridge 0.02 Class Prediction Probability dhole 0.01 kit_fox 0.02 Occlusion: 80% Class Prediction Probability envelope 0.15 kit_fox 0.01 Class Prediction Probability web_site 0.08 hare 0.01 Class Prediction Probability dhole 0.04 bustard 0.01 Class Prediction Probability red_fox 0.01 partridge 0.01 Class Prediction Probability honeycomb 0.01 bittern 0.01 Occlusion: 0% vending_machineBaselineSA+RA Class Prediction Probability vending_machine 0.65 vending_machine 0.98 Class Prediction Probability streetcar 0.04 streetcar 0.00 Class Prediction Probability refrigerator 0.02 refrigerator 0.00 Class Prediction Probability minibus 0.01 pop_bottle 0.00 Class Prediction Probability grocery_store 0.01 ambulance 0.00 Occlusion: 20% Class Prediction Probability vending_machine 0.39 vending_machine 0.83 Class Prediction Probability moving_van 0.12 streetcar 0.04 Class Prediction Probability refrigerator 0.12 moving_van 0.01 Class Prediction Probability web_site 0.05 refrigerator 0.01 Class Prediction Probability monitor 0.04 trolleybus 0.01 Occlusion: 40% Class Prediction Probability vending_machine 0.88 vending_machine 0.93 Class Prediction Probability refrigerator 0.03 refrigerator 0.01 Class Prediction Probability web_site 0.01 pay-phone 0.00 Class Prediction Probability desktop_computer 0.00 slot 0.00 Class Prediction Probability monitor 0.00 streetcar 0.00 Occlusion: 60% Class Prediction Probability screen 0.26 streetcar 0.03 Class Prediction Probability monitor 0.18 garbage_truck 0.02 Class Prediction Probability home_theater 0.14 parking_meter 0.01 Class Prediction Probability television 0.04 cab 0.01 Class Prediction Probability desktop_computer 0.04 trolleybus 0.01 Occlusion: 80% Class Prediction Probability home_theater 0.59 sliding_door 0.04 Class Prediction Probability monitor 0.13 refrigerator 0.02 Class Prediction Probability entertainment_center 0.05 vending_machine 0.02 Class Prediction Probability television 0.05 pay-phone 0.01 Class Prediction Probability screen 0.04 barbershop 0.01 Occlusion: 0% pirateBaselineSA+RA Class Prediction Probability pirate 1.00 pirate 0.93 Class Prediction Probability schooner 0.00 fireboat 0.01 Class Prediction Probability dock 0.00 dock 0.01 Class Prediction Probability fireboat 0.00 schooner 0.01 Class Prediction Probability crane 0.00 amphibian 0.00 Occlusion: 20% Class Prediction Probability pirate 0.98 pirate 0.76 Class Prediction Probability schooner 0.02 schooner 0.03 Class Prediction Probability dock 0.00 cannon 0.03 Class Prediction Probability stupa 0.00 dock 0.01 Class Prediction Probability crane 0.00 fireboat 0.01 Occlusion: 40% Class Prediction Probability crane 0.28 pirate 0.72 Class Prediction Probability pirate 0.22 schooner 0.04 Class Prediction Probability moving_van 0.12 fireboat 0.03 Class Prediction Probability schooner 0.05 dock 0.01 Class Prediction Probability scoreboard 0.03 drilling_platform 0.01 Occlusion: 60% Class Prediction Probability crane 0.43 pirate 0.26 Class Prediction Probability bookshop 0.08 schooner 0.03 Class Prediction Probability scoreboard 0.07 toyshop 0.03 Class Prediction Probability schooner 0.05 suspension_bridge 0.02 Class Prediction Probability drilling_platform 0.03 bookshop 0.02 Occlusion: 80% Class Prediction Probability pole 0.10 toyshop 0.03 Class Prediction Probability book_jacket 0.09 carousel 0.02 Class Prediction Probability comic_book 0.09 shoe_shop 0.02 Class Prediction Probability envelope 0.07 bookshop 0.01 Class Prediction Probability binder 0.06 totem_pole 0.01 Occlusion: 0% military_uniformBaselineSA+RA Class Prediction Probability military_uniform 0.74 military_uniform 0.70 Class Prediction Probability mortarboard 0.07 suit 0.06 Class Prediction Probability suit 0.05 bow_tie 0.02 Class Prediction Probability academic_gown 0.03 crutch 0.02 Class Prediction Probability cornet 0.03 groom 0.01 Occlusion: 20% Class Prediction Probability suit 0.19 crutch 0.23 Class Prediction Probability mortarboard 0.09 suit 0.14 Class Prediction Probability military_uniform 0.06 groom 0.09 Class Prediction Probability notebook 0.04 military_uniform 0.04 Class Prediction Probability lab_coat 0.04 turnstile 0.02 Occlusion: 40% Class Prediction Probability military_uniform 0.22 military_uniform 0.22 Class Prediction Probability lab_coat 0.07 projectile 0.04 Class Prediction Probability file 0.07 warplane 0.02 Class Prediction Probability bearskin 0.05 missile 0.02 Class Prediction Probability suit 0.05 grand_piano 0.01 Occlusion: 60% Class Prediction Probability military_uniform 0.11 military_uniform 0.14 Class Prediction Probability suit 0.10 lab_coat 0.02 Class Prediction Probability envelope 0.06 cowboy_hat 0.02 Class Prediction Probability kimono 0.06 rifle 0.02 Class Prediction Probability abaya 0.05 trombone 0.02 Occlusion: 80% Class Prediction Probability abaya 0.15 crutch 0.04 Class Prediction Probability space_heater 0.13 mortarboard 0.01 Class Prediction Probability web_site 0.13 academic_gown 0.01 Class Prediction Probability window_shade 0.12 trombone 0.01 Class Prediction Probability shower_curtain 0.04 shoe_shop 0.01 Figure 7. Examples of occluded ImageNet validation images and model predictions of ResNet-50.Appendix B.4. Self-Supervised Cifar-10/100 Self-supervised SimSiam experiments are run on a sin- gle Nvidia A6000 GPU. We follow the standard two-step training recipe [6]. 1) We first train the Siamese network in a self-supervised manner to learn visual features for 500 epochs with a cosine decay schedule and a batch size of 512. We apply Soft Augmentation only during this step. 2) The linear layer is then tuned with ground-truth labels for 100 epochs with an initial learning rate of 10 and 10× de- cay at epochs 60 and 80. Following [6], we set scalemin = 0.2, scalemax = 1 .0, ratiomin = 3 /4, ratiomax = 4 /3. Since down-weighting training samples in a batch effec- tively reduces learning rate and SimSiam is sensitive to it, we normalized the weight in a batch so that the mean re- mains 1 and re-tuned the learning rate (Table 8). Table 8. Soft Augmentation improve self supervised learning with SimSiam. Mean ± standard error of top-1 validation errors of three runs of ResNet-18 are reported. Task lr baseline SA#1 ∆#1 SA#2 ∆#2 Cifar100 0.1 39.50±0.13 40.21±0.03 +0.71 37 .39±0.06 −2.11 0.2 37.64±0.06 36.61±0.05 −1.03 39 .20±0.42 +1.56 0.4 40.28±2.49 37.68±0.06 −2.60 Diverged - 0.5 43.26±3.03 41.94±0.04 −1.32 Diverged - 0.8 78.88±9.05 55.44±4.15 −23.44 Diverged - Cifar10 0.2 9.87±0.03 9.31±0.01 −0.56 - - Table 9. SimSiam k tuning on Cifar-100 (single run) learning rate k Top-1 Error 0.2 1 37.78 2 37.27 3 36.34 4 36.31 Appendix C. Effects of Target Smoothing and Loss Reweighting on Loss Func- tions Consider the KL divergence loss of a single learning sample with a one-hot ground truth vector ytrue, and the softmax prediction vector of a model is denoted by ypred: L(ypred, ytrue) = w ∗ DKL(ytrue||ypred) =w ∗ NX n=1 ytrue n ∗ log(ytrue n ypred n ), (12) let n∗ be the ground truth class of an N-class classifica- tion task, Equation 12 can be re-written as: L(ypred, ytrue) = −w ∗ ytrue n∗ ∗ log(ypred n∗ ) + w ∗  ytrue n∗ ∗ log(ytrue n∗ ) + X n̸=n∗ ytrue n ∗ log(ytrue n ypred n )  . (13) In the case of hard one-hot ground truth target where ytrue n∗ = 1 and ytrue n = 0, n̸= n∗, with the default weight w = 1 it degenerates to cross entropy loss: L(ypred, ytrue) = −log(ypred n∗ ), (14) Now we apply label smoothing style softening to the one-hot target ytrue so that ytrue n∗ = p and ytrue n = (1 − p)/(N − 1) = q, n̸= n∗: L(ypred, ytrue) = −p ∗ log(ypred n∗ ) +  p ∗ log(p) + X n̸=n∗ q ∗ log( q ypred n )  . (15) If q is not distributed, and ytrue n = 0, n̸= n∗ (This con- figuration does not correspond to any of our experiments): L(ypred, ytrue) = −p ∗ log(ypred n∗ ) + p ∗ log(p), (16) When only weight w is softened to w = p: L(ypred, ytrue) = −p ∗ log(ypred n∗ ). (17) Note that p is not a function of model weights, so when we take the derivative w.r.t. model weights to compute gra- dient, Equations 16 and 17 yield the same gradient. When both the one-hot label and weight are softened with p: L(ypred, ytrue) = −p2 ∗ log(ypred n∗ ) + p ∗  p ∗ log(p) + X n̸=n∗ q ∗ log( q ypred n )  . (18) The three types of softening in Section 4 are unique as suggested by Equations 15, 17, and 18.",
      "references": [
        "A closer look at memorization in deep networks.",
        "The effects of regularization and data augmentation are class dependent.",
        "Soft-nms–improving object detection with one line of code.",
        "High-performance large-scale image recognition without normalization.",
        "Language models are few-shot learners.",
        "Exploring simple siamese representation learning.",
        "Autoaugment: Learning augmentation strategies from data.",
        "Randaugment: Practical automated data augmentation with a reduced search space.",
        "Imagenet: A large-scale hierarchical image database.",
        "Improved regularization of convolutional neural networks with cutout.",
        "Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition.",
        "Accurate, large mini-batch sgd: Training imagenet in 1 hour.",
        "On calibration of modern neural networks.",
        "Deep residual learning for image recognition.",
        "Identity mappings in deep residual networks.",
        "Distilling the knowledge in a neural network.",
        "Receptive fields of single neurones in the cat’s striate cortex.",
        "Spatial transformer networks.",
        "Co-mixup: Saliency guided joint mixup with super-modular diversity.",
        "Puzzle mix: Exploiting saliency and local statistics for optimal mixup.",
        "Compositional convolutional neural networks: A deep architecture with innate robustness to partial occlusion.",
        "Learning multiple layers of features from tiny images.",
        "Imagenet classification with deep convolutional neural networks.",
        "Simple and scalable predictive uncertainty estimation using deep ensembles.",
        "Deep learning.",
        "Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection.",
        "Fast autoaugment.",
        "Focal loss for dense object detection.",
        "Simple and principled uncertainty estimation with deterministic deep learning via distance awareness.",
        "Energy-based out-of-distribution detection.",
        "Learning by turning: Neural architecture aware optimization.",
        "Sgdr: Stochastic gradient descent with warm restarts.",
        "Object recognition from local scale-invariant features.",
        "Deep deterministic uncertainty: A simple baseline.",
        "When does label smoothing help?",
        "Trivialaugment: Tuning-free yet state-of-the-art data augmentation.",
        "Pytorch: An imperative style, high-performance deep learning library.",
        "Do imagenet classifiers generalize to imagenet?",
        "Learning to reweight examples for robust deep learning.",
        "Improved protein structure prediction using potentials from deep learning.",
        "Going deeper with convolutions.",
        "Rethinking the inception architecture for computer vision.",
        "Recurrent computations for visual pattern completion.",
        "Uncertainty estimation using a single deep deterministic neural network.",
        "Robust object detection under occlusion with context-aware compositionalnets.",
        "The limits of feedforward vision: Recurrent processing promotes robust object recognition when objects are degraded.",
        "Reweighting augmented samples by minimizing the maximal expected loss.",
        "Cutmix: Regularization strategy to train strong classifiers with localizable features.",
        "Wide residual networks.",
        "Barlow twins: Self-supervised learning via redundancy reduction.",
        "Delving deep into label smoothing.",
        "mixup: Beyond empirical risk minimization.",
        "Robustness of object recognition under extreme occlusion in humans and computational models."
      ],
      "meta_data": {
        "arxiv_id": "2211.04625v2",
        "authors": [
          "Yang Liu",
          "Shen Yan",
          "Laura Leal-Taixé",
          "James Hays",
          "Deva Ramanan"
        ],
        "published_date": "2022-11-09T01:04:06Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces Soft Augmentation, a novel approach that generalizes traditional data augmentation by softening learning targets in a non-linear manner based on the degree of image transformation (e.g., aggressive cropping or occlusion). This method not only allows for more aggressive augmentation policies but also significantly improves top-1 accuracy, occlusion robustness, and model calibration. It demonstrates improvements over standard methods like RandAugment and shows generalization to self-supervised learning.",
        "methodology": "The authors propose an adaptive label softening strategy where the confidence of the ground truth label is reduced as a function of the transformation severity. A power function is used to map image visibility to target confidence (p = 1 - (1 - pmin)*(1 - visibility)^k). The approach includes softening of both the target labels and the sample weights, and is applied to various augmentation types like cropping and additive noise. This methodology is integrated with existing augmentation techniques and validated across multiple network architectures.",
        "experimental_setup": "Experiments were conducted on several benchmark datasets including Cifar-10, Cifar-100, ImageNet-1K, and ImageNet-V2 using models such as ResNet-18, ResNet-50, WideResNet-28, and EfficientNet-B0. The setup includes comparisons with other augmentation techniques like RandAugment, TrivialAugment, Mixup, Cutout, and label smoothing. Metrics such as Top-1 error reduction, Expected Calibration Error (ECE), and occlusion robustness (evaluated via artificially occluded images) were used to validate the effectiveness of the proposed method.",
        "limitations": "The method relies on several assumptions: (1) that the information loss due to occlusion can be accurately estimated by a single visibility-to-confidence curve; (2) that all images have a 100% confident ground truth label, ignoring intrinsic label uncertainty; and (3) that a fixed softening curve applies uniformly across different image classes and augmentations. Tuning multiple hyperparameters (e.g., sigma and k) can be challenging and the approach may require adaptation for class-specific or multi-transform scenarios.",
        "future_research_directions": "Future work could extend Soft Augmentation to other types of transformations such as affine and photometric distortions, and adapt the softening strategy to be class- or sample-specific. There is also potential for applying this concept to other tasks like object detection and natural language processing. Furthermore, integrating reinforcement learning to efficiently search for optimal softening parameters when combining multiple augmentation transforms presents a promising research avenue.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "NOPE: Novel Object Pose Estimation from a Single Image",
      "full_text": "NOPE: Novel Object Pose Estimation from a Single Image Van Nguyen Nguyen1, Thibault Groueix2, Georgy Ponimatkin1, Yinlin Hu3, Renaud Marlet1,4, Mathieu Salzmann5, Vincent Lepetit1 1LIGM, Ecole des Ponts, 2Adobe, 3MagicLeap, 4Valeo.ai, 5EPFL Reference Query Predicted pose Pose distribution Reference Query Predicted pose Pose distribution Figure 1. Given as input a single reference view of a novel object, our method predicts the relative 3D pose (rotation) of a query view and its ambiguities. We visualize the predicted pose by rendering the object from this pose, but the 3D model is only used for visualization purposes, not as input to our method.Our method works by estimating a probability distribution over the space of 3D poses, visualized here on a sphere centered on the object. We use the canonical pose of the 3D model to visualize this distribution, but not as input to our method.From this distribution, we can also identify the pose ambiguities: For example, in the case of the bottle, any pose with the same pitch and roll is possible; in the case of the mug, a range of poses are possible as the handle is not visible in the query image. Our method is also robust to partial occlusions, as shown on the clock hidden in part by a rectangle in the query image. Abstract The practicality of 3D object pose estimation remains limited for many applications due to the need for prior knowledge of a 3D model and a training period for new objects. To address this limitation, we propose an approach that takes a single image of a new object as input and pre- dicts the relative pose of this object in new images without prior knowledge of the object’s 3D model and without re- quiring training time for new objects and categories. We achieve this by training a model to directly predict discrim- inative embeddings for viewpoints surrounding the object. This prediction is done using a simple U-Net architecture with attention and conditioned on the desired pose, which yields extremely fast inference. We compare our approach to state-of-the-art methods and show it outperforms them both in terms of accuracy and robustness. 1. Introduction Estimating the 3D pose of objects has seen significant progress in the past decade with regard to both robustness and accuracy [12, 16, 37, 50, 58]. Specifically, there has been a considerable increase in robustness to partial occlu- sions [8, 33, 34], and the need for large amounts of real an- notated training images has been relaxed through the use of domain transfer [1], domain randomization [14, 20, 47, 51], and self-supervised learning techniques [49] that leverage synthetic images for training. Unfortunately, the practicality of 3D object pose esti- mation remains limited for many applications, including robotics and augmented reality. Typically, existing ap- proaches require a 3D model [15, 31, 32, 55], a video se- quence [5, 46], or sparse multiple images of the target ob- 1 arXiv:2303.13612v2  [cs.CV]  29 Mar 2024ject [59], and a training stage. Several techniques aim to prevent the need for retraining by assuming that new ob- jects fall into a recognized category [4, 53], share similari- ties with the previously trained examples as in the T-LESS dataset [47], or exhibit noticeable corners [35]. In this paper, we introduce an approach, which we call NOPE for Novel Object Pose Estimation, that only requires a single image of the new object to predict the relative pose of this object in any new images, without the need for the object’s 3D model and without training on the new object. This is a very challenging task, as, by contrast with the mul- tiple views used in [46, 59] for example, a single view only provides limited information about the object’s geometry. To achieve this, we train NOPE to predict the appearance of the object under novel views. We use these predictions as ‘templates’ annotated with the corresponding poses. Match- ing these templates with new input views lets us estimate the object relative pose with respect to the initial view. This approach is motivated by the good performance of recent related work [32, 44]. In particular, [32] showed that tem- plate matching can be extremely fast and robust to partial occlusions. This contrasts with methods that rely on a deep network to predict the probability of a pose [59]. Since our method relies on predicting the appearance of the target object, it relates to recent developments in novel view synthesis. However, it has two critical differences: The first difference is that instead of predicting color im- ages, we directly predict discriminative embeddings of the views. These embeddings are extracted by passing the in- put image through a U-Net architecture with attention and conditioned on the desired pose for the new view. The second main difference of our approach with novel view synthesis is more fundamental. We first note that gen- erating novel views given a single view of an object is am- biguous. Novel view synthesis usually focuses on generat- ing a single possible image for a given point of view. This is however not suitable for our purpose: The view synthesis method will “invent” the parts that were not visible in the input view. As illustrated in Figure 2, these invented parts create a plausible novel view but there is no guarantee this view actually corresponds to the actual view. For our goal of pose estimation, the invented parts will not match in gen- eral the query view and this will result in incorrect pose es- timation. The limitations of using novel view synthesis for pose estimation will further be quantitatively demonstrated in our experiments (see Table 1). Our approach to handling the ambiguities in novel view synthesis for template matching is to consider the distribu- tion of all the possible appearances of the object for the tar- get viewpoint. More exactly, we train NOPE to predict the average of all the possible appearances of the object. We then treat the predicted average as a template: Under some simple assumptions, the distance between this template and the query view is directly related to the probability of the query view to be a sample from the distribution of the pos- sible appearances of the object. This approach allows us to Generated Recovered view from pose by Estimated the query template pose Reference Query GT pose matching distribution Figure 2. The limit of novel view synthesis for pose predic- tion. While the images generated by Wonder3D [21] look very realistic, they have to invent unseen parts, impairing the similarity computation between the query image and the generated view, and hence the pose estimation: The probability distributions computed by template matching do not peak on the right pose but show many wrong local maxima. This is not a limitation of Wonder3D but of view synthesis from a single view in general. deal with the ambiguities of novel view prediction in a ro- bust and efficient way: Predicting the average views is just a direct inference of NOPE and is thus very fast, and robust to partial occlusions thank to template-matching. Furthermore, our approach can identify the pose ambigu- ities due, for example, to symmetries [23], even if we do not have access to the object 3D model but only to a single view. To this end, we estimate the distribution over all poses for the query, which becomes increasingly less peaked as the pose suffers from increasingly many ambiguities. Figure 1 depicts a variety of ambiguous and unambiguous cases with their pose distributions. In summary, our main contribution is to show we can ef- ficiently and reliably recover the relative pose of an unseen object in novel views given only a single view of that object as reference. To the best of our knowledge, our approach is the first to predict ambiguities due to symmetries and partial occlusions of unseen objects from only a single view. 2. Related Work In this section, we first review various approaches to novel view synthesis. We then shift our focus to pose es- timation techniques that aim to achieve generalization. 2.1. Novel view synthesis from a single image Our method generates discriminative feature views, which are conditioned on a reference view and the rela- tive pose between the views. This relates to the pioneering work of NeRFs [28] since it performs novel-view synthe- sis. While recent advancements have improved the speed of NeRFs [29, 43, 56], our approach is still orders of mag- nitude faster as it does not require the creation of a full 3D volumetric model. Furthermore, our approach only requires a single input view, whereas a typical NeRF setup necessi- 2tates around 50 views. Reducing the number of views re- quired for NeRF reconstruction remains an active research area, especially in the single-view scenario [27, 57]. Recent works [27, 62] have had successes generating novel views via NERFs using a sparse set of views as in- put by leveraging 2D diffusion models. For images, the breakthrough in diffusion models [6,45] have unlocked sev- eral workflows [39, 41, 42]. For 3D applications, DreamFu- sion [36] pioneered a score-distillation sampling that allows for the use of a 2D diffusion model as an image-based loss, leveraged by 3D applications via differentiable rendering. This has resulted in significant improvements for tasks pre- viously trained with a CLIP-based image loss [9, 10, 13, 17, 38, 52]. By building on top of score-distillation sampling, SparseFusion [62] reconstructs a NeRF scene with as few as two views with relative pose, while the concurrent work RealFusion [27] does it from a single input view, although the reconstruction time is impractical for real-time applica- tions. Our approach is much faster as we do not create a 3D representation of the object. Closest to us, 3DiM [54] and Zero-1-to-3 [18] gener- ate novel views of an object by conditioning a diffusion model on the pose. Instead of leveraging foundation diffu- sion models in 2D like DreamFusion [36] does, they retrain a diffusion model specifically for this task. While they have not applied their approach to template-based pose estima- tion, we design such a baseline and compare against it. We find that the diffusion model tends to change the texture or invent wrong details which hinders the performance of the template-based approach. In contrast, our approach gen- erates average novel views directly in an embedding space instead of a pixel space, which is much more efficient [32]. Finally, several methods [25,26] generate novel views by conditioning a feed-forward neural network on the 3D pose, which we also do with a U-Net. We share with these meth- ods an advantage in speed: such feed-forward neural net- work are one or two orders of magnitude faster than current diffusion models. However, the way we perform pose esti- mation is fundamentally different. We use novel-view syn- thesis in a template-based matching approach [32], while they use it in a regression-based optimization. In practice, we found these methods to work well on a limited number of object categories, and we observed their performance to deteriorate significantly when testing on novel categories. 2.2. Generalizable object pose estimation Several techniques have been explored to generalize bet- ter to unseen object pose estimation, such as generic 2D-3D correspondences [35], an energy-based strategy [59], key- point matching [46], or template matching [15, 19, 31, 32, 44, 60]. Despite significant progress, these methods either need an accurate 3D model of the target or they rely on multiple annotated reference images from different view- points. These 3D annotations are challenging to obtain in practice. By contrast, we propose a strategy that works with neither the 3D model of the target nor the annotation of mul- tiple views. More importantly, our method predicts accurate poses with only a single reference image, and generalizes to novel objects without retraining. 3. Method In this section, we first introduce our formalism, then describe our architecture and how we train it, and finally how we use it for pose prediction and for identifying pose ambiguities. 3.1. Formalization Given a reference imageIr of a target object and a query image Iq of the same object, we would like to estimate the probability p(∆R | Ir, Iq) that the relative motion between Ir and Iq is a certain discretized relative pose ∆R. We as- sume that this probability follows a normal distribution in the embedding space of the images: p(∆R | Ir, Iq) = N(eq | e(er, ∆R), Σ(er, ∆R)) , (1) where eq and er are the embeddings for query image Iq and reference image Ir respectively, e(er, ∆R) is the mean of the normal distribution, and Σ(er, ∆R) its covariance. This approach allows us to handle the fact that the object can have various appearances from viewpoint∆R given the reference image, as discussed in the introduction. We take the mean e(er, ∆R) as the average embedding for the appearance of the object from pose ∆R over the possible 3D shapes for the object: e(er, ∆R) = Z M e(∆R, M)p(M|er)dM , (2) with M a 3D model of testing object and e(∆R, M) the image embedding of same object under pose ∆R. e(er, ∆R) may look complicated to compute, but it is in fact easy to train a deep network to predict it using the L2 loss: X (e1,e2,∆R) ∥F(er, ∆R) − e2∥2 . (3) F denotes the network, (e1, e2, ∆R) is a training sample where e1 is the embedding for a view of a training object and e2 the embedding for the view of the same object after pose change ∆R. During training, given enough samples, F(er, ∆R) will converge naturally towards e(er, ∆R). 3.2. Framework Figure 3 gives an overview of our approach. We train a deep architecture to predict the average embeddings of novel views of an object using pairs of images of objects and the corresponding pose changes from a first set of ob- ject categories. In practice, we consider embeddings com- puted from the pretrained V AE of [40], as it was shown to be robust for template matching. To generate these embed- dings, we use a U-Net-like network with a pose condition- ing mechanism that is very close to the one of 3DiM [54]. 3Figure 3. Overview. During training, we train a U-Net to predict the embedding of a novel view of an object, given a reference image of the object and a relative pose. The U-Net is conditioned on an embedding of the relative pose computed using an MLP, which we train jointly with the U-Net. At inference, our method first takes as input a reference image of a new object and predicts the embeddings of views of the object under many relative poses. This inference takes around 1 second on a single GPU V100. Then, given a query image of the object, we first compute its embedding and match it against the set of predicted embeddings. This gives us a distribution over the possible relative poses between the reference and query images, where the maximum corresponds to the predicted pose. More precisely, we first use an MLP to convert the de- sired relative viewpoint ∆R with respect to the object pose in the reference view to a pose embedding. We then in- tegrate this pose embedding into the feature map at every stage of our U-Net using cross-attention, as in [40]. Training. At each iteration, we build a batch composed of N pairs of images, a reference image and another image of the same object with a known relative pose. The U-Net model takes as input the embedding of the reference image and as conditioning the embedding of the relative pose to predict an embedding for the second image. We jointly op- timize the U-Net and the MLP by minimizing the Euclidean distance between this predicted embedding and the embed- ding of the query image. Note that we freeze the pretrained V AE network of [40] during the training. By training it on a dataset of diverse objects, this archi- tecture generalizes well to novel unseen object categories. Interestingly, our method does not explicitly learn any sym- metries during training, but it is able to detect pose ambigu- ities during testing as discussed below. 3.3. Pose prediction Template matching. Once our architecture is trained, we can use it to generate the embeddings for novel views: Given a reference image Ir and a set of N relative view- points P = (∆R1, ∆R2, . . . ,∆RN ), we can obtain a cor- responding set of predicted embeddings (e1, e2, . . . ,eN ). To define these viewpoints, we follow the approach used in [32]: We start with a regular icosahedron and subdivide each triangle recursively into four smaller triangles twice to get 342 final viewpoints. Finally, we simply perform a near- est neighbor search to determine the reference point that has the embedding closest to the embedding of the query image. Detecting pose ambiguities. Pose ambiguities arise when the object has symmetries or when an object part that could remove the ambiguity is not visible, as for the mug in Fig- ure 1. By considering the distance between the embedding of the query image and the generated embeddings, we not only can predict a single pose but also identify all the other poses that are possible given the reference and query views. This can be done simply by relying on the normal distri- bution introduced in Eq. (1): log p(∆R | Ir, Iq) ∝ ∥F(er, ∆R) − eq∥2 . (4) To illustrate this, we show in Figure 4 three distinct types of symmetry and visualize the pose distribution for corre- sponding pairs of reference and query images (not shown). The number of regions with high similarity scores is consis- tent with the number of symmetries and pose ambiguities: If an object has no symmetry, the probability distribution has 4No symmetry 90-symmetry 180-symmetry Circular symmetry Figure 4. Object symmetries and the pose ambiguities they may generate, as estimated by our method given a pair of ref- erence and query images. a clear mode. The probability distribution for objects with symmetries have typically several modes or even a continu- ous high-probability region in case of rotational symmetry. We provide additional qualitative results in Section 4. 4. Experiments In this section, we first describe our experimental setup in Section 4.1. We then compare our method to others [24, 25, 30, 32, 47, 54] on both synthetic and real-world datasets in Section 4.2. Section 4.3 reports an evaluation of the robustness to partial occlusions. We provide the run- time in Section 4.4. Finally, we discuss failure cases in Sec- tion 4.5. An ablation study is provided in the supp. mat. 4.1. Experimental setup To the best of our knowledge, we are the first method ad- dressing the problem of object pose estimation from a single image when the object belongs to a category not seen during training: PIZZA [30] evaluated on the DeepIM refinement benchmark, which is made of pairs of images with a small relative pose; SSVE [25] and ViewNet [24] evaluated only on objects from categories seen during training. We there- fore had to create a new benchmark to evaluate our method. Synthetic dataset. We created a dataset as in FORGE [11] using the same ShapeNet [2] object cat- egories. For the training set, we randomly select 1000 object instances from each of the 13 categories as done in FORGE ( airplane, bench, cabinet, car, chair, display, lamp, loudspeaker, rifle, sofa, table, telephone, and vessel), resulting in a total of 13,000 instances. We build two separate test sets for evaluation. The first test set is the “novel instances” set, which contains 50 new instances for each training category. The second test set is the “novel category” set, which includes 100 models per category for the 10 unseen categories selected by FORGE ( bus, guitar, clock, bottle, train, mug, washer, skateboard, dishwasher, and pistol). For each 3D model, we randomly select camera poses to produce five reference images and five query images. We use BlenderProc [3] as rendering engine. Figure 5 illustrates the categories used for training our ar- chitecture and the categories used for testing it. The shapes and appearances of the categories in the test set are very different from the shapes and appearances of the categories in the training set, and thus constitute a good test set for generalization to unseen categories. Real-world dataset. We evaluate on the T-LESS dataset [7] following the evaluation protocol of [47]: we train only on objects 1-18 and test on the full PrimeSense test set using the ground-truth masks. At inference, we randomly sam- ple a non-occluded reference image either from all views or only from front views (-45°≤ azimuth ≤ 45°), which of- ten offers more information on the object and illustrates the influence of the reference view. Metrics. For the ShapeNet dataset, we report two differ- ent metrics based on relative camera pose error as done in [25]. Specifically, we provide the median pose error across instances for each category in the test set, and the accuracy Acc 30 for which a prediction is treated as correct when the pose error is ≤ 30◦. Additionally, we present the results of our method for the top 3 and 5 nearest neighbors retrieved by template matching. For the T-LESS dataset, as most objects are symmetric, we report the recall VSD metric as done in [47]. Please note that for the evaluation on the T-LESS dataset, we also pre- dict the translation by using the same formula “projective distance estimation” as SSD-6D [12], as done in [47, 48]. This translation is deduced from the retrieved template and the relative scale factor between the two input images, as detailed in Section 8 of [32]. Baselines. We compare our work with all previous meth- ods that aim to predict a pose from a single view: PIZZA [30], a regression-based approach that directly predicts the relative pose, as well as SSVE [25] and ViewNet [24], which employ semi-supervised and self-supervised tech- niques to treat viewpoint estimation as an image reconstruc- tion problem using conditional generation. We also com- pare our method with the recent diffusion-based method 3DiM [54], which generates pixel-level view synthesis. Since 3DiM originally only targets view-synthesis and is not designed for 3D object pose, we use it to generate tem- plates and perform nearest neighbor search to estimate a 3D object pose. To make 3DiM work in the same setting as us, we retrain it using relative pose conditioning instead of canonical pose conditioning. Implementation. Only the code of PIZZA is available. The other methods did not release their code at the time of writing, however we re-implemented them. We use a ResNet18 backbone as in [30] for PIZZA, SSVE, and ViewNet. We train all models on input images with a res- olution of 256 ×256 except for 3DiM for which we use a resolution of 128×128 since 3DiM performs view synthe- sis in pixel space, which takes much more memory. Our re- implementations achieve similar performance as the origi- nal papers when evaluated on the same data for seen cat- 5Object categories in the training set airplane bench cabinet car chair display lamp loudspeaker rifle sofa table telephone vessel Object categories in the test set bottle bus clock dishwasher guitar mug pistol skateboard train washer Figure 5. Visualization of training and test sets from the ShapeNet dataset [2].The shapes and appearances in the training and test sets are very different and thus constitute a good test bed for generalization to unseen categories. Method novel inst. bottle∗ bus clock dishwasher guitar mug pistol skateboard train washer mean Acc 30↑ ViewNet [24] 77.5 48.4 36.2 23.5 16.4 37.8 31.3 17.9 33.9 44.8 25.1 35.7 SSVE [25] 75.3 61.5 38.2 41.8 21.3 46.8 38.4 36.8 62.3 41.5 50.8 46.8 PIZZA [30] 72.3 76.0 38.6 38.5 32.6 30.8 35.6 40.4 58.3 52.9 61.0 48.8 3DiM [54] 77.3 95.1 43.5 23.6 24.5 36.0 32.0 31.9 50.3 37.0 56.1 46.1 Ours (top 1) 75.5 96.0 53.6 48.0 48.0 49.0 44.6 69.0 57.8 55.2 60.6 59.8 Ours (top 3) 92.0 97.4 83.8 73.4 78.5 66.8 56.0 83.8 86.2 86.0 84.4 80.8 Ours (top 5) 95.5 97.8 89.8 80.4 88.2 74.6 62.8 88.4 92.8 95.4 93.4 87.1 Median↓ ViewNet [24] 6.6 26.7 35.8 40.3 96.3 50.6 51.6 42.8 37.4 26.8 44.3 41.7 SSVE [25] 6.1 23.8 45.2 41.9 90.4 47.6 49.6 24.0 13.5 24.9 48.1 37.7 PIZZA [30] 5.8 25.5 26.4 43.2 80.6 40.2 45.5 23.4 17.3 20.3 38.5 33.3 3DiM [54] 5.7 1.8 19.8 47.3 98.8 35.2 35.7 21.2 12.5 17.6 19.2 28.6 Ours (top 1) 8.1 1.8 18.4 39.9 77.6 31.6 35.5 13.4 15.5 18.3 8.5 24.4 Ours (top 3) 5.0 1.3 5.8 9.1 4.8 16.0 22.6 8.1 6.5 6.7 5.7 8.3 Ours (top 5) 4.5 1.2 4.5 7.1 4.4 11.6 18.4 6.1 5.6 4.9 5.0 6.6 Table 1. Quantitative results on ShapeNet.*We treat “bottle” as a symmetric category, i.e., the error is only the difference of elevation angle. Since the quality of prediction may depend on the reference image, we report the score as the average over 5 runs with 5 different reference images. egories, as shown in Table 1, which validates our compar- isons. Our method also uses the frozen encoder from [40] to encode the input images into embeddings of size 32×32×8. In all settings, we train the baselines and our method using the same training set and AdamW [22] with an initial learn- ing rate of 5 ×10−5. Training takes about 20 hours on 4 V100 GPUs for each method. 4.2. Comparison with the state of the art 4.2.1 Results on ShapeNet Table 1 summarizes the results of our method compared with the baselines discussed above. Under both the Acc30 and Median metrics, our method consistently achieves the best overall performance, outperforming the baselines by more than 10% in Acc30 and 10 o in Median. In particu- lar, while other works produce reasonable results on unseen instances of seen training categories, they often struggle to Method Ref. image sampling Recall VSD Seen obj. Novel obj. Avg GT CAD Nguyen et al. [32] - 60.15 58.70 59.57 MultiPath [47] - 43.17 43.33 43.24 1 ref. image (avg 5 runs) PIZZA [30] all views 20.05 15.90 18.39 Ours all views 47.03 45.69 46.49 PIZZA [30] front views 21.63 15.55 19.19 Ours front views 49.30 48.46 48.96 Table 2. Comparison to PIZZA [30] and CAD-based meth- ods [32, 47]on seen (obj. 1-18) and novel (obj. 19-30) objects of T-LESS. We report numbers averaged over 5 different samplings and runs. estimate the 3D pose of objects from unseen categories. By contrast, our method works well in this case, demonstrating a better generalization ability on unseen categories. 6Seen objects: #4, #14 Novel objects: #20, #22 Reference Query Prediction Reference Query Prediction Figure 6. Qualitative results on real images of T-LESS.For each sample, we show in the last column the predicted poses. Dishwasher Clock Mug Guitar Figure 7. Failure cases.“Dishwashers”, “clocks”, and “dishwash- ers” are “nearly symmetrical” while “guitars” are barely visible from some viewpoints. This makes the pose estimation very chal- lenging, and all the methods perform poorly on these categories. Figure 8 shows some visualization results of our method on unseen categories, with and without symmetries. Our method produces more accurate 3D poses than the baselines when there is a symmetry axis. 4.2.2 Results on T-LESS Table 2 shows our comparison with [30, 32, 47] on real im- ages of T-LESS. While our method focuses on the more challenging case of using a single reference image, [32, 47] rely on ground-truth CAD models. Our method consis- tently outperforms the baseline PIZZA by a large margin. Interestingly, although there is still a gap compared to the SOTA [32], our method outperforms MultiPath [47]. Fig- ure 6 shows results on seen and unseen objects of T-LESS. 4.3. Robustness to occlusions To evaluate the robustness of our method against oc- clusions, we added random rectangle filled with Gaussian noise to the query images over the objects, in a similar way to Random Erasing [61]. We vary the size of the rectangles to cover a range betwen 0% to 25% of the bounding box of the object. Figures 1 and 8 show several examples. Table 3 compares PIZZA, the best second performing method in our previous evaluation, to our method for differ- ent occlusion rates. Our method remains robust even under large occlusions, thanks to embedding matching. Figure 8 shows that our pose probabilities remain peaked on the cor- rect maximum and shows clearly the symmetries. Acc 30↑Method 0% 5% 10% 15% 20% 25% PIZZA [30] 48.9 44.6 33.3 24.5 18.2 14.6 NOPE (ours)59.8 54.3 48.4 45.1 43.7 40.5 Table 3. Robustness to partial occlusions.We add rectangles of Gaussian noise to the query image, and vary the ratio between the area of the rectangle and the area of the object’s 2D bounding box. Our method remains robust under large occlusions, while PIZZA’s performance decreases significantly. Method Memory Run-time Processing Neighbors search 3DiM [54] 358.6 MB 13 min 0.31 s NOPE (ours) 22.4 MB 1.01 s 0.18 s Table 4. Average run-timeof our method and 3DiM [54] on a single GPU V100. We report the memory used for storing novel views, the time taken to generate novel views, and the time taken for nearest neighbor search to obtain the final prediction. 4.4. Runtime analysis We report the running time of NOPE and 3DiM in Ta- ble 4. Our method is significantly faster than 3DiM, thanks to our strategy of predicting the embedding of novel view- points with a single step instead of multiple diffusion steps. 4.5. Failure cases All the methods fail to yield accurate results when eval- uated on “clock”, “dishwasher”, “guitar”, and “mug” cate- gories, as indicated by the high median errors. As shown in Figure 7, these categories except “guitar” are “almost sym- metric”, in the sense that only small details make the pose non-ambiguous. Our predictions using the top-3 and top-5 nearest neighbors significantly improves median errors for 90-symmetrical, 180-symmetrical objects, but not circular- symmetrical as mug objects. Additionally, guitar objects can appear very thin under certain viewpoints. 5. Conclusion Our experiments have shown that direct inference of av- erage view embeddings from a single view, as in NOPE, leads to accurate object pose estimation. This is true even for objects from unseen categories, while requiring neither retraining nor a 3D model. NOPE also lets us estimate the pose ambiguities that arise for many objects. Acknowledgments. The authors thank Elliot Vincent, Mathis Petrovich and Nicolas Dufour for helpful discussions. This project was funded in part by the European Union (ERC Advanced Grant explorer Funding ID #101097259). This work was performed us- ing HPC resources from GENCI–IDRIS 2022-AD011012294R2 and 2022-AD011012294R3. 7without occlusions Bottle Clock Dishwasher Guitar Mug Pistol Skateboard Train Washer with partial occlusions Reference Query PIZZA [30] Ours Pose distribution Reference Query PIZZA [30] Ours Pose distribution Figure 8. Visual results on unseen categoriesfrom ShapeNet. An arrow indicates the pose with the highest probability as recovered by our method. We visually compare with PIZZA, which is the method with the second best performance. We visualize the predicted poses by rendering the object from these poses, but the 3D model is only used for visualization purposes, not as input to our method. Similarly, we use the canonical pose of the 3D model to visualize this distribution, but not as input to our method. 8References [1] Seungryul Baek, Kwang In Kim, and Tae-Kyun Kim. Weakly-Supervised Domain Adaptation via GAN and Mesh Model for Estimating 3D Hand Poses Interacting Objects. In CVPR, 2020. 1 [2] Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, and Others. ShapeNet: An Information-Rich 3D Model Repository. In arXiv, 2015. 5, 6 [3] Maximilian Denninger, Martin Sundermeyer, Dominik Winkelbauer, Youssef Zidan, Dmitry Olefir, Mohamad El- badrawy, Ahsan Lodhi, and Harinandan Katam. Blender- Proc. In arXiv, 2019. 5 [4] Alexander Grabner, Peter M. Roth, and Vincent Lepetit. 3D Pose Estimation and 3D Model Retrieval for Objects in the Wild. In CVPR, 2018. 2 [5] Xingyi He, Jiaming Sun, Yuang Wang, Di Huang, Hujun Bao, and Xiaowei Zhou. OnePose++: Keypoint-Free One- Shot Object Pose Estimation Without CAD Models. In NeurIPS, 2022. 1 [6] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Dif- fusion Probabilistic Models. In NeurIPS, 2020. 3 [7] Tomas Hodan, Pavel Haluza, Stepan Obdrzalek, Jiri Matas, Manolis Lourakis, and Xenophon Zabulis. T-LESS: An RGB-D Dataset for 6D Pose Estimation of Texture-Less Ob- jects. In WACV, 2017. 5 [8] Yinlin Hu, Joachim Hugonot, Pascal Fua, and Mathieu Salz- mann. Segmentation-Driven 6D Object Pose Estimation. In CVPR, 2019. 1 [9] Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, and Ben Poole. Zero-Shot Text-Guided Object Gen- eration with Dream Fields. In CVPR, 2022. 3 [10] Ajay Jain, Amber Xie, and Pieter Abbeel. VectorFusion: Text-to-SVG By Abstracting Pixel-Based Diffusion Models. In SIGGRAPH, 2022. 3 [11] Hanwen Jiang, Zhenyu Jiang, Kristen Grauman, and Yuke Zhu. Few-View Object Reconstruction with Unknown Cate- gories and Camera Poses. In 3DV, 2022. 5 [12] Wadim Kehl, Fabian Manhardt, Federico Tombari, Slobodan Ilic, and Nassir Navab. SSD-6D: Making RGB-Based 3D Detection and 6D Pose Estimation Great Again. In ICCV, 2017. 1, 5 [13] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, and Popa Tiberiu. CLIP-Mesh: Generating Textured Meshes from Text Using Pretrained Image-Text Models. In SIG- GRAPH, 2022. 3 [14] Yann Labb ´e, Justin Carpentier, Mathieu Aubry, and Josef Sivic. CosyPose: Consistent Multi-View Multi-Object 6D Pose Estimation. In ECCV, 2020. 1 [15] Yann Labb ´e, Lucas Manuelli, Arsalan Mousavian, Stephen Tyree, Stan Birchfield, Jonathan Tremblay, Justin Carpentier, Mathieu Aubry, Dieter Fox, and Josef Sivic. MegaPose: 6D Pose Estimation of Novel Objects via Render & Compare. In CoRL, 2022. 1, 3 [16] Yi Li, Gu Wang, Xiangyang Ji, Yu Xiang, and Dieter Fox. DeepIM: Deep Iterative Matching for 6D Pose Estimation. In ECCV, 2018. 1 [17] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3D: High-Resolution Text-to-3D Content Creation. In arXiv, 2022. 3 [18] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok- makov, Sergey Zakharov, and Carl V ondrick. Zero-1-to-3: Zero-shot one image to 3d object. In ICCV, 2023. 3 [19] Yuan Liu, Yilin Wen, Sida Peng, Cheng Lin, Xiaoxiao Long, Taku Komura, and Wenping Wang. Gen6D: Generalizable Model-Free 6DoF Object Pose Estimation from RGB Im- ages. In ECCV, 2022. 3 [20] Vianney Loing, Renaud Marlet, and Mathieu Aubry. Virtual training for a real application: Accurate object-robot relative localization without calibration. IJCV, 126(9):1045–1060, Sept. 2018. 1 [21] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, and Wenping Wang. Wonder3d: Single image to 3d using cross-domain diffusion. arXiv preprint arXiv:2310.15008, 2023. 2 [22] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In ICLR, 2019. 6 [23] Fabian Manhardt, Diego Martin Arroyo, Christian Rup- precht, Benjamin Busam, Tolga Birdal, Nassir Navab, and Federico Tombari. Explaining the Ambiguity of Object De- tection and 6D Pose From Visual Data. In ICCV, 2019. 2 [24] Octave Mariotti, Oisin Mac Aodha, and Hakan Bilen. ViewNet: Unsupervised Viewpoint Estimation from Condi- tional Generation. In ICCV, 2021. 5, 6 [25] Octave Mariotti and Hakan Bilen. Semi-Supervised View- point Estimation with Geometry-aware Conditional Genera- tion. In ECCV Workshop, 2020. 3, 5, 6 [26] Octave Mariotti, Oisin Mac Aodha, and Hakan Bilen. ViewNeRF: Unsupervised Viewpoint Estimation Using Category-Level Neural Radiance Fields. In BMVC, 2022. 3 [27] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi. RealFusion: 360o Reconstruction of Any Object from a Single Image. In arXiv, 2023. 3 [28] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing Scenes as Neural Radiance Fields for View Synthesis. In ECCV, 2020. 2 [29] Thomas M ¨uller, Alex Evans, Christoph Schied, and Alexan- der Keller. Instant Neural Graphics Primitives with a Multiresolution Hash Encoding. TRA, 41(4), July 2022. arXiv:2201.05989 [cs]. 2 [30] Van Nguyen Nguyen, Yuming Du, Yang Xiao, Michael Ramamonjisoa, and Vincent Lepetit. PIZZA: A Powerful Image-only Zero-Shot Zero-CAD Approach to 6 DoF Track- ing. In 3DV, 2022. 5, 6, 7, 8 [31] Van Nguyen Nguyen, Thibault Groueix, Mathieu Salzmann, and Vincent Lepetit. GigaPose: Fast and Robust Novel Ob- ject Pose Estimation via One Correspondence. In CVPR, 2024. 1, 3 9[32] Van Nguyen Nguyen, Yinlin Hu, Yang Xiao, Mathieu Salz- mann, and Vincent Lepetit. Templates for 3D Object Pose Estimation Revisited: Generalization to New Objects and Robustness to Occlusions. In CVPR, 2022. 1, 2, 3, 4, 5, 6, 7 [33] Markus Oberweger, Mahdi Rad, and Vincent Lepetit. Mak- ing Deep Heatmaps Robust to Partial Occlusions for 3D Ob- ject Pose Estimation. In ECCV, 2018. 1 [34] Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hu- jun Bao. PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation. In CVPR, 2019. 1 [35] Giorgia Pitteri, Aur ´elie Bugeau, Slobodan Ilic, and Vincent Lepetit. 3D Object Detection and Pose Estimation of Unseen Objects in Color Images with Local Surface Embeddings. In ACCV, 2020. 2, 3 [36] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden- hall. DreamFusion: Text-to-3D Using 2D Diffusion. In arXiv, 2022. 3 [37] Mahdi Rad and Vincent Lepetit. BB8: A Scalable, Accu- rate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects Without Using Depth. In ICCV, 2017. 1 [38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision. In ICML, 2021. 3 [39] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical Text-Conditional Image Gen- eration with CLIP Latents. In arXiv, 2022. 3 [40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj ´orn Ommer. High-Resolution Image Synthesis with Latent Diffusion Models. In CVPR, 2022. 3, 4, 6 [41] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. DreamBooth: Fine Tuning Text-To-Image Diffusion Models for Subject-Driven Generation. In arXiv, 2022. 3 [42] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, and Others. Photorealistic Text-to- Image Diffusion Models with Deep Language Understand- ing. In arXiv, 2022. 3 [43] Katja Schwarz, Axel Sauer, Michael Niemeyer, Yiyi Liao, and Andreas Geiger. V oxGRAF: Fast 3D-Aware Image Syn- thesis with Sparse V oxel Grids. InarXiv, 2022. 2 [44] Ivan Shugurov, Fu Li, Benjamin Busam, and Slobodan Ilic. OSOP: A Multi-Stage One Shot Object Pose Estimation Framework. In CVPR, 2022. 2, 3 [45] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois- ing Diffusion Implicit Models. In ICLR, 2021. 3 [46] Jiaming Sun, Zihao Wang, Siyu Zhang, Xingyi He, Hongcheng Zhao, Guofeng Zhang, and Xiaowei Zhou. OnePose: One-Shot Object Pose Estimation Without CAD Models. In CVPR, 2022. 1, 2, 3 [47] Martin Sundermeyer, Maximilian Durner, En Yen Puang, Zoltan-Csaba Marton, Narunas Vaskevicius, Kai O. Arras, and Rudolph Triebel. Multi-Path Learning for Object Pose Estimation Across Domains. In CVPR, 2020. 1, 2, 5, 6, 7 [48] Martin Sundermeyer, Zoltan-Csaba Marton, Maximilian Durner, Manuel Brucker, and Rudolph Triebel. Implicit 3D Orientation Learning for 6D Object Detection from RGB Im- ages. In ECCV, 2018. 5 [49] Martin Sundermeyer, Zoltan-Csaba Marton, Maximilian Durner, and Rudolph Triebel. Augmented Autoencoders: Implicit 3D Orientation Learning for 6D Object Detection. IJCV, 128(3), 2020. 1 [50] Bugra Tekin, Sudipta N. Sinha, and Pascal Fua. Real-Time Seamless Single Shot 6D Object Pose Prediction. In CVPR, 2018. 1 [51] Jonathan Tremblay, Thang To, Balakumar Sundaralingam, Yu Xiang, Dieter Fox, and Stan Birchfield. Deep Object Pose Estimation for Semantic Robotic Grasping of House- hold Objects. In CoRL, 2018. 1 [52] Yael Vinker, Ehsan Pajouheshgar, Jessica Y . Bo, Ro- man Christian Bachmann, Amit Haim Bermano, Daniel Cohen-Or, Amir Zamir, and Ariel Shamir. CLIPasso: Semantically-Aware Object Sketching. In SIGGRAPH, 2022. 3 [53] He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas J. Guibas. Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation. In CVPR, 2019. 2 [54] Daniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi. Novel View Synthesis with Diffusion Models. In ICLR, 2023. 3, 5, 6, 7 [55] Yang Xiao, Xuchong Qiu, Pierre-Alain Langlois, Mathieu Aubry, and Renaud Marlet. Pose from Shape: Deep Pose Estimation for Arbitrary 3D Objects. In BMVC, 2019. 1 [56] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance Fields Without Neural Networks. In CVPR, 2022. 2 [57] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelNeRF: Neural Radiance Fields from One or Few Im- ages. In CVPR, 2021. 3 [58] Sergey Zakharov, Ivan S. Shugurov, and Slobodan Ilic. DPOD: 6D Pose Object Detector and Refiner. In ICCV, 2019. 1 [59] Jason Y . Zhang, Deva Ramanan, and Shubham Tulsiani. Rel- Pose: Predicting Probabilistic Relative Rotation for Single Objects in the Wild. In ECCV, 2022. 2, 3 [60] Chen Zhao, Yinlin Hu, and Mathieu Salzmann. Fusing Local Similarities for Retrieval-based 3D Orientation Estimation of Unseen Objects. In ECCV, 2022. 3 [61] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. InAAAI, 2020. 7 [62] Zhizhuo Zhou and Shubham Tulsiani. SparseFusion: Distill- ing View-conditioned Diffusion for 3D Reconstruction. In arXiv, 2023. 3 10",
      "references": [
        "Weakly-Supervised Domain Adaptation via GAN and Mesh Model for Estimating 3D Hand Poses Interacting Objects.",
        "ShapeNet: An Information-Rich 3D Model Repository.",
        "Blender-Proc.",
        "3D Pose Estimation and 3D Model Retrieval for Objects in the Wild.",
        "OnePose++: Keypoint-Free One-Shot Object Pose Estimation Without CAD Models.",
        "Denoising Dif- fusion Probabilistic Models.",
        "T-LESS: An RGB-D Dataset for 6D Pose Estimation of Texture-Less Ob- jects.",
        "Segmentation-Driven 6D Object Pose Estimation.",
        "Zero-Shot Text-Guided Object Gen- eration with Dream Fields.",
        "VectorFusion: Text-to-SVG By Abstracting Pixel-Based Diffusion Models.",
        "Few-View Object Reconstruction with Unknown Cate- gories and Camera Poses.",
        "SSD-6D: Making RGB-Based 3D Detection and 6D Pose Estimation Great Again.",
        "CLIP-Mesh: Generating Textured Meshes from Text Using Pretrained Image-Text Models.",
        "CosyPose: Consistent Multi-View Multi-Object 6D Pose Estimation.",
        "MegaPose: 6D Pose Estimation of Novel Objects via Render & Compare.",
        "DeepIM: Deep Iterative Matching for 6D Pose Estimation.",
        "Magic3D: High-Resolution Text-to-3D Content Creation.",
        "Zero-1-to-3: Zero-shot one image to 3d object.",
        "Gen6D: Generalizable Model-Free 6DoF Object Pose Estimation from RGB Im- ages.",
        "Virtual training for a real application: Accurate object-robot relative localization without calibration.",
        "Wonder3d: Single image to 3d using cross-domain diffusion.",
        "Decoupled Weight Decay Regularization.",
        "Explaining the Ambiguity of Object De- tection and 6D Pose From Visual Data.",
        "ViewNet: Unsupervised Viewpoint Estimation from Condi- tional Generation.",
        "Semi-Supervised View- point Estimation with Geometry-aware Conditional Genera- tion.",
        "ViewNeRF: Unsupervised Viewpoint Estimation Using Category-Level Neural Radiance Fields.",
        "RealFusion: 360o Reconstruction of Any Object from a Single Image.",
        "Nerf: Representing Scenes as Neural Radiance Fields for View Synthesis.",
        "Instant Neural Graphics Primitives with a Multiresolution Hash Encoding.",
        "PIZZA: A Powerful Image-only Zero-Shot Zero-CAD Approach to 6 DoF Track- ing.",
        "GigaPose: Fast and Robust Novel Ob- ject Pose Estimation via One Correspondence.",
        "Templates for 3D Object Pose Estimation Revisited: Generalization to New Objects and Robustness to Occlusions.",
        "Making Deep Heatmaps Robust to Partial Occlusions for 3D Ob- ject Pose Estimation.",
        "PVNet: Pixel-Wise V oting Network for 6DoF Pose Estimation.",
        "3D Object Detection and Pose Estimation of Unseen Objects in Color Images with Local Surface Embeddings.",
        "DreamFusion: Text-to-3D Using 2D Diffusion.",
        "BB8: A Scalable, Accu- rate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects Without Using Depth.",
        "Learning Transferable Visual Models From Natural Language Supervision.",
        "Hierarchical Text-Conditional Image Gen- eration with CLIP Latents.",
        "High-Resolution Image Synthesis with Latent Diffusion Models.",
        "DreamBooth: Fine Tuning Text-To-Image Diffusion Models for Subject-Driven Generation.",
        "Photorealistic Text-to- Image Diffusion Models with Deep Language Understand- ing.",
        "V oxGRAF: Fast 3D-Aware Image Syn- thesis with Sparse V oxel Grids.",
        "OSOP: A Multi-Stage One Shot Object Pose Estimation Framework.",
        "Denois- ing Diffusion Implicit Models.",
        "OnePose: One-Shot Object Pose Estimation Without CAD Models.",
        "Multi-Path Learning for Object Pose Estimation Across Domains.",
        "Implicit 3D Orientation Learning for 6D Object Detection from RGB Im- ages.",
        "Augmented Autoencoders: Implicit 3D Orientation Learning for 6D Object Detection.",
        "Real-Time Seamless Single Shot 6D Object Pose Prediction.",
        "Deep Object Pose Estimation for Semantic Robotic Grasping of House- hold Objects.",
        "CLIPasso: Semantically-Aware Object Sketching.",
        "Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation.",
        "Novel View Synthesis with Diffusion Models.",
        "Pose from Shape: Deep Pose Estimation for Arbitrary 3D Objects.",
        "Plenoxels: Radiance Fields Without Neural Networks.",
        "pixelNeRF: Neural Radiance Fields from One or Few Im- ages.",
        "DPOD: 6D Pose Object Detector and Refiner.",
        "Rel- Pose: Predicting Probabilistic Relative Rotation for Single Objects in the Wild.",
        "Fusing Local Similarities for Retrieval-based 3D Orientation Estimation of Unseen Objects.",
        "Random erasing data augmentation.",
        "SparseFusion: Distill- ing View-conditioned Diffusion for 3D Reconstruction."
      ],
      "meta_data": {
        "arxiv_id": "2303.13612v2",
        "authors": [
          "Van Nguyen Nguyen",
          "Thibault Groueix",
          "Yinlin Hu",
          "Mathieu Salzmann",
          "Vincent Lepetit"
        ],
        "published_date": "2023-03-23T18:55:43Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper introduces NOPE, a novel approach for object pose estimation from a single image, which can predict the relative 3D pose of an unseen object without requiring its 3D model or additional training. A significant contribution is the ability to directly infer average view embeddings that capture pose ambiguities due to symmetries and occlusions, enabling robust and fast pose estimation even on novel object categories.",
        "methodology": "The method leverages a U-Net architecture enhanced with attention mechanisms and conditioned on a relative pose embedding, computed via a jointly trained MLP, to predict discriminative embeddings of novel object views. By comparing these predicted embeddings (acting as templates) with the query image embedding, the system performs a nearest neighbor search to retrieve the most likely pose. The training leverages a pre-trained VAE for embedding extraction, and the approach is framed probabilistically using a normal distribution to model potential ambiguities.",
        "experimental_setup": "The experiments are performed on both synthetic and real-world datasets. For synthetic data, the authors use a ShapeNet-based dataset with 13 training categories and two test sets (novel instances and novel categories), rendered using BlenderProc. For real-world validation, the T-LESS dataset is employed, evaluating on both seen objects (with ground-truth masks) and novel object instances, using metrics such as Acc30, median pose error, and VSD recall. Extensive comparisons with baselines (e.g., PIZZA, SSVE, ViewNet, 3DiM) and evaluations under varying levels of occlusion are also provided.",
        "limitations": "The method faces challenges in cases with extreme or near-circular symmetries and when objects exhibit very subtle details that differentiate ambiguous poses, as seen in failure cases with objects like clocks, dishwashers, guitars, and mugs. Additionally, while effective with single-image input, the approach may be limited by the inherent ambiguity in novel view synthesis from a single viewpoint and relies on assumptions regarding the relationship between embedding distance and actual appearance probabilities.",
        "future_research_directions": "Future work could explore enhancing pose disambiguation in cases of weak or near-symmetries, potentially by integrating additional cues or combining with refined diffusion-based methods. Extending the framework to predict full 6D poses including more reliable translation estimation, improving robustness to extreme occlusions, and reducing dependency on specific embedding assumptions are promising directions. Further research might also investigate combining this technique with other generative models to enhance detail recovery when only partial views are available.",
        "experimental_code": "",
        "experimental_info": ""
      }
    }
  ],
  "research_hypothesis": {
    "open_problems": "Current soft augmentation techniques employ fixed softening curves and static consistency weights that are not tailored to the specific content of each image or the severity of its augmentation. This rigidity forces manual hyperparameter tuning and may fail to maintain robust prediction invariance when images undergo aggressive or heterogeneous transformations. Moreover, existing methods typically focus on output alignment without enforcing consistency at the feature representation level.",
    "method": "We propose a dual-branch dynamic soft augmentation framework that integrates two data-adaptive modules. First, a lightweight auxiliary network estimates both a temperature scaling factor and a dynamic consistency weight from a joint representation of transformation severity (e.g., visibility) and semantic features extracted from an intermediate layer of the backbone network. Second, we enforce consistency not only on the final output predictions (via KL divergence) but also at an intermediate feature level. This dual-consistency approach encourages robust, invariant representations across diverse augmentations without the need for manual hyperparameter tuning, making the method novel compared to existing fixed or uniformly applied techniques.",
    "experimental_setup": "The proposed method will be validated on standard image classification benchmarks such as CIFAR-10 and CIFAR-100 using ResNet-18 as the backbone model. The experimental design includes: (1) baseline training with existing Soft Augmentation, (2) training with our proposed dynamic soft augmentation framework, and (3) performing ablation studies on the auxiliary modulation network and feature-level consistency branch. Evaluation metrics include classification accuracy as the primary metric, Expected Calibration Error (ECE), and prediction variance under aggressive transformations.",
    "primary_metric": "accuracy",
    "experimental_code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DynamicSoftAugmentationLoss(nn.Module):\n    def __init__(self, base_loss=nn.CrossEntropyLoss()):\n        super(DynamicSoftAugmentationLoss, self).__init__()\n        self.base_loss = base_loss\n        # Auxiliary network takes a one-dimensional transformation severity and extracts a modulation vector\n        self.modulator = nn.Sequential(\n            nn.Linear(1, 16),\n            nn.ReLU(),\n            nn.Linear(16, 2)  # outputs: [dynamic_weight, temperature_offset]\n        )\n        # Feature alignment layer to help incorporate intermediate feature consistency\n        self.feature_projector = nn.Linear(128, 64)  # Assuming intermediate features of dimension 128\n\n    def forward(self, logits, targets, logits_aug, visibility, features, features_aug, base_temperature=1.0):\n        # Compute soft targets using a power function based on transformation severity\n        pmin = 0.5\n        k = 2\n        soft_targets = pmin + (1 - pmin) * visibility.pow(k)\n        one_hot = F.one_hot(targets, num_classes=logits.size(1)).float()\n        soft_labels = one_hot * soft_targets.unsqueeze(1) + (1 - one_hot) * (1 - soft_targets).unsqueeze(1)\n        soft_labels = soft_labels / soft_labels.sum(dim=1, keepdim=True)\n        loss_ce = -torch.sum(soft_labels * F.log_softmax(logits, dim=1), dim=1).mean()\n\n        # Obtain dynamic scaling parameters from average visibility\n        avg_vis = torch.mean(visibility).unsqueeze(0).unsqueeze(0)  # shape [1,1]\n        mod_out = self.modulator(avg_vis)  # shape [1,2]\n        dynamic_weight = torch.sigmoid(mod_out[0,0])  # in (0,1)\n        temperature = base_temperature + F.softplus(mod_out[0,1])  \n\n        # Temperature scaling for predictions\n        scaled_logits = logits / temperature\n        scaled_logits_aug = logits_aug / temperature\n        loss_output_consistency = F.kl_div(F.log_softmax(scaled_logits, dim=1), F.softmax(scaled_logits_aug, dim=1), reduction='batchmean')\n\n        # Feature-level consistency: project features and compute L2 loss\n        proj_features = self.feature_projector(features)\n        proj_features_aug = self.feature_projector(features_aug)\n        loss_feature_consistency = F.mse_loss(proj_features, proj_features_aug)\n\n        total_loss = loss_ce + dynamic_weight * (loss_output_consistency + 0.5 * loss_feature_consistency)\n        return total_loss\n\n# Example usage:\nif __name__ == '__main__':\n    batch_size, num_classes = 8, 10\n    logits = torch.randn(batch_size, num_classes)\n    logits_aug = torch.randn(batch_size, num_classes)\n    targets = torch.randint(0, num_classes, (batch_size,))\n    # Simulated visibility scores representing augmentation severity\n    visibility = torch.rand(batch_size)\n    # Simulated intermediate features from a backbone network (e.g., from a layer of ResNet)\n    features = torch.randn(batch_size, 128)\n    features_aug = torch.randn(batch_size, 128)\n\n    criterion = DynamicSoftAugmentationLoss()\n    loss = criterion(logits, targets, logits_aug, visibility, features, features_aug)\n    print('Total Loss:', loss.item())",
    "expected_result": "We anticipate that the proposed dynamic augmentation framework will lead to an improvement in classification accuracy by approximately 1-3% over the baseline Soft Augmentation on CIFAR-10 and CIFAR-100. In addition, we expect a reduction in the Expected Calibration Error (ECE) and a lower prediction variance under aggressive transformation regimes. Ablation studies should confirm the positive contributions of both the dynamic modulation and the feature-level consistency terms.",
    "expected_conclusion": "The refined hypothesis demonstrates academic novelty and practical significance by introducing a dual-consistency mechanism—both at output and feature levels—paired with an adaptive modulation network to dynamically calibrate augmentation effects. This method addresses key limitations in fixed soft augmentation approaches by automating complex hyperparameter tuning and ensuring robust, transformation-invariant representations. It is expected to enhance model accuracy and calibration, offering a valuable advancement for data-efficient image classification tasks."
  },
  "experimental_design": {
    "experiment_summary": "This experiment aims to demonstrate the superiority of a dual-branch dynamic soft augmentation framework for image classification. The model (ResNet-18) is trained on CIFAR-10 using our proposed dynamic soft augmentation method, which integrates an auxiliary modulation network to dynamically adjust consistency weights and temperature scaling factors based on the transformation severity (visibility) and intermediate semantic features. In parallel, the method enforces consistency at both the output prediction level (via KL divergence) and the intermediate feature level (via L2 loss). The experiment is designed to run within the available Runner resources (self-hosted, gpu-runner with NVIDIA A100/H200, 80GB+ VRAM, 2048GB+ RAM) by using a moderate training schedule (e.g., 100 epochs, typical batch sizes) and is scaled for efficiency while maintaining reliable statistical significance for metrics such as accuracy (primary), Expected Calibration Error (ECE), and prediction variance under aggressive transformations.",
    "runner_config": {
      "runner_label": [
        "self-hosted",
        "gpu-runner"
      ],
      "description": "NVIDIA A100 or H200, VRAM: 80 GB or more, RAM: 2048 GB or more"
    },
    "evaluation_metrics": [
      {
        "name": "accuracy",
        "description": "Measures the proportion of correctly classified images. Correct predictions are determined by matching the predicted label with the ground truth. Calculation: (Number of correct predictions / Total predictions). Suitable for classification tasks. Visualization: Bar charts or line plots (e.g., learning curves) can illustrate changes over epochs."
      },
      {
        "name": "Expected Calibration Error (ECE)",
        "description": "ECE quantifies the calibration of the model's predicted probabilities. It is computed by grouping predictions into bins, comparing the average confidence with the accuracy in each bin, and averaging the weighted differences. It is particularly important to assess model reliability under varying augmentation conditions. Visualization: Reliability diagrams."
      },
      {
        "name": "Prediction Variance",
        "description": "Measures the variability of predictions under aggressive augmentations. Lower variance indicates more robust and invariant predictions. This metric is computed as the variance of prediction probabilities across multiple augmentation instances of the same image. Visualization: Error distribution histograms."
      }
    ],
    "models_to_use": [
      "ResNet-18 (approximately 11M parameters)"
    ],
    "datasets_to_use": [
      "CIFAR-10"
    ],
    "proposed_method": {
      "method_name": "Dynamic Soft Augmentation with Dual Consistency",
      "description": "A dual-branch framework that employs a lightweight auxiliary network to dynamically modulate augmentation parameters (temperature scaling and consistency weight) based on transformation severity and semantic features, combined with enforcing both output and feature-level consistency.",
      "training_config": {
        "learning_rate": 0.1,
        "batch_size": 128,
        "epochs": 100,
        "optimizer": "SGD",
        "warmup_steps": 500,
        "weight_decay": 0.0001,
        "gradient_clip": 1.0,
        "scheduler": "linear",
        "seed": 42,
        "additional_params": "{\"modulation_network_layers\": [16, 2], \"feature_projector_output_dim\": 64, \"soft_target_params\": {\"pmin\": 0.5, \"k\": 2}}"
      },
      "optuna_config": {
        "enabled": true,
        "n_trials": 20,
        "search_spaces": [
          {
            "param_name": "learning_rate",
            "distribution_type": "loguniform",
            "low": 0.0001,
            "high": 0.1
          },
          {
            "param_name": "weight_decay",
            "distribution_type": "loguniform",
            "low": 1e-05,
            "high": 0.01
          },
          {
            "param_name": "dynamic_weight_scale",
            "distribution_type": "uniform",
            "low": 0.5,
            "high": 2.0
          }
        ]
      }
    },
    "comparative_methods": [
      {
        "method_name": "Baseline Fixed Soft Augmentation",
        "description": "A standard soft augmentation technique with fixed softening curves and constant consistency weights, enforcing output-level consistency via KL divergence but without feature-level alignment and dynamic modulation. It serves as the baseline for comparison against the proposed dynamic approach.",
        "training_config": {
          "learning_rate": 0.1,
          "batch_size": 128,
          "epochs": 100,
          "optimizer": "SGD",
          "warmup_steps": 500,
          "weight_decay": 0.0001,
          "gradient_clip": 1.0,
          "scheduler": "linear",
          "seed": 42,
          "additional_params": "{\"constant_consistency_weight\": 1.0, \"temperature\": 1.0, \"soft_target_params\": {\"pmin\": 0.5, \"k\": 2}}"
        },
        "optuna_config": {
          "enabled": true,
          "n_trials": 20,
          "search_spaces": [
            {
              "param_name": "learning_rate",
              "distribution_type": "loguniform",
              "low": 0.0001,
              "high": 0.1
            },
            {
              "param_name": "weight_decay",
              "distribution_type": "loguniform",
              "low": 1e-05,
              "high": 0.01
            }
          ]
        }
      }
    ]
  },
  "experiment_code": {
    "train_py": "import os\nimport math\nimport json\nimport random\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Tuple\n\nimport hydra\nimport numpy as np\nimport optuna\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom omegaconf import DictConfig, OmegaConf\nfrom sklearn.metrics import confusion_matrix\nfrom torch.utils.data import DataLoader\nimport wandb\n\nfrom src.preprocess import get_datasets\nfrom src.model import get_model_and_loss\n\n# -----------------------------------------------------------------------------\n#                              Utility helpers\n# -----------------------------------------------------------------------------\n\ndef set_seed(seed: int) -> None:\n    \"\"\"Set python, numpy and torch RNG for full reproducibility.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef accuracy_fn(logits: torch.Tensor, targets: torch.Tensor) -> float:\n    preds = logits.argmax(dim=1)\n    return preds.eq(targets).float().mean().item()\n\n\ndef expected_calibration_error(\n    probs: torch.Tensor, labels: torch.Tensor, n_bins: int = 15\n) -> float:\n    \"\"\"Compute Expected Calibration Error (ECE).\"\"\"\n    if probs.numel() == 0:\n        return 0.0\n    confidences, predictions = probs.max(dim=1)\n    accuracies = predictions.eq(labels)\n    bin_boundaries = torch.linspace(0.0, 1.0, steps=n_bins + 1, device=probs.device)\n    ece = torch.zeros(1, device=probs.device)\n    for i in range(n_bins):\n        in_bin = (confidences > bin_boundaries[i]) & (\n            confidences <= bin_boundaries[i + 1]\n        )\n        prop_in_bin = in_bin.float().mean()\n        if prop_in_bin.item() > 0:\n            accuracy_in_bin = accuracies[in_bin].float().mean()\n            avg_conf_in_bin = confidences[in_bin].mean()\n            ece += torch.abs(avg_conf_in_bin - accuracy_in_bin) * prop_in_bin\n    return ece.item()\n\n# -----------------------------------------------------------------------------\n#                           Training & Validation\n# -----------------------------------------------------------------------------\n\ndef _train_one_epoch(\n    model: nn.Module,\n    criterion: nn.Module,\n    loader: DataLoader,\n    optimizer: torch.optim.Optimizer,\n    device: torch.device,\n    epoch: int,\n    cfg: DictConfig,\n    global_step: int,\n) -> Tuple[int, float]:\n    \"\"\"Train for exactly one epoch, returning updated global_step and last batch acc.\"\"\"\n    model.train()\n    last_train_acc = 0.0\n    for batch_idx, (img, img_aug, vis, labels) in enumerate(loader):\n        if cfg.mode == \"trial\" and batch_idx >= 2:\n            break\n\n        if epoch == 0 and batch_idx == 0:\n            assert img.shape == img_aug.shape, \"Clean/Aug image shape mismatch\"\n            assert img.shape[0] == labels.shape[0], \"Batch size mismatch with labels\"\n\n        img, img_aug, vis, labels = [\n            t.to(device, non_blocking=True) for t in (img, img_aug, vis, labels)\n        ]\n\n        logits, feats = model(img, return_features=True)\n        logits_aug, feats_aug = model(img_aug, return_features=True)\n\n        loss = criterion(\n            logits=logits,\n            targets=labels,\n            logits_aug=logits_aug,\n            visibility=vis.squeeze(),\n            features=feats,\n            features_aug=feats_aug,\n        )\n        if not torch.isfinite(loss):\n            raise RuntimeError(\n                f\"Non-finite loss encountered (epoch={epoch}, step={global_step}): {loss.item()}\"\n            )\n\n        loss.backward()\n\n        # Gradient integrity assertions\n        grad_norm_sum = 0.0\n        for p in model.parameters():\n            if p.grad is not None:\n                grad_norm_sum += p.grad.abs().sum().item()\n        assert grad_norm_sum > 0.0, \"All gradients are zero – graph disconnected!\"\n        assert not math.isnan(grad_norm_sum) and not math.isinf(grad_norm_sum), \"Bad gradients!\"\n\n        nn.utils.clip_grad_norm_(model.parameters(), cfg.training.gradient_clip)\n        optimizer.step()\n        optimizer.zero_grad(set_to_none=True)\n\n        last_train_acc = accuracy_fn(logits, labels)\n        if cfg.wandb.mode != \"disabled\":\n            wandb.log(\n                {\n                    \"train_loss\": loss.item(),\n                    \"train_acc\": last_train_acc,\n                    \"epoch\": epoch,\n                    \"step\": global_step,\n                    \"lr\": optimizer.param_groups[0][\"lr\"],\n                }\n            )\n        global_step += 1\n    return global_step, last_train_acc\n\n\ndef _evaluate(\n    model: nn.Module,\n    loader: DataLoader,\n    device: torch.device,\n    epoch: int,\n    cfg: DictConfig,\n    prefix: str = \"val\",\n) -> Dict[str, Any]:\n    model.eval()\n    total_correct = 0\n    total_samples = 0\n    losses: List[float] = []\n    all_probs: List[torch.Tensor] = []\n    all_labels: List[torch.Tensor] = []\n\n    with torch.no_grad():\n        for batch_idx, (img, labels) in enumerate(loader):\n            if cfg.mode == \"trial\" and batch_idx >= 2:\n                break\n            img = img.to(device, non_blocking=True)\n            labels = labels.to(device, non_blocking=True)\n\n            logits, _ = model(img, return_features=True)\n            probs = torch.softmax(logits, dim=1)\n            losses.append(F.cross_entropy(logits, labels).item())\n            total_correct += logits.argmax(1).eq(labels).sum().item()\n            total_samples += labels.size(0)\n            all_probs.append(probs.cpu())\n            all_labels.append(labels.cpu())\n\n    mean_loss = float(np.mean(losses)) if losses else 0.0\n    acc = total_correct / max(1, total_samples)\n    probs_cat = torch.cat(all_probs, dim=0) if all_probs else torch.empty(0)\n    labels_cat = torch.cat(all_labels, dim=0) if all_labels else torch.empty(0, dtype=torch.long)\n    ece = expected_calibration_error(probs_cat, labels_cat) if probs_cat.numel() else 0.0\n    cm = (\n        confusion_matrix(labels_cat.numpy(), probs_cat.argmax(1).numpy())\n        if probs_cat.numel()\n        else None\n    )\n\n    metrics = {\n        f\"{prefix}_loss\": mean_loss,\n        f\"{prefix}_acc\": acc,\n        f\"{prefix}_ece\": ece,\n        f\"{prefix}_confusion_matrix\": cm.tolist() if cm is not None else None,\n    }\n\n    if cfg.wandb.mode != \"disabled\":\n        log_dict = {k: v for k, v in metrics.items() if v is not None}\n        log_dict[\"epoch\"] = epoch\n        wandb.log(log_dict)\n\n    return metrics\n\n# -----------------------------------------------------------------------------\n#                         Optuna – Hyper-parameter Search\n# -----------------------------------------------------------------------------\n\ndef _suggest_from_cfg(trial: optuna.Trial, search_spaces: List[Dict[str, Any]]) -> Dict[str, Any]:\n    suggestions: Dict[str, Any] = {}\n    for space in search_spaces:\n        name = space[\"param_name\"]\n        dist = space[\"distribution_type\"].lower()\n        if dist == \"loguniform\":\n            suggestions[name] = trial.suggest_float(\n                name, float(space[\"low\"]), float(space[\"high\"]), log=True\n            )\n        elif dist == \"uniform\":\n            suggestions[name] = trial.suggest_float(\n                name, float(space[\"low\"]), float(space[\"high\"]), log=False\n            )\n        elif dist == \"categorical\":\n            suggestions[name] = trial.suggest_categorical(name, space[\"choices\"])\n        else:\n            raise ValueError(f\"Unsupported Optuna distribution type: {dist}\")\n    return suggestions\n\n\ndef _run_quick_eval(cfg: DictConfig, hyperparams: Dict[str, Any]) -> float:\n    cfg = OmegaConf.clone(cfg)\n    OmegaConf.set_struct(cfg, False)\n    for k, v in hyperparams.items():\n        if k in cfg.training:\n            cfg.training[k] = v\n        else:\n            cfg.training.additional_params[k] = v\n    cfg.mode = \"trial\"\n    cfg.wandb.mode = \"disabled\"\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    train_ds, val_ds, _ = get_datasets(cfg, subset_ratio=0.15)\n    train_loader = DataLoader(\n        train_ds, batch_size=min(64, cfg.training.batch_size), shuffle=True, num_workers=2\n    )\n    val_loader = DataLoader(\n        val_ds, batch_size=min(64, cfg.training.batch_size), shuffle=False, num_workers=2\n    )\n    model, criterion = get_model_and_loss(cfg)\n    model.to(device)\n    criterion.to(device)\n\n    optimiser = torch.optim.SGD(\n        model.parameters(),\n        lr=cfg.training.learning_rate,\n        momentum=0.9,\n        weight_decay=cfg.training.weight_decay,\n    )\n\n    global_step = 0\n    for epoch in range(3):\n        global_step, _ = _train_one_epoch(\n            model,\n            criterion,\n            train_loader,\n            optimiser,\n            device,\n            epoch,\n            cfg,\n            global_step,\n        )\n    val_metrics = _evaluate(model, val_loader, device, epoch=2, cfg=cfg)\n    return 1.0 - val_metrics[\"val_acc\"]\n\n# -----------------------------------------------------------------------------\n#                                Main Entrypoint\n# -----------------------------------------------------------------------------\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\", version_base=None)\ndef main(cfg: DictConfig):  # noqa: C901\n    if cfg.run is None:\n        raise ValueError(\"run=<run_id> must be supplied.\")\n\n    run_id: str = cfg.run\n    run_yaml = (\n        Path(__file__).resolve().parent.parent / \"config\" / \"runs\" / f\"{run_id}.yaml\"\n    )\n    if not run_yaml.exists():\n        raise FileNotFoundError(f\"Run-specific YAML not found: {run_yaml}\")\n\n    run_cfg = OmegaConf.load(run_yaml)\n    cfg = OmegaConf.merge(cfg, run_cfg)\n    OmegaConf.set_struct(cfg, False)\n\n    if cfg.mode not in {\"trial\", \"full\"}:\n        raise ValueError(\"mode must be 'trial' or 'full'\")\n    if cfg.mode == \"trial\":\n        cfg.training.epochs = 1\n        cfg.wandb.mode = \"disabled\"\n        cfg.optuna.n_trials = 0\n    else:\n        cfg.wandb.mode = \"online\"\n\n    for section in (\"training\", \"dataset\", \"model\"):\n        assert hasattr(cfg, section), f\"Missing required config section: {section}\"\n\n    set_seed(int(cfg.training.seed))\n    results_root = Path(cfg.results_dir).expanduser().resolve()\n    run_dir = results_root / run_id\n    run_dir.mkdir(parents=True, exist_ok=True)\n\n    # ----------------------------- Optuna ---------------------------------- #\n    if cfg.optuna.n_trials and int(cfg.optuna.n_trials) > 0:\n\n        def objective(trial: optuna.Trial) -> float:\n            suggestions = _suggest_from_cfg(trial, cfg.optuna.search_spaces)\n            cfg_trial = OmegaConf.clone(cfg)\n            return _run_quick_eval(cfg_trial, suggestions)\n\n        study = optuna.create_study(direction=\"minimize\")\n        study.optimize(objective, n_trials=int(cfg.optuna.n_trials), show_progress_bar=True)\n        best_params = study.best_params\n        with open(run_dir / \"optuna_best_params.json\", \"w\") as fp:\n            json.dump(best_params, fp, indent=2)\n        for k, v in best_params.items():\n            if k in cfg.training:\n                cfg.training[k] = v\n            else:\n                cfg.training.additional_params[k] = v\n\n    # -------------------------- Data preparation -------------------------- #\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    train_ds, val_ds, test_ds = get_datasets(cfg)\n\n    def _build_loader(ds, shuffle: bool):\n        return DataLoader(\n            ds,\n            batch_size=cfg.training.batch_size,\n            shuffle=shuffle,\n            num_workers=min(os.cpu_count(), 8),\n            pin_memory=torch.cuda.is_available(),\n            drop_last=shuffle,\n        )\n\n    train_loader = _build_loader(train_ds, shuffle=True)\n    val_loader = _build_loader(val_ds, shuffle=False)\n    test_loader = _build_loader(test_ds, shuffle=False)\n\n    # -------------------------- Model & Optimiser -------------------------- #\n    model, criterion = get_model_and_loss(cfg)\n    model.to(device)\n    criterion.to(device)\n\n    assert (\n        hasattr(model, \"num_classes\") and model.num_classes == cfg.dataset.num_classes\n    ), \"Model's num_classes attribute mismatch!\"\n\n    if cfg.training.optimizer.lower() == \"sgd\":\n        optimizer = torch.optim.SGD(\n            model.parameters(),\n            lr=cfg.training.learning_rate,\n            momentum=cfg.training.momentum,\n            weight_decay=cfg.training.weight_decay,\n        )\n    elif cfg.training.optimizer.lower() == \"adamw\":\n        optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=cfg.training.learning_rate,\n            weight_decay=cfg.training.weight_decay,\n        )\n    else:\n        raise ValueError(f\"Unsupported optimiser: {cfg.training.optimizer}\")\n\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.training.epochs)\n\n    # --------------------------- WandB init -------------------------------- #\n    if cfg.wandb.mode != \"disabled\":\n        wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=run_id,\n            resume=\"allow\",\n            config=OmegaConf.to_container(cfg, resolve=True),\n            mode=cfg.wandb.mode,\n        )\n        print(f\"[WandB] URL: {wandb.run.get_url()}\")\n\n    # ------------------------------ Training ------------------------------ #\n    best_val_acc = 0.0\n    global_step = 0\n    last_train_acc = 0.0\n    for epoch in range(cfg.training.epochs):\n        global_step, last_train_acc = _train_one_epoch(\n            model,\n            criterion,\n            train_loader,\n            optimizer,\n            device,\n            epoch,\n            cfg,\n            global_step,\n        )\n        val_metrics = _evaluate(model, val_loader, device, epoch, cfg, prefix=\"val\")\n        scheduler.step()\n        if val_metrics[\"val_acc\"] > best_val_acc:\n            best_val_acc = val_metrics[\"val_acc\"]\n            torch.save(model.state_dict(), run_dir / \"best.pt\")\n        if cfg.wandb.mode != \"disabled\":\n            wandb.summary.update(\n                {\n                    \"best_val_acc\": best_val_acc,\n                    \"val_acc\": val_metrics[\"val_acc\"],\n                    \"val_ece\": val_metrics[\"val_ece\"],\n                    \"val_confusion_matrix\": val_metrics[\"val_confusion_matrix\"],\n                }\n            )\n\n    # ------------------------- Final evaluation --------------------------- #\n    test_metrics = _evaluate(model, test_loader, device, epoch=cfg.training.epochs, cfg=cfg, prefix=\"test\")\n    torch.save(model.state_dict(), run_dir / \"last.pt\")\n\n    if cfg.wandb.mode != \"disabled\":\n        wandb.summary.update(\n            {\n                \"accuracy\": best_val_acc,\n                \"train_acc_final\": last_train_acc,\n                \"test_acc\": test_metrics[\"test_acc\"],\n                \"test_ece\": test_metrics[\"test_ece\"],\n                \"test_confusion_matrix\": test_metrics[\"test_confusion_matrix\"],\n                \"final_checkpoint\": str(run_dir / \"last.pt\"),\n            }\n        )\n        wandb.finish()\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "import argparse\nimport json\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport wandb\nfrom scipy import stats\n\nPRIMARY_METRIC = \"accuracy\"  # Must match train.py WandB summary key\n\n\ndef _plot_learning_curve(history_df: pd.DataFrame, run_id: str, out_dir: Path) -> Path:\n    fig, ax = plt.subplots(figsize=(6, 4))\n    for col in history_df.columns:\n        if col.startswith(\"train_\") and col.endswith(\"acc\"):\n            sns.lineplot(data=history_df, x=\"epoch\", y=col, ax=ax, label=col)\n        if col.startswith(\"val_\") and col.endswith(\"acc\"):\n            sns.lineplot(data=history_df, x=\"epoch\", y=col, ax=ax, label=col)\n    ax.set_title(f\"Learning Curve – {run_id}\")\n    ax.set_ylabel(\"Accuracy\")\n    ax.set_xlabel(\"Epoch\")\n    ax.legend()\n    fig.tight_layout()\n    path = out_dir / f\"{run_id}_learning_curve.pdf\"\n    fig.savefig(path)\n    plt.close(fig)\n    return path\n\n\ndef _plot_confusion_matrix(cm: np.ndarray, run_id: str, out_dir: Path) -> Path:\n    fig, ax = plt.subplots(figsize=(5, 4))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n    ax.set_xlabel(\"Predicted\")\n    ax.set_ylabel(\"True\")\n    ax.set_title(f\"Confusion Matrix – {run_id}\")\n    fig.tight_layout()\n    path = out_dir / f\"{run_id}_confusion_matrix.pdf\"\n    fig.savefig(path)\n    plt.close(fig)\n    return path\n\n\ndef _export_run_metrics(run, out_dir: Path) -> Dict[str, Any]:\n    out_dir.mkdir(parents=True, exist_ok=True)\n    history_df = run.history()  # pandas DataFrame\n    summary = dict(run.summary)\n    config = dict(run.config)\n\n    metrics_path = out_dir / \"metrics.json\"\n    with open(metrics_path, \"w\") as fp:\n        json.dump(\n            {\n                \"history\": history_df.to_dict(orient=\"list\"),\n                \"summary\": summary,\n                \"config\": config,\n            },\n            fp,\n            indent=2,\n        )\n\n    generated_paths: List[Path] = []\n    if not history_df.empty and \"epoch\" in history_df:\n        generated_paths.append(_plot_learning_curve(history_df, run.id, out_dir))\n\n    # Confusion matrix figure (prefer test, else val)\n    cm_key = \"test_confusion_matrix\" if \"test_confusion_matrix\" in summary else \"val_confusion_matrix\"\n    if cm_key in summary and summary[cm_key] is not None:\n        cm_arr = np.array(summary[cm_key])\n        generated_paths.append(_plot_confusion_matrix(cm_arr, run.id, out_dir))\n\n    for p in generated_paths:\n        print(p)\n\n    # Return numeric summary metrics only\n    numeric_metrics = {k: v for k, v in summary.items() if isinstance(v, (int, float))}\n    return numeric_metrics\n\n\ndef _is_metric_to_minimise(metric_name: str) -> bool:\n    low_better_tokens = [\"loss\", \"error\", \"ece\", \"variance\", \"perplexity\"]\n    return any(tok in metric_name.lower() for tok in low_better_tokens)\n\n\ndef main():\n    parser = argparse.ArgumentParser(\"Independent evaluation & visualisation script\")\n    parser.add_argument(\"results_dir\", type=str)\n    parser.add_argument(\"run_ids\", type=str, help=\"JSON list string e.g. '[\\\"run1\\\",\\\"run2\\\"]'\")\n    args = parser.parse_args()\n\n    results_root = Path(args.results_dir).expanduser().resolve()\n    run_ids: List[str] = json.loads(args.run_ids)\n\n    # Load global WandB config\n    import yaml\n    base_cfg = yaml.safe_load(\n        open(Path(__file__).resolve().parent.parent / \"config\" / \"config.yaml\", \"r\")\n    )\n    entity = base_cfg[\"wandb\"][\"entity\"]\n    project = base_cfg[\"wandb\"][\"project\"]\n\n    api = wandb.Api()\n\n    aggregated_metrics: Dict[str, Dict[str, float]] = {}\n    per_run_primary: Dict[str, float] = {}\n    val_acc_time_series: Dict[str, List[float]] = {}\n\n    for rid in run_ids:\n        run = api.run(f\"{entity}/{project}/{rid}\")\n        run_dir = results_root / rid\n        numeric_metrics = _export_run_metrics(run, run_dir)\n        for mname, mval in numeric_metrics.items():\n            aggregated_metrics.setdefault(mname, {})[rid] = float(mval)\n        per_run_primary[rid] = numeric_metrics.get(PRIMARY_METRIC, 0.0)\n\n        # Collect validation accuracy series for significance testing / box plots\n        hist_df = run.history()\n        if \"val_acc\" in hist_df:\n            val_acc_time_series[rid] = list(hist_df[\"val_acc\"].dropna().values)\n\n    # ---------------- Aggregated metrics file ---------------- #\n    comparison_dir = results_root / \"comparison\"\n    comparison_dir.mkdir(parents=True, exist_ok=True)\n\n    # Identify best proposed / baseline\n    best_proposed_id = max(\n        (rid for rid in run_ids if \"proposed\" in rid), key=lambda x: per_run_primary.get(x, -np.inf)\n    )\n    best_baseline_id = max(\n        (rid for rid in run_ids if (\"comparative\" in rid or \"baseline\" in rid)),\n        key=lambda x: per_run_primary.get(x, -np.inf),\n    )\n\n    gap_direction = -1.0 if _is_metric_to_minimise(PRIMARY_METRIC) else 1.0\n    raw_gap = (\n        per_run_primary[best_proposed_id] - per_run_primary[best_baseline_id]\n    )\n    gap = gap_direction * raw_gap / max(1e-8, abs(per_run_primary[best_baseline_id])) * 100.0\n\n    # Statistical significance test (Welch t-test) using val_acc time series\n    p_value = None\n    if best_proposed_id in val_acc_time_series and best_baseline_id in val_acc_time_series:\n        try:\n            t_stat, p_val = stats.ttest_ind(\n                val_acc_time_series[best_proposed_id],\n                val_acc_time_series[best_baseline_id],\n                equal_var=False,\n            )\n            p_value = float(p_val)\n        except Exception:\n            p_value = None\n\n    aggregated_json = {\n        \"primary_metric\": PRIMARY_METRIC,\n        \"metrics\": aggregated_metrics,\n        \"best_proposed\": {\n            \"run_id\": best_proposed_id,\n            \"value\": per_run_primary[best_proposed_id],\n        },\n        \"best_baseline\": {\n            \"run_id\": best_baseline_id,\n            \"value\": per_run_primary[best_baseline_id],\n        },\n        \"gap\": gap,\n        \"p_value\": p_value,\n    }\n    with open(comparison_dir / \"aggregated_metrics.json\", \"w\") as fp:\n        json.dump(aggregated_json, fp, indent=2)\n    print(comparison_dir / \"aggregated_metrics.json\")\n\n    # ---------------- Comparison figures ---------------- #\n    # 1. Bar charts for all collected metrics\n    for metric_name, run_dict in aggregated_metrics.items():\n        df_plot = pd.DataFrame(list(run_dict.items()), columns=[\"run_id\", metric_name])\n        fig, ax = plt.subplots(figsize=(6, 4))\n        sns.barplot(data=df_plot, x=\"run_id\", y=metric_name, ax=ax)\n        for p in ax.patches:\n            height = p.get_height()\n            ax.annotate(f\"{height:.3f}\", (p.get_x() + p.get_width() / 2., height),\n                        ha=\"center\", va=\"bottom\")\n        ax.set_title(f\"{metric_name} Comparison\")\n        ax.set_xlabel(\"Run ID\")\n        ax.set_ylabel(metric_name)\n        fig.autofmt_xdate(rotation=45)\n        fig.tight_layout()\n        path = comparison_dir / f\"comparison_{metric_name}_bar_chart.pdf\"\n        fig.savefig(path)\n        plt.close(fig)\n        print(path)\n\n    # 2. Box plot for primary metric across epochs\n    if val_acc_time_series:\n        df_box = pd.DataFrame(\n            [(rid, acc) for rid, accs in val_acc_time_series.items() for acc in accs],\n            columns=[\"run_id\", \"val_acc\"],\n        )\n        fig, ax = plt.subplots(figsize=(6, 4))\n        sns.boxplot(data=df_box, x=\"run_id\", y=\"val_acc\", ax=ax)\n        ax.set_title(\"Validation Accuracy Distribution Across Epochs\")\n        ax.set_ylabel(\"val_acc\")\n        ax.set_xlabel(\"Run ID\")\n        fig.tight_layout()\n        path_box = comparison_dir / \"comparison_accuracy_box_plot.pdf\"\n        fig.savefig(path_box)\n        plt.close(fig)\n        print(path_box)\n\n    # 3. Append significance result to a text file for transparency\n    if p_value is not None:\n        sig_path = comparison_dir / \"statistical_significance.txt\"\n        with open(sig_path, \"w\") as fp:\n            fp.write(\n                f\"Welch t-test between {best_proposed_id} and {best_baseline_id} on val_acc\\n\"\n                f\"p-value: {p_value:.6f}\\n\"\n            )\n        print(sig_path)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "import random\nfrom typing import Tuple, Optional\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.data import Dataset, Subset\nfrom torchvision import datasets, transforms\nfrom omegaconf import DictConfig\n\nCACHE_DIR = \".cache/\"\nCIFAR_MEAN_STD = {\n    \"cifar10\": ((0.4914, 0.4822, 0.4465), (0.2471, 0.2435, 0.2616)),\n    \"cifar100\": ((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n}\n\n\nclass DualAugVisionDataset(Dataset):\n    \"\"\"Returns (clean_img, aug_img, visibility, label).\"\"\"\n\n    def __init__(self, base_dataset: datasets.VisionDataset, normalize: bool = True):\n        self.base = base_dataset\n        self.normalize = normalize\n        mean, std = CIFAR_MEAN_STD[base_dataset.__class__.__name__.lower()]\n        self.transform_clean = transforms.Compose(\n            [\n                transforms.RandomCrop(32, padding=4),\n                transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std) if normalize else transforms.Lambda(lambda x: x),\n            ]\n        )\n        self.rand_aug = transforms.RandAugment(num_ops=2, magnitude=9)\n        self.transform_aug = transforms.Compose(\n            [\n                self.rand_aug,\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std) if normalize else transforms.Lambda(lambda x: x),\n            ]\n        )\n\n    def __len__(self):\n        return len(self.base)\n\n    def __getitem__(self, idx):\n        img, label = self.base[idx]\n        img_clean = self.transform_clean(img)\n        img_aug = self.transform_aug(img)\n        visibility = torch.tensor(random.uniform(0.3, 1.0), dtype=torch.float32)\n        return img_clean, img_aug, visibility, torch.tensor(label, dtype=torch.long)\n\n\nclass SingleTransformVisionDataset(Dataset):\n    def __init__(self, base_dataset: datasets.VisionDataset, normalize: bool = True):\n        self.base = base_dataset\n        mean, std = CIFAR_MEAN_STD[base_dataset.__class__.__name__.lower()]\n        self.transform = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std) if normalize else transforms.Lambda(lambda x: x),\n            ]\n        )\n\n    def __len__(self):\n        return len(self.base)\n\n    def __getitem__(self, idx):\n        img, label = self.base[idx]\n        return self.transform(img), torch.tensor(label, dtype=torch.long)\n\n\ndef _get_base_dataset(name: str, train: bool):\n    root = Path(CACHE_DIR).expanduser()\n    if name.lower() == \"cifar10\":\n        return datasets.CIFAR10(root=root, train=train, download=True)\n    elif name.lower() == \"cifar100\":\n        return datasets.CIFAR100(root=root, train=train, download=True)\n    else:\n        raise ValueError(f\"Unsupported dataset: {name}\")\n\n\ndef get_datasets(cfg: DictConfig, subset_ratio: Optional[float] = None) -> Tuple[Dataset, Dataset, Dataset]:\n    name = cfg.dataset.name.lower()\n    num_classes = 10 if name == \"cifar10\" else 100\n    cfg.dataset.num_classes = num_classes\n\n    full_train_base = _get_base_dataset(name, train=True)\n    test_base = _get_base_dataset(name, train=False)\n\n    total_size = len(full_train_base)\n    train_size = int(cfg.dataset.train_split)\n    val_size = int(cfg.dataset.val_split)\n    assert train_size + val_size <= total_size, \"Train/val split exceeds size\"\n\n    indices = list(range(total_size))\n    random.seed(cfg.training.seed)\n    random.shuffle(indices)\n\n    train_idx = indices[:train_size]\n    val_idx = indices[train_size : train_size + val_size]\n\n    dual_aug_ds = DualAugVisionDataset(full_train_base, normalize=cfg.dataset.preprocessing.normalize)\n    single_ds = SingleTransformVisionDataset(full_train_base, normalize=cfg.dataset.preprocessing.normalize)\n    test_ds = SingleTransformVisionDataset(test_base, normalize=cfg.dataset.preprocessing.normalize)\n\n    train_ds: Dataset = Subset(dual_aug_ds, train_idx)\n    val_ds: Dataset = Subset(single_ds, val_idx)\n\n    if subset_ratio is not None and 0 < subset_ratio < 1.0:\n        sub_train_len = int(len(train_ds) * subset_ratio)\n        sub_val_len = int(len(val_ds) * subset_ratio)\n        train_ds = Subset(train_ds, list(range(sub_train_len)))\n        val_ds = Subset(val_ds, list(range(sub_val_len)))\n\n    return train_ds, val_ds, test_ds\n",
    "model_py": "from typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom omegaconf import DictConfig\nfrom torchvision import models\n\n\nclass ResNet18Wrapper(nn.Module):\n    def __init__(self, num_classes: int, pretrained: bool = False):\n        super().__init__()\n        backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)\n        in_features = backbone.fc.in_features\n        backbone.fc = nn.Identity()\n        self.backbone = backbone\n        self.classifier = nn.Linear(in_features, num_classes)\n        self.num_classes = num_classes\n\n    def forward(self, x: torch.Tensor, return_features: bool = False):\n        feats = self.backbone(x)\n        logits = self.classifier(feats)\n        if return_features:\n            return logits, feats\n        return logits\n\n\nclass DynamicSoftAugmentationLoss(nn.Module):\n    def __init__(\n        self,\n        base_temperature: float = 1.0,\n        pmin: float = 0.5,\n        k: float = 2.0,\n        feature_dim: int = 512,\n        projector_dim: int = 64,\n    ):\n        super().__init__()\n        self.base_temperature = base_temperature\n        self.pmin = pmin\n        self.k = k\n        self.modulator = nn.Sequential(\n            nn.Linear(1, 16),\n            nn.ReLU(),\n            nn.Linear(16, 2),\n        )\n        self.feature_projector = nn.Linear(feature_dim, projector_dim)\n\n    def forward(\n        self,\n        logits: torch.Tensor,\n        targets: torch.Tensor,\n        logits_aug: torch.Tensor,\n        visibility: torch.Tensor,\n        features: torch.Tensor,\n        features_aug: torch.Tensor,\n    ) -> torch.Tensor:\n        batch_size, num_classes = logits.size()\n        device = logits.device\n        soft_targets = self.pmin + (1 - self.pmin) * visibility.pow(self.k)\n        one_hot = F.one_hot(targets, num_classes=num_classes).float().to(device)\n        soft_labels = one_hot * soft_targets.unsqueeze(1) + (1 - one_hot) * (\n            1 - soft_targets\n        ).unsqueeze(1)\n        soft_labels = soft_labels / soft_labels.sum(dim=1, keepdim=True)\n        loss_ce = -torch.sum(soft_labels * F.log_softmax(logits, dim=1), dim=1).mean()\n\n        avg_vis = visibility.mean().unsqueeze(0).unsqueeze(0)\n        dyn_params = self.modulator(avg_vis)\n        dynamic_weight = torch.sigmoid(dyn_params[0, 0])\n        temperature = self.base_temperature + F.softplus(dyn_params[0, 1])\n\n        scaled_logits = logits / temperature\n        scaled_logits_aug = logits_aug / temperature\n        loss_output_consistency = F.kl_div(\n            F.log_softmax(scaled_logits, dim=1),\n            F.softmax(scaled_logits_aug, dim=1),\n            reduction=\"batchmean\",\n        )\n        proj_f = self.feature_projector(features)\n        proj_f_aug = self.feature_projector(features_aug)\n        loss_feature_consistency = F.mse_loss(proj_f, proj_f_aug)\n        return loss_ce + dynamic_weight * (loss_output_consistency + 0.5 * loss_feature_consistency)\n\n\nclass FixedSoftAugmentationLoss(nn.Module):\n    def __init__(\n        self,\n        consistency_weight: float = 1.0,\n        temperature: float = 1.0,\n        pmin: float = 0.5,\n        k: float = 2.0,\n    ):\n        super().__init__()\n        self.consistency_weight = consistency_weight\n        self.temperature = temperature\n        self.pmin = pmin\n        self.k = k\n\n    def forward(self, logits: torch.Tensor, targets: torch.Tensor, logits_aug: torch.Tensor, **_) -> torch.Tensor:\n        batch_size, num_classes = logits.size()\n        device = logits.device\n        soft_targets = self.pmin + (1 - self.pmin) * torch.ones(batch_size, device=device)\n        one_hot = F.one_hot(targets, num_classes=num_classes).float().to(device)\n        soft_labels = one_hot * soft_targets.unsqueeze(1) + (1 - one_hot) * (\n            1 - soft_targets\n        ).unsqueeze(1)\n        soft_labels = soft_labels / soft_labels.sum(dim=1, keepdim=True)\n        loss_ce = -torch.sum(soft_labels * F.log_softmax(logits, dim=1), dim=1).mean()\n\n        scaled_logits = logits / self.temperature\n        scaled_logits_aug = logits_aug / self.temperature\n        loss_consistency = F.kl_div(\n            F.log_softmax(scaled_logits, dim=1),\n            F.softmax(scaled_logits_aug, dim=1),\n            reduction=\"batchmean\",\n        )\n        return loss_ce + self.consistency_weight * loss_consistency\n\n\ndef get_model_and_loss(cfg: DictConfig) -> Tuple[nn.Module, nn.Module]:\n    num_classes = cfg.dataset.num_classes\n    if cfg.model.name.lower() == \"resnet18\":\n        model = ResNet18Wrapper(num_classes=num_classes, pretrained=cfg.model.pretrained)\n        feature_dim = 512\n    else:\n        raise ValueError(f\"Unsupported model: {cfg.model.name}\")\n\n    method = cfg.method.lower()\n    if \"dynamic\" in method:\n        params = cfg.training.additional_params\n        criterion = DynamicSoftAugmentationLoss(\n            base_temperature=1.0,\n            pmin=params.soft_target_params.pmin,\n            k=params.soft_target_params.k,\n            feature_dim=feature_dim,\n            projector_dim=params.feature_projector_output_dim,\n        )\n    else:\n        params = cfg.training.additional_params\n        criterion = FixedSoftAugmentationLoss(\n            consistency_weight=params.constant_consistency_weight,\n            temperature=params.temperature,\n            pmin=params.soft_target_params.pmin,\n            k=params.soft_target_params.k,\n        )\n    return model, criterion\n",
    "main_py": "import subprocess\nfrom pathlib import Path\n\nimport hydra\nfrom omegaconf import DictConfig\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\", version_base=None)\ndef main(cfg: DictConfig):\n    if cfg.run is None:\n        raise ValueError(\"run=<run_id> must be supplied\")\n    if cfg.mode not in {\"trial\", \"full\"}:\n        raise ValueError(\"mode must be 'trial' or 'full'\")\n\n    results_dir = Path(cfg.results_dir).expanduser().resolve()\n    results_dir.mkdir(parents=True, exist_ok=True)\n\n    cmd = [\n        \"python\",\n        \"-u\",\n        \"-m\",\n        \"src.train\",\n        f\"run={cfg.run}\",\n        f\"results_dir={results_dir}\",\n        f\"mode={cfg.mode}\",\n    ]\n    print(\"[main] Launching:\", \" \".join(map(str, cmd)))\n    subprocess.run(cmd, check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[project]\nname = \"dynamic-soft-augmentation\"\nversion = \"0.1.0\"\ndependencies = [\n    \"torch>=2.1.0\",\n    \"torchvision>=0.16.0\",\n    \"hydra-core>=1.3.2\",\n    \"wandb>=0.16.0\",\n    \"optuna>=3.4.0\",\n    \"scikit-learn>=1.4.0\",\n    \"pandas>=2.2.0\",\n    \"matplotlib>=3.8.0\",\n    \"seaborn>=0.13.0\",\n    \"scipy>=1.12.0\",\n]\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\n",
    "config_yaml": "defaults:\n  - _self_\n\nrun: null  # supplied via CLI\nresults_dir: ./results\nmode: full  # trial or full\n\nwandb:\n  entity: gengaru617-personal\n  project: 2026-01-21\n  mode: online\n\noptuna:\n  n_trials: 0\n  search_spaces: []\n",
    "run_configs": {
      "proposed-resnet18-cifar10": "run_id: proposed-resnet18-cifar10\nmethod: Dynamic Soft Augmentation with Dual Consistency\nmodel:\n  name: resnet18\n  num_parameters: 11000000\n  backbone: ResNet-18\n  pretrained: false\ndataset:\n  name: cifar10\n  image_size: 32\n  channels: 3\n  train_split: 45000\n  val_split: 5000\n  test_split: 10000\n  preprocessing:\n    augmentation: randaugment\n    normalize: true\ntraining:\n  learning_rate: 0.1\n  batch_size: 128\n  epochs: 100\n  optimizer: sgd\n  momentum: 0.9\n  warmup_steps: 500\n  weight_decay: 0.0001\n  gradient_clip: 1.0\n  scheduler: linear\n  seed: 42\n  additional_params:\n    modulation_network_layers: [16, 2]\n    feature_projector_output_dim: 64\n    soft_target_params:\n      pmin: 0.5\n      k: 2\noptuna:\n  n_trials: 20\n  search_spaces:\n    - param_name: learning_rate\n      distribution_type: loguniform\n      low: 0.0001\n      high: 0.1\n    - param_name: weight_decay\n      distribution_type: loguniform\n      low: 1e-05\n      high: 0.01\n    - param_name: dynamic_weight_scale\n      distribution_type: uniform\n      low: 0.5\n      high: 2.0",
      "comparative-1-resnet18-cifar10": "run_id: comparative-1-resnet18-cifar10\nmethod: Baseline Fixed Soft Augmentation\nmodel:\n  name: resnet18\n  num_parameters: 11000000\n  backbone: ResNet-18\n  pretrained: false\ndataset:\n  name: cifar10\n  image_size: 32\n  channels: 3\n  train_split: 45000\n  val_split: 5000\n  test_split: 10000\n  preprocessing:\n    augmentation: randaugment\n    normalize: true\ntraining:\n  learning_rate: 0.1\n  batch_size: 128\n  epochs: 100\n  optimizer: sgd\n  momentum: 0.9\n  warmup_steps: 500\n  weight_decay: 0.0001\n  gradient_clip: 1.0\n  scheduler: linear\n  seed: 42\n  additional_params:\n    constant_consistency_weight: 1.0\n    temperature: 1.0\n    soft_target_params:\n      pmin: 0.5\n      k: 2\noptuna:\n  n_trials: 20\n  search_spaces:\n    - param_name: learning_rate\n      distribution_type: loguniform\n      low: 0.0001\n      high: 0.1\n    - param_name: weight_decay\n      distribution_type: loguniform\n      low: 1e-05\n      high: 0.01"
    }
  }
}